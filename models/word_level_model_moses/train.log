2024-05-11 19:20:21,537 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-11 19:20:21,539 - INFO - joeynmt.helpers -                           cfg.name : word_level_model_moses
2024-05-11 19:20:21,540 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0
2024-05-11 19:20:21,540 - INFO - joeynmt.helpers -                     cfg.data.train : data2/train
2024-05-11 19:20:21,540 - INFO - joeynmt.helpers -                       cfg.data.dev : data2/dev
2024-05-11 19:20:21,540 - INFO - joeynmt.helpers -                      cfg.data.test : data2/test
2024-05-11 19:20:21,541 - INFO - joeynmt.helpers -              cfg.data.dataset_type : plain
2024-05-11 19:20:21,541 - INFO - joeynmt.helpers -                  cfg.data.src.lang : en
2024-05-11 19:20:21,541 - INFO - joeynmt.helpers -                 cfg.data.src.level : word
2024-05-11 19:20:21,541 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False
2024-05-11 19:20:21,541 - INFO - joeynmt.helpers -       cfg.data.src.max_sent_length : 100
2024-05-11 19:20:21,542 - INFO - joeynmt.helpers -             cfg.data.src.voc_limit : 2000
2024-05-11 19:20:21,542 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.pretokenizer : moses
2024-05-11 19:20:21,542 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : it
2024-05-11 19:20:21,542 - INFO - joeynmt.helpers -                 cfg.data.trg.level : word
2024-05-11 19:20:21,543 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False
2024-05-11 19:20:21,543 - INFO - joeynmt.helpers -       cfg.data.trg.max_sent_length : 100
2024-05-11 19:20:21,543 - INFO - joeynmt.helpers -             cfg.data.trg.voc_limit : 2000
2024-05-11 19:20:21,543 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.pretokenizer : moses
2024-05-11 19:20:21,543 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5
2024-05-11 19:20:21,544 - INFO - joeynmt.helpers -                  cfg.testing.alpha : 1.0
2024-05-11 19:20:21,544 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42
2024-05-11 19:20:21,544 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam
2024-05-11 19:20:21,544 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens
2024-05-11 19:20:21,544 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003
2024-05-11 19:20:21,545 - INFO - joeynmt.helpers -            cfg.training.batch_size : 2048
2024-05-11 19:20:21,545 - INFO - joeynmt.helpers -            cfg.training.batch_type : token
2024-05-11 19:20:21,545 - INFO - joeynmt.helpers -       cfg.training.eval_batch_size : 1024
2024-05-11 19:20:21,546 - INFO - joeynmt.helpers -       cfg.training.eval_batch_type : token
2024-05-11 19:20:21,546 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau
2024-05-11 19:20:21,546 - INFO - joeynmt.helpers -              cfg.training.patience : 8
2024-05-11 19:20:21,546 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0
2024-05-11 19:20:21,546 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7
2024-05-11 19:20:21,546 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2024-05-11 19:20:21,547 - INFO - joeynmt.helpers -                cfg.training.epochs : 10
2024-05-11 19:20:21,547 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 500
2024-05-11 19:20:21,547 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100
2024-05-11 19:20:21,547 - INFO - joeynmt.helpers -           cfg.training.eval_metric : bleu
2024-05-11 19:20:21,547 - INFO - joeynmt.helpers -             cfg.training.model_dir : word_level_model_moses
2024-05-11 19:20:21,548 - INFO - joeynmt.helpers -             cfg.training.overwrite : False
2024-05-11 19:20:21,548 - INFO - joeynmt.helpers -               cfg.training.shuffle : True
2024-05-11 19:20:21,548 - INFO - joeynmt.helpers -              cfg.training.use_cuda : True
2024-05-11 19:20:21,548 - INFO - joeynmt.helpers -     cfg.training.max_output_length : 100
2024-05-11 19:20:21,548 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3, 4]
2024-05-11 19:20:21,549 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.3
2024-05-11 19:20:21,549 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier_uniform
2024-05-11 19:20:21,549 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros
2024-05-11 19:20:21,549 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0
2024-05-11 19:20:21,549 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier_uniform
2024-05-11 19:20:21,550 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0
2024-05-11 19:20:21,550 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : False
2024-05-11 19:20:21,550 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True
2024-05-11 19:20:21,550 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer
2024-05-11 19:20:21,550 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 4
2024-05-11 19:20:21,551 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 2
2024-05-11 19:20:21,551 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256
2024-05-11 19:20:21,551 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2024-05-11 19:20:21,551 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0
2024-05-11 19:20:21,551 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256
2024-05-11 19:20:21,551 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 512
2024-05-11 19:20:21,552 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0
2024-05-11 19:20:21,552 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer
2024-05-11 19:20:21,552 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 1
2024-05-11 19:20:21,552 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 2
2024-05-11 19:20:21,552 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256
2024-05-11 19:20:21,553 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2024-05-11 19:20:21,553 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0
2024-05-11 19:20:21,553 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256
2024-05-11 19:20:21,553 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 512
2024-05-11 19:20:21,553 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0
2024-05-11 19:20:21,606 - INFO - joeynmt.data - Building tokenizer...
2024-05-11 19:20:22,075 - INFO - joeynmt.tokenizers - en tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=moses)
2024-05-11 19:20:22,076 - INFO - joeynmt.tokenizers - it tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=moses)
2024-05-11 19:20:22,076 - INFO - joeynmt.data - Loading train set...
2024-05-11 19:20:55,399 - INFO - joeynmt.data - Building vocabulary...
2024-05-11 19:20:58,173 - INFO - joeynmt.data - Loading dev set...
2024-05-11 19:20:59,578 - INFO - joeynmt.data - Loading test set...
2024-05-11 19:21:01,198 - INFO - joeynmt.data - Data loaded.
2024-05-11 19:21:01,198 - INFO - joeynmt.data - Train dataset: PlaintextDataset(split=train, len=100000, src_lang=en, trg_lang=it, has_trg=True, random_subset=-1)
2024-05-11 19:21:01,199 - INFO - joeynmt.data - Valid dataset: PlaintextDataset(split=dev, len=929, src_lang=en, trg_lang=it, has_trg=True, random_subset=-1)
2024-05-11 19:21:01,199 - INFO - joeynmt.data -  Test dataset: PlaintextDataset(split=test, len=1566, src_lang=en, trg_lang=it, has_trg=True, random_subset=-1)
2024-05-11 19:21:01,199 - INFO - joeynmt.data - First training example:
	[SRC] Al Gore : Averting the climate crisis
	[TRG] Al Gore : arrestiamo il riscaldamento globale
2024-05-11 19:21:01,199 - INFO - joeynmt.data - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) a
2024-05-11 19:21:01,200 - INFO - joeynmt.data - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) di (7) che (8) e (9) è
2024-05-11 19:21:01,200 - INFO - joeynmt.data - Number of unique Src tokens (vocab_size): 2004
2024-05-11 19:21:01,200 - INFO - joeynmt.data - Number of unique Trg tokens (vocab_size): 2004
2024-05-11 19:21:01,211 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-11 19:21:01,426 - INFO - joeynmt.model - Enc-dec model built.
2024-05-11 19:21:01,438 - INFO - joeynmt.model - Total params: 3925248
2024-05-11 19:21:01,439 - DEBUG - joeynmt.model - Trainable parameters: ['decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2024-05-11 19:21:01,440 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=2004),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=2004),
	loss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.3))
2024-05-11 19:21:02,930 - INFO - joeynmt.builders - Adam(lr=0.0003, weight_decay=0.0, betas=(0.9, 0.999))
2024-05-11 19:21:02,931 - INFO - joeynmt.builders - ReduceLROnPlateau(mode=min, verbose=False, threshold_mode=abs, eps=0.0, factor=0.7, patience=8)
2024-05-11 19:21:02,932 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 2048
	effective batch size (w. parallel & accumulation): 2048
2024-05-11 19:21:02,932 - INFO - joeynmt.training - EPOCH 1
2024-05-11 19:21:06,716 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     3.036971, Batch Acc: 0.217618, Tokens per Sec:    17549, Lr: 0.000300
2024-05-11 19:21:09,131 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     2.951005, Batch Acc: 0.255186, Tokens per Sec:    27310, Lr: 0.000300
2024-05-11 19:21:12,010 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     2.949220, Batch Acc: 0.275120, Tokens per Sec:    22029, Lr: 0.000300
2024-05-11 19:21:14,457 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     2.739931, Batch Acc: 0.282411, Tokens per Sec:    27942, Lr: 0.000300
2024-05-11 19:21:16,840 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     2.583365, Batch Acc: 0.299164, Tokens per Sec:    27256, Lr: 0.000300
2024-05-11 19:21:16,841 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:21:16,841 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:21:25,769 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.63, ppl:  13.86, acc:   0.31, generation: 8.8364[sec], evaluation: 0.0000[sec]
2024-05-11 19:21:25,770 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:21:26,098 - INFO - joeynmt.training - Example #0
2024-05-11 19:21:26,098 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:21:26,099 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:21:26,099 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', ',', '<unk>', ',', '<unk>', ',', '<unk>', ',', '<unk>', ',', '<unk>', ',', '<unk>', ',', 'il', '<unk>', ',', 'il', '<unk>', ',', 'il', '<unk>', ',', 'il', '<unk>', ',', 'il', '<unk>', ',', 'il', '<unk>', 'di', '<unk>', '.', '</s>']
2024-05-11 19:21:26,100 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:21:26,100 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:21:26,100 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, il <unk>, il <unk>, il <unk>, il <unk>, il <unk>, il <unk> di <unk>.
2024-05-11 19:21:26,100 - INFO - joeynmt.training - Example #1
2024-05-11 19:21:26,101 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:21:26,101 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:21:26,101 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'questo', '<unk>', 'di', '<unk>', '<unk>', '<unk>', 'che', '<unk>', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-11 19:21:26,102 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:21:26,102 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:21:26,102 - INFO - joeynmt.training - 	Hypothesis: Ma questo questo <unk> di <unk> <unk> <unk> che <unk> <unk> <unk> <unk>.
2024-05-11 19:21:26,102 - INFO - joeynmt.training - Example #2
2024-05-11 19:21:26,103 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:21:26,103 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:21:26,103 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', '<unk>', '<unk>', ',', 'è', 'un', '<unk>', ',', '<unk>', ',', 'il', '<unk>', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-11 19:21:26,103 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:21:26,104 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:21:26,104 - INFO - joeynmt.training - 	Hypothesis: La <unk> <unk>, è un <unk>, <unk>, il <unk> <unk> <unk> <unk>.
2024-05-11 19:21:26,104 - INFO - joeynmt.training - Example #3
2024-05-11 19:21:26,104 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:21:26,105 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:21:26,105 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'e', '<unk>', '<unk>', '.', '</s>']
2024-05-11 19:21:26,105 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:21:26,106 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:21:26,106 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> e <unk> <unk>.
2024-05-11 19:21:26,106 - INFO - joeynmt.training - Example #4
2024-05-11 19:21:26,106 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:21:26,107 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:21:26,107 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', '<unk>', '<unk>', 'che', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', 'di', '<unk>', 'di', '<unk>', '.', '</s>']
2024-05-11 19:21:26,107 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:21:26,108 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:21:26,108 - INFO - joeynmt.training - 	Hypothesis: La <unk> <unk> che <unk> <unk> <unk> <unk> <unk> <unk> di <unk> di <unk>.
2024-05-11 19:21:28,899 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     2.500620, Batch Acc: 0.311678, Tokens per Sec:    20818, Lr: 0.000300
2024-05-11 19:21:31,581 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     2.543998, Batch Acc: 0.322090, Tokens per Sec:    24256, Lr: 0.000300
2024-05-11 19:21:34,074 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     2.486083, Batch Acc: 0.334617, Tokens per Sec:    26058, Lr: 0.000300
2024-05-11 19:21:36,893 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     2.399015, Batch Acc: 0.344414, Tokens per Sec:    23463, Lr: 0.000300
2024-05-11 19:21:40,080 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     2.263312, Batch Acc: 0.358099, Tokens per Sec:    20253, Lr: 0.000300
2024-05-11 19:21:40,081 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:21:40,081 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:21:47,476 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.31, ppl:  10.08, acc:   0.35, generation: 7.3503[sec], evaluation: 0.0000[sec]
2024-05-11 19:21:47,476 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:21:47,726 - INFO - joeynmt.training - Example #0
2024-05-11 19:21:47,727 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:21:47,727 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:21:47,727 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'in', 'anno', ',', '<unk>', '<unk>', 'che', '<unk>', '<unk>', 'che', '<unk>', '<unk>', ',', 'che', '<unk>', ',', 'che', '<unk>', '<unk>', 'di', '<unk>', '<unk>', ',', 'il', '<unk>', 'di', '<unk>', ',', 'il', '<unk>', 'del', '<unk>', ',', 'il', '<unk>', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-11 19:21:47,728 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:21:47,729 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:21:47,729 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> in anno, <unk> <unk> che <unk> <unk> che <unk> <unk>, che <unk>, che <unk> <unk> di <unk> <unk>, il <unk> di <unk>, il <unk> del <unk>, il <unk> <unk> <unk> <unk>.
2024-05-11 19:21:47,729 - INFO - joeynmt.training - Example #1
2024-05-11 19:21:47,729 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:21:47,729 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:21:47,729 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', '<unk>', 'perché', 'non', '<unk>', 'il', '<unk>', 'del', '<unk>', '.', '</s>']
2024-05-11 19:21:47,730 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:21:47,730 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:21:47,731 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo <unk> perché non <unk> il <unk> del <unk>.
2024-05-11 19:21:47,731 - INFO - joeynmt.training - Example #2
2024-05-11 19:21:47,731 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:21:47,731 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:21:47,731 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', '<unk>', ',', 'in', 'un', '<unk>', '<unk>', '<unk>', 'il', '<unk>', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-11 19:21:47,732 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:21:47,732 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:21:47,732 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è <unk>, in un <unk> <unk> <unk> il <unk> <unk> <unk> <unk>.
2024-05-11 19:21:47,733 - INFO - joeynmt.training - Example #3
2024-05-11 19:21:47,733 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:21:47,733 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:21:47,733 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', '<unk>', '.', '</s>']
2024-05-11 19:21:47,733 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:21:47,734 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:21:47,734 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in <unk>.
2024-05-11 19:21:47,734 - INFO - joeynmt.training - Example #4
2024-05-11 19:21:47,734 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:21:47,735 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:21:47,735 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', '<unk>', 'che', '<unk>', 'un', '<unk>', 'di', '<unk>', 'che', 'è', 'il', '<unk>', 'di', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-11 19:21:47,735 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:21:47,735 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:21:47,736 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> <unk> che <unk> un <unk> di <unk> che è il <unk> di <unk> <unk> <unk>.
2024-05-11 19:21:50,688 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     2.124021, Batch Acc: 0.365489, Tokens per Sec:    20736, Lr: 0.000300
2024-05-11 19:21:53,970 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     2.224324, Batch Acc: 0.372927, Tokens per Sec:    19446, Lr: 0.000300
2024-05-11 19:21:56,730 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     1.977924, Batch Acc: 0.382901, Tokens per Sec:    23220, Lr: 0.000300
2024-05-11 19:21:59,276 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     2.039974, Batch Acc: 0.394089, Tokens per Sec:    25158, Lr: 0.000300
2024-05-11 19:22:01,799 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     2.151250, Batch Acc: 0.401498, Tokens per Sec:    25943, Lr: 0.000300
2024-05-11 19:22:01,799 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:22:01,800 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:22:11,053 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.07, ppl:   7.96, acc:   0.40, generation: 9.2014[sec], evaluation: 0.0000[sec]
2024-05-11 19:22:11,054 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:22:11,289 - INFO - joeynmt.training - Example #0
2024-05-11 19:22:11,289 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:22:11,290 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:22:11,290 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'anno', 'ho', '<unk>', 'due', '<unk>', 'che', '<unk>', '<unk>', 'che', 'il', '<unk>', '<unk>', '<unk>', ',', 'che', 'per', 'il', '<unk>', 'di', '<unk>', 'anni', 'ha', '<unk>', 'il', '<unk>', 'di', '<unk>', ',', '<unk>', ',', '<unk>', '<unk>', ',', 'ha', '<unk>', '<unk>', ',', '<unk>', '%', '.', '</s>']
2024-05-11 19:22:11,291 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:22:11,291 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:22:11,291 - INFO - joeynmt.training - 	Hypothesis: <unk> anno ho <unk> due <unk> che <unk> <unk> che il <unk> <unk> <unk>, che per il <unk> di <unk> anni ha <unk> il <unk> di <unk>, <unk>, <unk> <unk>, ha <unk> <unk>, <unk>%.
2024-05-11 19:22:11,292 - INFO - joeynmt.training - Example #1
2024-05-11 19:22:11,292 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:22:11,292 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:22:11,292 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', '<unk>', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', '<unk>', 'la', '<unk>', 'della', '<unk>', '.', '</s>']
2024-05-11 19:22:11,293 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:22:11,293 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:22:11,293 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> <unk> <unk> di questo problema perché non <unk> la <unk> della <unk>.
2024-05-11 19:22:11,294 - INFO - joeynmt.training - Example #2
2024-05-11 19:22:11,294 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:22:11,294 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:22:11,294 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', '<unk>', ',', 'in', 'un', 'senso', ',', 'il', '<unk>', 'del', 'sistema', 'del', 'sistema', 'del', 'sistema', '.', '</s>']
2024-05-11 19:22:11,295 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:22:11,295 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:22:11,295 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è <unk>, in un senso, il <unk> del sistema del sistema del sistema.
2024-05-11 19:22:11,295 - INFO - joeynmt.training - Example #3
2024-05-11 19:22:11,295 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:22:11,296 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:22:11,296 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', '<unk>', '.', '</s>']
2024-05-11 19:22:11,296 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:22:11,296 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:22:11,297 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in <unk>.
2024-05-11 19:22:11,297 - INFO - joeynmt.training - Example #4
2024-05-11 19:22:11,297 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:22:11,297 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:22:11,298 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'che', 'mi', '<unk>', 'essere', 'un', '<unk>', 'di', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', '<unk>', 'il', '<unk>', 'anni', '.', '</s>']
2024-05-11 19:22:11,298 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:22:11,298 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:22:11,299 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> che mi <unk> essere un <unk> di <unk> <unk> di ciò che è <unk> il <unk> anni.
2024-05-11 19:22:14,110 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     2.075339, Batch Acc: 0.408339, Tokens per Sec:    20925, Lr: 0.000300
2024-05-11 19:22:16,741 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     2.008519, Batch Acc: 0.423207, Tokens per Sec:    25126, Lr: 0.000300
2024-05-11 19:22:19,959 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     1.817321, Batch Acc: 0.427533, Tokens per Sec:    20230, Lr: 0.000300
2024-05-11 19:22:22,912 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     1.976529, Batch Acc: 0.433488, Tokens per Sec:    22173, Lr: 0.000300
2024-05-11 19:22:25,513 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     1.939696, Batch Acc: 0.439731, Tokens per Sec:    24537, Lr: 0.000300
2024-05-11 19:22:25,513 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:22:25,514 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:22:34,306 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.92, ppl:   6.82, acc:   0.43, generation: 8.6950[sec], evaluation: 0.0000[sec]
2024-05-11 19:22:34,307 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:22:34,629 - INFO - joeynmt.training - Example #0
2024-05-11 19:22:34,631 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:22:34,631 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:22:34,631 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'anno', 'che', '<unk>', 'due', '<unk>', '<unk>', 'che', 'il', '<unk>', 'che', 'il', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'delle', 'tre', 'anni', 'ha', '<unk>', 'la', '<unk>', '<unk>', ',', 'ha', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'ha', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'ha', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'ha', '<unk>', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'ha', '<unk>', '<unk>', '<unk>', '<unk>']
2024-05-11 19:22:34,633 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:22:34,633 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:22:34,633 - INFO - joeynmt.training - 	Hypothesis: <unk> anno che <unk> due <unk> <unk> che il <unk> che il <unk> <unk>, che per la maggior parte delle tre anni ha <unk> la <unk> <unk>, ha <unk> <unk> <unk> <unk>, ha <unk> <unk> <unk> <unk>, ha <unk> <unk> <unk> <unk>, ha <unk> <unk> <unk> <unk>, <unk> <unk> <unk> <unk>, ha <unk> <unk> <unk> <unk>
2024-05-11 19:22:34,634 - INFO - joeynmt.training - Example #1
2024-05-11 19:22:34,634 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:22:34,634 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:22:34,634 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', ',', 'perché', 'non', '<unk>', 'la', '<unk>', 'del', '<unk>', '.', '</s>']
2024-05-11 19:22:34,635 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:22:34,635 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:22:34,635 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo problema, perché non <unk> la <unk> del <unk>.
2024-05-11 19:22:34,636 - INFO - joeynmt.training - Example #2
2024-05-11 19:22:34,636 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:22:34,636 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:22:34,636 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', '<unk>', ',', 'in', 'un', 'senso', ',', 'in', 'un', 'sistema', 'del', '<unk>', 'globale', '.', '</s>']
2024-05-11 19:22:34,637 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:22:34,637 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:22:34,637 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è <unk>, in un senso, in un sistema del <unk> globale.
2024-05-11 19:22:34,637 - INFO - joeynmt.training - Example #3
2024-05-11 19:22:34,637 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:22:34,638 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:22:34,638 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', '<unk>', '.', '</s>']
2024-05-11 19:22:34,638 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:22:34,639 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:22:34,639 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in <unk>.
2024-05-11 19:22:34,639 - INFO - joeynmt.training - Example #4
2024-05-11 19:22:34,639 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:22:34,639 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:22:34,639 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', '<unk>', '<unk>', 'che', '<unk>', 'un', '<unk>', 'di', '<unk>', 'di', '<unk>', 'di', 'quello', 'che', 'è', 'successo', 'nel', '<unk>', 'anni', '.', '</s>']
2024-05-11 19:22:34,640 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:22:34,640 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:22:34,640 - INFO - joeynmt.training - 	Hypothesis: La <unk> <unk> che <unk> un <unk> di <unk> di <unk> di quello che è successo nel <unk> anni.
2024-05-11 19:22:37,464 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     1.831066, Batch Acc: 0.445456, Tokens per Sec:    20183, Lr: 0.000300
2024-05-11 19:22:40,101 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     1.997207, Batch Acc: 0.455323, Tokens per Sec:    24171, Lr: 0.000300
2024-05-11 19:22:42,831 - INFO - joeynmt.training - Epoch   1, Step:     2300, Batch Loss:     1.751227, Batch Acc: 0.456828, Tokens per Sec:    24050, Lr: 0.000300
2024-05-11 19:22:45,537 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     1.859458, Batch Acc: 0.455214, Tokens per Sec:    23417, Lr: 0.000300
2024-05-11 19:22:48,850 - INFO - joeynmt.training - Epoch   1, Step:     2500, Batch Loss:     2.030149, Batch Acc: 0.464445, Tokens per Sec:    19847, Lr: 0.000300
2024-05-11 19:22:48,850 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:22:48,850 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:22:56,579 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.82, ppl:   6.17, acc:   0.46, generation: 7.6369[sec], evaluation: 0.0000[sec]
2024-05-11 19:22:56,579 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:22:56,876 - INFO - joeynmt.training - Example #0
2024-05-11 19:22:56,876 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:22:56,877 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:22:56,877 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'che', '<unk>', 'due', '<unk>', '<unk>', 'che', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'più', '<unk>', ',', 'che', 'per', 'la', 'più', '<unk>', 'della', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'ha', '<unk>', 'il', '<unk>', '%', '.', '</s>']
2024-05-11 19:22:56,878 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:22:56,879 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:22:56,879 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> che <unk> due <unk> <unk> che il ghiaccio <unk> <unk>, che per la più <unk>, che per la più <unk> della <unk> <unk> <unk>, <unk> <unk> <unk> <unk>, ha <unk> il <unk>%.
2024-05-11 19:22:56,879 - INFO - joeynmt.training - Example #1
2024-05-11 19:22:56,880 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:22:56,880 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:22:56,880 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', '<unk>', 'perché', 'non', '<unk>', 'il', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:22:56,881 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:22:56,881 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:22:56,881 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> <unk> il <unk> di questo problema <unk> perché non <unk> il <unk> del ghiaccio.
2024-05-11 19:22:56,881 - INFO - joeynmt.training - Example #2
2024-05-11 19:22:56,881 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:22:56,881 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:22:56,882 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'senso', ',', 'il', '<unk>', 'del', '<unk>', 'globale', '.', '</s>']
2024-05-11 19:22:56,882 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:22:56,883 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:22:56,883 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> <unk> è, in un senso, il <unk> del <unk> globale.
2024-05-11 19:22:56,883 - INFO - joeynmt.training - Example #3
2024-05-11 19:22:56,883 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:22:56,883 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:22:56,883 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:22:56,884 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:22:56,884 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:22:56,884 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in <unk> e <unk> in estate.
2024-05-11 19:22:56,885 - INFO - joeynmt.training - Example #4
2024-05-11 19:22:56,885 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:22:56,885 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:22:56,885 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'che', '<unk>', '<unk>', '<unk>', 'un', '<unk>', 'di', 'ciò', 'che', 'è', 'successo', 'nel', '<unk>', 'il', '25', 'anni', '.', '</s>']
2024-05-11 19:22:56,886 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:22:56,886 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:22:56,886 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> che <unk> <unk> <unk> un <unk> di ciò che è successo nel <unk> il 25 anni.
2024-05-11 19:23:00,054 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     1.732249, Batch Acc: 0.468365, Tokens per Sec:    18752, Lr: 0.000300
2024-05-11 19:23:03,302 - INFO - joeynmt.training - Epoch   1, Step:     2700, Batch Loss:     1.669010, Batch Acc: 0.468627, Tokens per Sec:    20183, Lr: 0.000300
2024-05-11 19:23:05,995 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     1.618565, Batch Acc: 0.478272, Tokens per Sec:    24281, Lr: 0.000300
2024-05-11 19:23:08,665 - INFO - joeynmt.training - Epoch   1, Step:     2900, Batch Loss:     1.632728, Batch Acc: 0.480597, Tokens per Sec:    24485, Lr: 0.000300
2024-05-11 19:23:11,276 - INFO - joeynmt.training - Epoch   1, Step:     3000, Batch Loss:     1.900230, Batch Acc: 0.480813, Tokens per Sec:    24843, Lr: 0.000300
2024-05-11 19:23:11,277 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:23:11,277 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:23:18,836 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.74, ppl:   5.71, acc:   0.48, generation: 7.5072[sec], evaluation: 0.0000[sec]
2024-05-11 19:23:18,836 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:23:19,077 - INFO - joeynmt.helpers - delete word_level_model_moses/500.ckpt
2024-05-11 19:23:19,089 - INFO - joeynmt.training - Example #0
2024-05-11 19:23:19,090 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:23:19,090 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:23:19,090 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'anno', 'che', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'così', '<unk>', 'che', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'delle', '<unk>', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-11 19:23:19,091 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:23:19,091 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:23:19,092 - INFO - joeynmt.training - 	Hypothesis: <unk> anno che ho mostrato questi due <unk> così <unk> che il ghiaccio <unk> <unk>, che per la maggior parte delle <unk> <unk>, <unk> <unk>, <unk> <unk> <unk>, <unk> <unk> <unk>, <unk> <unk> <unk>, <unk> <unk>, <unk> <unk> <unk>, <unk> <unk> <unk>, <unk> <unk> <unk>, <unk> <unk> <unk>.
2024-05-11 19:23:19,092 - INFO - joeynmt.training - Example #1
2024-05-11 19:23:19,092 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:23:19,092 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:23:19,092 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', 'problema', 'di', 'questo', 'problema', 'perché', 'non', '<unk>', 'il', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:23:19,093 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:23:19,093 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:23:19,093 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il problema di questo problema perché non <unk> il <unk> del ghiaccio.
2024-05-11 19:23:19,094 - INFO - joeynmt.training - Example #2
2024-05-11 19:23:19,094 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:23:19,094 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:23:19,094 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', '<unk>', ',', 'in', 'un', 'senso', ',', 'il', '<unk>', 'del', 'sistema', '<unk>', 'globale', '.', '</s>']
2024-05-11 19:23:19,095 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:23:19,095 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:23:19,095 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è <unk>, in un senso, il <unk> del sistema <unk> globale.
2024-05-11 19:23:19,096 - INFO - joeynmt.training - Example #3
2024-05-11 19:23:19,096 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:23:19,096 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:23:19,096 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:23:19,096 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:23:19,097 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:23:19,097 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:23:19,097 - INFO - joeynmt.training - Example #4
2024-05-11 19:23:19,097 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:23:19,098 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:23:19,098 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', '<unk>', '<unk>', 'che', 'vi', '<unk>', 'un', '<unk>', 'di', 'quello', 'che', 'è', '<unk>', '<unk>', '<unk>', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:23:19,098 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:23:19,099 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:23:19,099 - INFO - joeynmt.training - 	Hypothesis: La <unk> <unk> che vi <unk> un <unk> di quello che è <unk> <unk> <unk> di quello che è successo negli ultimi 25 anni.
2024-05-11 19:23:21,350 - INFO - joeynmt.training - Epoch   1: total training loss 6722.30
2024-05-11 19:23:21,351 - INFO - joeynmt.training - EPOCH 2
2024-05-11 19:23:21,914 - INFO - joeynmt.training - Epoch   2, Step:     3100, Batch Loss:     1.606432, Batch Acc: 0.504333, Tokens per Sec:    23192, Lr: 0.000300
2024-05-11 19:23:24,614 - INFO - joeynmt.training - Epoch   2, Step:     3200, Batch Loss:     1.659640, Batch Acc: 0.500939, Tokens per Sec:    24453, Lr: 0.000300
2024-05-11 19:23:27,746 - INFO - joeynmt.training - Epoch   2, Step:     3300, Batch Loss:     1.808299, Batch Acc: 0.498957, Tokens per Sec:    20973, Lr: 0.000300
2024-05-11 19:23:30,821 - INFO - joeynmt.training - Epoch   2, Step:     3400, Batch Loss:     1.679608, Batch Acc: 0.497944, Tokens per Sec:    21592, Lr: 0.000300
2024-05-11 19:23:33,344 - INFO - joeynmt.training - Epoch   2, Step:     3500, Batch Loss:     1.596570, Batch Acc: 0.502797, Tokens per Sec:    25722, Lr: 0.000300
2024-05-11 19:23:33,345 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:23:33,345 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:23:40,465 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.70, ppl:   5.49, acc:   0.49, generation: 7.0339[sec], evaluation: 0.0000[sec]
2024-05-11 19:23:40,466 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:23:40,759 - INFO - joeynmt.helpers - delete word_level_model_moses/1000.ckpt
2024-05-11 19:23:40,791 - INFO - joeynmt.training - Example #0
2024-05-11 19:23:40,791 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:23:40,791 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:23:40,791 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'anno', 'ho', 'mostrato', 'queste', 'due', '<unk>', 'così', 'che', '<unk>', 'il', 'ghiaccio', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'delle', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'ha', '<unk>', '<unk>', 'da', '40', '%', '.', '</s>']
2024-05-11 19:23:40,792 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:23:40,792 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:23:40,793 - INFO - joeynmt.training - 	Hypothesis: <unk> anno ho mostrato queste due <unk> così che <unk> il ghiaccio che <unk> il ghiaccio <unk>, che per la maggior parte delle <unk> <unk>, <unk> <unk> <unk> <unk> <unk>, ha <unk> <unk> da 40%.
2024-05-11 19:23:40,793 - INFO - joeynmt.training - Example #1
2024-05-11 19:23:40,793 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:23:40,793 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:23:40,793 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'particolare', 'problema', 'perché', 'non', '<unk>', 'l&apos;', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:23:40,794 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:23:40,794 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:23:40,794 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo particolare problema perché non <unk> l' <unk> del ghiaccio.
2024-05-11 19:23:40,794 - INFO - joeynmt.training - Example #2
2024-05-11 19:23:40,795 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:23:40,795 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:23:40,795 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:23:40,796 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:23:40,796 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:23:40,796 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> <unk> è, in un senso, il cuore del sistema globale.
2024-05-11 19:23:40,796 - INFO - joeynmt.training - Example #3
2024-05-11 19:23:40,796 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:23:40,796 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:23:40,796 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:23:40,797 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:23:40,797 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:23:40,797 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:23:40,798 - INFO - joeynmt.training - Example #4
2024-05-11 19:23:40,798 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:23:40,798 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:23:40,798 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', '<unk>', '<unk>', 'che', 'vi', '<unk>', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:23:40,799 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:23:40,799 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:23:40,799 - INFO - joeynmt.training - 	Hypothesis: La <unk> <unk> che vi <unk> di quello che è successo negli ultimi 25 anni.
2024-05-11 19:23:44,276 - INFO - joeynmt.training - Epoch   2, Step:     3600, Batch Loss:     1.734770, Batch Acc: 0.502693, Tokens per Sec:    17348, Lr: 0.000300
2024-05-11 19:23:46,995 - INFO - joeynmt.training - Epoch   2, Step:     3700, Batch Loss:     1.645276, Batch Acc: 0.503934, Tokens per Sec:    24409, Lr: 0.000300
2024-05-11 19:23:49,799 - INFO - joeynmt.training - Epoch   2, Step:     3800, Batch Loss:     1.601643, Batch Acc: 0.505833, Tokens per Sec:    22819, Lr: 0.000300
2024-05-11 19:23:52,501 - INFO - joeynmt.training - Epoch   2, Step:     3900, Batch Loss:     1.636196, Batch Acc: 0.505833, Tokens per Sec:    23795, Lr: 0.000300
2024-05-11 19:23:55,429 - INFO - joeynmt.training - Epoch   2, Step:     4000, Batch Loss:     1.557492, Batch Acc: 0.509192, Tokens per Sec:    22450, Lr: 0.000300
2024-05-11 19:23:55,430 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:23:55,430 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:24:02,607 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.67, ppl:   5.29, acc:   0.50, generation: 7.1282[sec], evaluation: 0.0000[sec]
2024-05-11 19:24:02,608 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:24:02,836 - INFO - joeynmt.helpers - delete word_level_model_moses/1500.ckpt
2024-05-11 19:24:02,852 - INFO - joeynmt.training - Example #0
2024-05-11 19:24:02,852 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:24:02,853 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:24:02,853 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'anno', 'ho', 'mostrato', 'queste', 'due', '<unk>', 'così', 'che', '<unk>', 'il', 'ghiaccio', 'che', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stato', 'stato', 'stato', '<unk>', 'da', '<unk>', ',', '<unk>', 'da', '40', '%', '.', '</s>']
2024-05-11 19:24:02,853 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:24:02,854 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:24:02,854 - INFO - joeynmt.training - 	Hypothesis: <unk> anno ho mostrato queste due <unk> così che <unk> il ghiaccio che <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stato stato stato <unk> da <unk>, <unk> da 40%.
2024-05-11 19:24:02,854 - INFO - joeynmt.training - Example #1
2024-05-11 19:24:02,854 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:24:02,855 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:24:02,855 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', '<unk>', 'il', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:24:02,855 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:24:02,856 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:24:02,856 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo problema perché non <unk> il <unk> del ghiaccio.
2024-05-11 19:24:02,856 - INFO - joeynmt.training - Example #2
2024-05-11 19:24:02,856 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:24:02,856 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:24:02,857 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'senso', ',', 'il', '<unk>', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:24:02,857 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:24:02,857 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:24:02,858 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un senso, il <unk> del sistema globale.
2024-05-11 19:24:02,858 - INFO - joeynmt.training - Example #3
2024-05-11 19:24:02,858 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:24:02,858 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:24:02,858 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:24:02,859 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:24:02,859 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:24:02,859 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:24:02,860 - INFO - joeynmt.training - Example #4
2024-05-11 19:24:02,860 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:24:02,860 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:24:02,860 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', '<unk>', 'che', 'vi', '<unk>', 'una', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:24:02,861 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:24:02,861 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:24:02,861 - INFO - joeynmt.training - 	Hypothesis: La <unk> che vi <unk> una <unk> <unk> di ciò che è successo negli ultimi 25 anni.
2024-05-11 19:24:05,655 - INFO - joeynmt.training - Epoch   2, Step:     4100, Batch Loss:     1.520160, Batch Acc: 0.512776, Tokens per Sec:    21537, Lr: 0.000300
2024-05-11 19:24:08,615 - INFO - joeynmt.training - Epoch   2, Step:     4200, Batch Loss:     1.678791, Batch Acc: 0.507361, Tokens per Sec:    21668, Lr: 0.000300
2024-05-11 19:24:11,925 - INFO - joeynmt.training - Epoch   2, Step:     4300, Batch Loss:     1.618910, Batch Acc: 0.515306, Tokens per Sec:    20043, Lr: 0.000300
2024-05-11 19:24:14,549 - INFO - joeynmt.training - Epoch   2, Step:     4400, Batch Loss:     1.560776, Batch Acc: 0.515037, Tokens per Sec:    24408, Lr: 0.000300
2024-05-11 19:24:17,149 - INFO - joeynmt.training - Epoch   2, Step:     4500, Batch Loss:     1.499160, Batch Acc: 0.512182, Tokens per Sec:    24559, Lr: 0.000300
2024-05-11 19:24:17,149 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:24:17,150 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:24:25,269 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.64, ppl:   5.15, acc:   0.50, generation: 8.0727[sec], evaluation: 0.0000[sec]
2024-05-11 19:24:25,270 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:24:25,495 - INFO - joeynmt.helpers - delete word_level_model_moses/2000.ckpt
2024-05-11 19:24:25,512 - INFO - joeynmt.training - Example #0
2024-05-11 19:24:25,512 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:24:25,512 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:24:25,513 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'anno', 'che', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'così', 'che', 'il', 'ghiaccio', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'di', 'anni', 'è', 'stato', '<unk>', ',', 'ha', '<unk>', 'da', '40', '%', '.', '</s>']
2024-05-11 19:24:25,513 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:24:25,514 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:24:25,514 - INFO - joeynmt.training - 	Hypothesis: <unk> anno che ho mostrato questi due <unk> così che il ghiaccio <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione di anni è stato <unk>, ha <unk> da 40%.
2024-05-11 19:24:25,514 - INFO - joeynmt.training - Example #1
2024-05-11 19:24:25,514 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:24:25,514 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:24:25,515 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', '<unk>', 'il', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:24:25,515 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:24:25,515 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:24:25,516 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo problema perché non <unk> il <unk> del ghiaccio.
2024-05-11 19:24:25,516 - INFO - joeynmt.training - Example #2
2024-05-11 19:24:25,516 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:24:25,516 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:24:25,516 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'senso', ',', 'il', '<unk>', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:24:25,517 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:24:25,517 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:24:25,517 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un senso, il <unk> del sistema globale.
2024-05-11 19:24:25,517 - INFO - joeynmt.training - Example #3
2024-05-11 19:24:25,518 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:24:25,518 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:24:25,518 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:24:25,518 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:24:25,519 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:24:25,519 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:24:25,519 - INFO - joeynmt.training - Example #4
2024-05-11 19:24:25,520 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:24:25,520 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:24:25,520 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', 'mostrerò', 'un', '<unk>', 'di', 'cosa', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:24:25,520 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:24:25,521 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:24:25,521 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi mostrerò un <unk> di cosa è successo negli ultimi 25 anni.
2024-05-11 19:24:28,312 - INFO - joeynmt.training - Epoch   2, Step:     4600, Batch Loss:     1.595440, Batch Acc: 0.515905, Tokens per Sec:    21526, Lr: 0.000300
2024-05-11 19:24:30,970 - INFO - joeynmt.training - Epoch   2, Step:     4700, Batch Loss:     1.649522, Batch Acc: 0.519836, Tokens per Sec:    24857, Lr: 0.000300
2024-05-11 19:24:33,634 - INFO - joeynmt.training - Epoch   2, Step:     4800, Batch Loss:     1.956720, Batch Acc: 0.517527, Tokens per Sec:    23736, Lr: 0.000300
2024-05-11 19:24:36,836 - INFO - joeynmt.training - Epoch   2, Step:     4900, Batch Loss:     1.714207, Batch Acc: 0.519750, Tokens per Sec:    20828, Lr: 0.000300
2024-05-11 19:24:39,799 - INFO - joeynmt.training - Epoch   2, Step:     5000, Batch Loss:     1.727133, Batch Acc: 0.523479, Tokens per Sec:    22496, Lr: 0.000300
2024-05-11 19:24:39,800 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:24:39,800 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:24:46,082 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.61, ppl:   5.01, acc:   0.51, generation: 6.2338[sec], evaluation: 0.0000[sec]
2024-05-11 19:24:46,083 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:24:46,311 - INFO - joeynmt.helpers - delete word_level_model_moses/2500.ckpt
2024-05-11 19:24:46,332 - INFO - joeynmt.training - Example #0
2024-05-11 19:24:46,333 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:24:46,333 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:24:46,334 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'anno', 'ho', 'mostrato', 'questi', 'due', '<unk>', '<unk>', 'così', 'che', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'dei', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'dei', '<unk>', '<unk>', '<unk>', ',', 'ha', '<unk>', 'da', '40', '%', '.', '</s>']
2024-05-11 19:24:46,334 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:24:46,335 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:24:46,335 - INFO - joeynmt.training - 	Hypothesis: <unk> anno ho mostrato questi due <unk> <unk> così che il ghiaccio <unk> <unk>, che per la maggior parte dei <unk> <unk>, che per la maggior parte dei <unk> <unk> <unk>, ha <unk> da 40%.
2024-05-11 19:24:46,335 - INFO - joeynmt.training - Example #1
2024-05-11 19:24:46,335 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:24:46,335 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:24:46,336 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', '<unk>', 'il', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:24:46,336 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:24:46,336 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:24:46,336 - INFO - joeynmt.training - 	Hypothesis: Ma questo questo <unk> il <unk> di questo problema perché non <unk> il <unk> del ghiaccio.
2024-05-11 19:24:46,336 - INFO - joeynmt.training - Example #2
2024-05-11 19:24:46,336 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:24:46,336 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:24:46,337 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', '<unk>', 'è', 'il', 'ghiaccio', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:24:46,337 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:24:46,337 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:24:46,337 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> <unk> è il ghiaccio, in un certo senso, il cuore del sistema globale.
2024-05-11 19:24:46,337 - INFO - joeynmt.training - Example #3
2024-05-11 19:24:46,337 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:24:46,337 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:24:46,338 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:24:46,338 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:24:46,338 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:24:46,338 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:24:46,338 - INFO - joeynmt.training - Example #4
2024-05-11 19:24:46,338 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:24:46,338 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:24:46,338 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', '<unk>', 'un', '<unk>', '<unk>', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:24:46,339 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:24:46,339 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:24:46,339 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi <unk> un <unk> <unk> di quello che è successo negli ultimi 25 anni.
2024-05-11 19:24:49,342 - INFO - joeynmt.training - Epoch   2, Step:     5100, Batch Loss:     1.654410, Batch Acc: 0.524024, Tokens per Sec:    20110, Lr: 0.000300
2024-05-11 19:24:52,596 - INFO - joeynmt.training - Epoch   2, Step:     5200, Batch Loss:     1.495774, Batch Acc: 0.521127, Tokens per Sec:    20224, Lr: 0.000300
2024-05-11 19:24:55,332 - INFO - joeynmt.training - Epoch   2, Step:     5300, Batch Loss:     1.554979, Batch Acc: 0.518755, Tokens per Sec:    23447, Lr: 0.000300
2024-05-11 19:24:57,992 - INFO - joeynmt.training - Epoch   2, Step:     5400, Batch Loss:     1.499304, Batch Acc: 0.524893, Tokens per Sec:    24958, Lr: 0.000300
2024-05-11 19:25:00,646 - INFO - joeynmt.training - Epoch   2, Step:     5500, Batch Loss:     1.559775, Batch Acc: 0.528852, Tokens per Sec:    24506, Lr: 0.000300
2024-05-11 19:25:00,647 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:25:00,647 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:25:10,164 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.60, ppl:   4.95, acc:   0.52, generation: 9.4665[sec], evaluation: 0.0000[sec]
2024-05-11 19:25:10,165 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:25:10,397 - INFO - joeynmt.helpers - delete word_level_model_moses/3000.ckpt
2024-05-11 19:25:10,409 - INFO - joeynmt.training - Example #0
2024-05-11 19:25:10,410 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:25:10,410 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:25:10,410 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'anno', 'ho', 'mostrato', 'queste', 'due', '<unk>', 'così', 'che', 'il', 'ghiaccio', '<unk>', 'che', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', ',', 'sono', 'stati', '<unk>', 'da', '40', '%', '.', '</s>']
2024-05-11 19:25:10,411 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:25:10,412 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:25:10,412 - INFO - joeynmt.training - 	Hypothesis: <unk> anno ho mostrato queste due <unk> così che il ghiaccio <unk> che il ghiaccio <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni, sono stati <unk> da 40%.
2024-05-11 19:25:10,412 - INFO - joeynmt.training - Example #1
2024-05-11 19:25:10,412 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:25:10,413 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:25:10,413 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', '<unk>', 'il', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:25:10,413 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:25:10,414 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:25:10,414 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo problema perché non <unk> il <unk> del ghiaccio.
2024-05-11 19:25:10,414 - INFO - joeynmt.training - Example #2
2024-05-11 19:25:10,414 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:25:10,414 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:25:10,415 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:25:10,415 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:25:10,415 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:25:10,415 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un senso, il cuore del sistema globale.
2024-05-11 19:25:10,416 - INFO - joeynmt.training - Example #3
2024-05-11 19:25:10,416 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:25:10,416 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:25:10,416 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:25:10,417 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:25:10,417 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:25:10,417 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:25:10,418 - INFO - joeynmt.training - Example #4
2024-05-11 19:25:10,418 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:25:10,418 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:25:10,418 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:25:10,419 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:25:10,419 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:25:10,419 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi <unk> <unk> di ciò che è successo negli ultimi 25 anni.
2024-05-11 19:25:13,258 - INFO - joeynmt.training - Epoch   2, Step:     5600, Batch Loss:     1.576178, Batch Acc: 0.524286, Tokens per Sec:    20864, Lr: 0.000300
2024-05-11 19:25:16,005 - INFO - joeynmt.training - Epoch   2, Step:     5700, Batch Loss:     1.558526, Batch Acc: 0.523293, Tokens per Sec:    23298, Lr: 0.000300
2024-05-11 19:25:19,335 - INFO - joeynmt.training - Epoch   2, Step:     5800, Batch Loss:     1.667468, Batch Acc: 0.529230, Tokens per Sec:    19868, Lr: 0.000300
2024-05-11 19:25:22,027 - INFO - joeynmt.training - Epoch   2, Step:     5900, Batch Loss:     1.575609, Batch Acc: 0.527178, Tokens per Sec:    24122, Lr: 0.000300
2024-05-11 19:25:24,643 - INFO - joeynmt.training - Epoch   2, Step:     6000, Batch Loss:     1.563806, Batch Acc: 0.522304, Tokens per Sec:    24662, Lr: 0.000300
2024-05-11 19:25:24,643 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:25:24,644 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:25:32,871 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.57, ppl:   4.80, acc:   0.52, generation: 8.1770[sec], evaluation: 0.0000[sec]
2024-05-11 19:25:32,872 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:25:33,126 - INFO - joeynmt.helpers - delete word_level_model_moses/3500.ckpt
2024-05-11 19:25:33,142 - INFO - joeynmt.training - Example #0
2024-05-11 19:25:33,143 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:25:33,143 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:25:33,143 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'anno', 'ho', 'mostrato', 'queste', 'due', '<unk>', '<unk>', 'così', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'le', 'dimensioni', '<unk>', '<unk>', '<unk>', ',', 'ha', '<unk>', 'il', '40', '%', '.', '</s>']
2024-05-11 19:25:33,144 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:25:33,144 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:25:33,145 - INFO - joeynmt.training - 	Hypothesis: <unk> anno ho mostrato queste due <unk> <unk> così che <unk> il ghiaccio <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata le dimensioni <unk> <unk> <unk>, ha <unk> il 40%.
2024-05-11 19:25:33,145 - INFO - joeynmt.training - Example #1
2024-05-11 19:25:33,145 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:25:33,145 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:25:33,145 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', '<unk>', 'perché', 'non', '<unk>', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:25:33,146 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:25:33,146 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:25:33,146 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo problema <unk> perché non <unk> la <unk> del ghiaccio.
2024-05-11 19:25:33,147 - INFO - joeynmt.training - Example #2
2024-05-11 19:25:33,147 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:25:33,147 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:25:33,147 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', '<unk>', '<unk>', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:25:33,147 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:25:33,148 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:25:33,148 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un certo senso, il <unk> <unk> del sistema globale.
2024-05-11 19:25:33,148 - INFO - joeynmt.training - Example #3
2024-05-11 19:25:33,148 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:25:33,149 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:25:33,149 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:25:33,149 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:25:33,149 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:25:33,150 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> in <unk> e <unk> in estate.
2024-05-11 19:25:33,150 - INFO - joeynmt.training - Example #4
2024-05-11 19:25:33,150 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:25:33,150 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:25:33,150 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', '<unk>', 'un', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', '<unk>', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:25:33,151 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:25:33,151 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:25:33,151 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi <unk> un <unk> <unk> di ciò che è <unk> negli ultimi 25 anni.
2024-05-11 19:25:35,956 - INFO - joeynmt.training - Epoch   2, Step:     6100, Batch Loss:     1.350678, Batch Acc: 0.525784, Tokens per Sec:    21148, Lr: 0.000300
2024-05-11 19:25:37,307 - INFO - joeynmt.training - Epoch   2: total training loss 4916.16
2024-05-11 19:25:37,308 - INFO - joeynmt.training - EPOCH 3
2024-05-11 19:25:38,579 - INFO - joeynmt.training - Epoch   3, Step:     6200, Batch Loss:     1.423138, Batch Acc: 0.540731, Tokens per Sec:    25053, Lr: 0.000300
2024-05-11 19:25:41,362 - INFO - joeynmt.training - Epoch   3, Step:     6300, Batch Loss:     1.451474, Batch Acc: 0.546191, Tokens per Sec:    23111, Lr: 0.000300
2024-05-11 19:25:44,355 - INFO - joeynmt.training - Epoch   3, Step:     6400, Batch Loss:     1.333237, Batch Acc: 0.545268, Tokens per Sec:    21946, Lr: 0.000300
2024-05-11 19:25:47,348 - INFO - joeynmt.training - Epoch   3, Step:     6500, Batch Loss:     1.420394, Batch Acc: 0.541858, Tokens per Sec:    21980, Lr: 0.000300
2024-05-11 19:25:47,349 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:25:47,349 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:25:54,790 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.55, ppl:   4.73, acc:   0.53, generation: 7.3884[sec], evaluation: 0.0000[sec]
2024-05-11 19:25:54,791 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:25:55,014 - INFO - joeynmt.helpers - delete word_level_model_moses/4000.ckpt
2024-05-11 19:25:55,036 - INFO - joeynmt.training - Example #0
2024-05-11 19:25:55,036 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:25:55,036 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:25:55,037 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', '<unk>', 'così', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '<unk>', '<unk>', ',', '<unk>', 'da', '40', '%', '.', '</s>']
2024-05-11 19:25:55,037 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:25:55,038 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:25:55,038 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due <unk> così che <unk> il ghiaccio <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione del <unk> <unk>, <unk> da 40%.
2024-05-11 19:25:55,038 - INFO - joeynmt.training - Example #1
2024-05-11 19:25:55,038 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:25:55,039 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:25:55,039 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', '<unk>', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:25:55,039 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:25:55,039 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:25:55,040 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo problema perché non <unk> la <unk> del ghiaccio.
2024-05-11 19:25:55,040 - INFO - joeynmt.training - Example #2
2024-05-11 19:25:55,040 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:25:55,040 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:25:55,040 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'clima', 'globale', '.', '</s>']
2024-05-11 19:25:55,041 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:25:55,041 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:25:55,041 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un senso, il cuore del clima globale.
2024-05-11 19:25:55,042 - INFO - joeynmt.training - Example #3
2024-05-11 19:25:55,042 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:25:55,042 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:25:55,042 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:25:55,043 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:25:55,043 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:25:55,043 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:25:55,043 - INFO - joeynmt.training - Example #4
2024-05-11 19:25:55,044 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:25:55,044 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:25:55,044 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', 'mostrerò', 'un', '<unk>', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:25:55,044 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:25:55,045 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:25:55,045 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi mostrerò un <unk> di ciò che è successo negli ultimi 25 anni.
2024-05-11 19:25:58,218 - INFO - joeynmt.training - Epoch   3, Step:     6600, Batch Loss:     1.514369, Batch Acc: 0.543699, Tokens per Sec:    18659, Lr: 0.000300
2024-05-11 19:26:01,468 - INFO - joeynmt.training - Epoch   3, Step:     6700, Batch Loss:     1.716441, Batch Acc: 0.545850, Tokens per Sec:    20229, Lr: 0.000300
2024-05-11 19:26:04,126 - INFO - joeynmt.training - Epoch   3, Step:     6800, Batch Loss:     1.480389, Batch Acc: 0.543873, Tokens per Sec:    24765, Lr: 0.000300
2024-05-11 19:26:06,817 - INFO - joeynmt.training - Epoch   3, Step:     6900, Batch Loss:     1.499893, Batch Acc: 0.545096, Tokens per Sec:    24573, Lr: 0.000300
2024-05-11 19:26:09,461 - INFO - joeynmt.training - Epoch   3, Step:     7000, Batch Loss:     1.490152, Batch Acc: 0.544681, Tokens per Sec:    24642, Lr: 0.000300
2024-05-11 19:26:09,464 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:26:09,464 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:26:17,367 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.55, ppl:   4.69, acc:   0.53, generation: 7.8544[sec], evaluation: 0.0000[sec]
2024-05-11 19:26:17,368 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:26:17,590 - INFO - joeynmt.helpers - delete word_level_model_moses/4500.ckpt
2024-05-11 19:26:17,606 - INFO - joeynmt.training - Example #0
2024-05-11 19:26:17,607 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:26:17,607 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:26:17,607 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'anno', 'ho', 'mostrato', 'queste', 'due', '<unk>', 'così', '<unk>', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'le', 'dimensioni', '<unk>', ',', 'ha', '<unk>', 'il', '40', '%', '.', '</s>']
2024-05-11 19:26:17,608 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:26:17,608 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:26:17,608 - INFO - joeynmt.training - 	Hypothesis: <unk> anno ho mostrato queste due <unk> così <unk> che <unk> il ghiaccio <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata le dimensioni <unk>, ha <unk> il 40%.
2024-05-11 19:26:17,608 - INFO - joeynmt.training - Example #1
2024-05-11 19:26:17,609 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:26:17,609 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:26:17,609 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', '<unk>', 'il', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:26:17,609 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:26:17,610 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:26:17,610 - INFO - joeynmt.training - 	Hypothesis: Ma questa <unk> il <unk> di questo problema perché non <unk> il <unk> del ghiaccio.
2024-05-11 19:26:17,610 - INFO - joeynmt.training - Example #2
2024-05-11 19:26:17,610 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:26:17,610 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:26:17,611 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', '<unk>', '<unk>', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:26:17,611 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:26:17,611 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:26:17,612 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un certo senso, il <unk> <unk> del sistema globale.
2024-05-11 19:26:17,612 - INFO - joeynmt.training - Example #3
2024-05-11 19:26:17,612 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:26:17,612 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:26:17,612 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', 'estate', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:26:17,613 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:26:17,613 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:26:17,613 - INFO - joeynmt.training - 	Hypothesis: <unk> in estate <unk> e <unk> in estate.
2024-05-11 19:26:17,613 - INFO - joeynmt.training - Example #4
2024-05-11 19:26:17,614 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:26:17,614 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:26:17,614 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', 'mostrerò', 'una', '<unk>', '<unk>', 'di', 'quello', 'che', 'è', '<unk>', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:26:17,614 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:26:17,615 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:26:17,615 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi mostrerò una <unk> <unk> di quello che è <unk> negli ultimi 25 anni.
2024-05-11 19:26:20,468 - INFO - joeynmt.training - Epoch   3, Step:     7100, Batch Loss:     1.577939, Batch Acc: 0.546734, Tokens per Sec:    20747, Lr: 0.000300
2024-05-11 19:26:23,077 - INFO - joeynmt.training - Epoch   3, Step:     7200, Batch Loss:     1.500980, Batch Acc: 0.545948, Tokens per Sec:    25197, Lr: 0.000300
2024-05-11 19:26:25,818 - INFO - joeynmt.training - Epoch   3, Step:     7300, Batch Loss:     1.402919, Batch Acc: 0.545810, Tokens per Sec:    24006, Lr: 0.000300
2024-05-11 19:26:29,229 - INFO - joeynmt.training - Epoch   3, Step:     7400, Batch Loss:     1.442631, Batch Acc: 0.546371, Tokens per Sec:    19243, Lr: 0.000300
2024-05-11 19:26:31,811 - INFO - joeynmt.training - Epoch   3, Step:     7500, Batch Loss:     1.457239, Batch Acc: 0.547832, Tokens per Sec:    25330, Lr: 0.000300
2024-05-11 19:26:31,812 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:26:31,812 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:26:38,107 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.53, ppl:   4.62, acc:   0.53, generation: 6.2458[sec], evaluation: 0.0000[sec]
2024-05-11 19:26:38,108 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:26:38,336 - INFO - joeynmt.helpers - delete word_level_model_moses/5000.ckpt
2024-05-11 19:26:38,351 - INFO - joeynmt.training - Example #0
2024-05-11 19:26:38,352 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:26:38,352 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:26:38,352 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'anno', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'così', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stato', 'il', '40', '%', '.', '</s>']
2024-05-11 19:26:38,353 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:26:38,353 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:26:38,353 - INFO - joeynmt.training - 	Hypothesis: <unk> anno ho mostrato questi due <unk> così che <unk> il ghiaccio <unk>, che per la maggior parte degli ultimi tre milioni di anni è stato il 40%.
2024-05-11 19:26:38,354 - INFO - joeynmt.training - Example #1
2024-05-11 19:26:38,354 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:26:38,354 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:26:38,354 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', '<unk>', 'il', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:26:38,355 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:26:38,355 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:26:38,355 - INFO - joeynmt.training - 	Hypothesis: Ma questa <unk> il <unk> di questo problema perché non <unk> il <unk> del ghiaccio.
2024-05-11 19:26:38,355 - INFO - joeynmt.training - Example #2
2024-05-11 19:26:38,356 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:26:38,356 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:26:38,356 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:26:38,356 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:26:38,357 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:26:38,357 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio <unk> <unk> è, in un senso, il cuore del sistema globale.
2024-05-11 19:26:38,357 - INFO - joeynmt.training - Example #3
2024-05-11 19:26:38,357 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:26:38,357 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:26:38,358 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:26:38,358 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:26:38,358 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:26:38,359 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:26:38,359 - INFO - joeynmt.training - Example #4
2024-05-11 19:26:38,359 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:26:38,360 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:26:38,360 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', '<unk>', '<unk>', 'una', '<unk>', 'di', 'cosa', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:26:38,360 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:26:38,360 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:26:38,361 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi <unk> <unk> una <unk> di cosa è successo negli ultimi 25 anni.
2024-05-11 19:26:41,640 - INFO - joeynmt.training - Epoch   3, Step:     7600, Batch Loss:     1.510048, Batch Acc: 0.546066, Tokens per Sec:    18776, Lr: 0.000300
2024-05-11 19:26:44,643 - INFO - joeynmt.training - Epoch   3, Step:     7700, Batch Loss:     1.506163, Batch Acc: 0.548481, Tokens per Sec:    21988, Lr: 0.000300
2024-05-11 19:26:47,424 - INFO - joeynmt.training - Epoch   3, Step:     7800, Batch Loss:     1.435755, Batch Acc: 0.551549, Tokens per Sec:    23794, Lr: 0.000300
2024-05-11 19:26:50,114 - INFO - joeynmt.training - Epoch   3, Step:     7900, Batch Loss:     1.458351, Batch Acc: 0.543160, Tokens per Sec:    24321, Lr: 0.000300
2024-05-11 19:26:52,777 - INFO - joeynmt.training - Epoch   3, Step:     8000, Batch Loss:     1.510257, Batch Acc: 0.548912, Tokens per Sec:    24400, Lr: 0.000300
2024-05-11 19:26:52,777 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:26:52,778 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:27:00,212 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.52, ppl:   4.56, acc:   0.53, generation: 7.3867[sec], evaluation: 0.0000[sec]
2024-05-11 19:27:00,213 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:27:00,432 - INFO - joeynmt.helpers - delete word_level_model_moses/5500.ckpt
2024-05-11 19:27:00,448 - INFO - joeynmt.training - Example #0
2024-05-11 19:27:00,448 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:27:00,449 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:27:00,449 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'anno', 'ho', 'mostrato', 'queste', 'due', '<unk>', '<unk>', 'che', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'dei', 'tre', 'milioni', 'di', 'anni', 'è', 'stato', 'la', 'dimensione', 'del', '<unk>', '<unk>', ',', 'ha', '<unk>', 'il', '40', '%', '.', '</s>']
2024-05-11 19:27:00,450 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:27:00,450 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:27:00,451 - INFO - joeynmt.training - 	Hypothesis: <unk> anno ho mostrato queste due <unk> <unk> che il ghiaccio <unk> <unk>, che per la maggior parte dei tre milioni di anni è stato la dimensione del <unk> <unk>, ha <unk> il 40%.
2024-05-11 19:27:00,451 - INFO - joeynmt.training - Example #1
2024-05-11 19:27:00,451 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:27:00,451 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:27:00,452 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'la', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', '<unk>', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:27:00,452 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:27:00,452 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:27:00,453 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> la <unk> di questo problema perché non <unk> la <unk> del ghiaccio.
2024-05-11 19:27:00,453 - INFO - joeynmt.training - Example #2
2024-05-11 19:27:00,453 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:27:00,453 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:27:00,453 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', '<unk>', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:27:00,454 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:27:00,454 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:27:00,454 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un senso, il cuore <unk> del sistema globale.
2024-05-11 19:27:00,454 - INFO - joeynmt.training - Example #3
2024-05-11 19:27:00,455 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:27:00,455 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:27:00,455 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:27:00,455 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:27:00,456 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:27:00,456 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:27:00,456 - INFO - joeynmt.training - Example #4
2024-05-11 19:27:00,456 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:27:00,457 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:27:00,457 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', '<unk>', 'un', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:27:00,457 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:27:00,457 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:27:00,458 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi <unk> un <unk> <unk> di ciò che è successo negli ultimi 25 anni.
2024-05-11 19:27:03,278 - INFO - joeynmt.training - Epoch   3, Step:     8100, Batch Loss:     1.494078, Batch Acc: 0.544156, Tokens per Sec:    21069, Lr: 0.000300
2024-05-11 19:27:05,917 - INFO - joeynmt.training - Epoch   3, Step:     8200, Batch Loss:     1.466914, Batch Acc: 0.546961, Tokens per Sec:    24123, Lr: 0.000300
2024-05-11 19:27:08,983 - INFO - joeynmt.training - Epoch   3, Step:     8300, Batch Loss:     1.516670, Batch Acc: 0.542424, Tokens per Sec:    21477, Lr: 0.000300
2024-05-11 19:27:12,153 - INFO - joeynmt.training - Epoch   3, Step:     8400, Batch Loss:     1.497677, Batch Acc: 0.546350, Tokens per Sec:    20788, Lr: 0.000300
2024-05-11 19:27:14,722 - INFO - joeynmt.training - Epoch   3, Step:     8500, Batch Loss:     1.438664, Batch Acc: 0.549000, Tokens per Sec:    25006, Lr: 0.000300
2024-05-11 19:27:14,723 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:27:14,723 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:27:21,005 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.52, ppl:   4.55, acc:   0.53, generation: 6.1941[sec], evaluation: 0.0000[sec]
2024-05-11 19:27:21,006 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:27:21,285 - INFO - joeynmt.helpers - delete word_level_model_moses/6000.ckpt
2024-05-11 19:27:21,304 - INFO - joeynmt.training - Example #0
2024-05-11 19:27:21,305 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:27:21,305 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:27:21,305 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'anno', 'che', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'così', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stato', 'il', '40', '%', '.', '</s>']
2024-05-11 19:27:21,306 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:27:21,307 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:27:21,307 - INFO - joeynmt.training - 	Hypothesis: <unk> anno che ho mostrato questi due <unk> così che <unk> il ghiaccio <unk>, che per la maggior parte degli ultimi tre milioni di anni è stato il 40%.
2024-05-11 19:27:21,307 - INFO - joeynmt.training - Example #1
2024-05-11 19:27:21,308 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:27:21,308 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:27:21,308 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', ',', 'perché', 'non', '<unk>', 'il', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:27:21,309 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:27:21,309 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:27:21,309 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo problema, perché non <unk> il <unk> del ghiaccio.
2024-05-11 19:27:21,309 - INFO - joeynmt.training - Example #2
2024-05-11 19:27:21,309 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:27:21,309 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:27:21,309 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', '<unk>', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', '<unk>', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:27:21,310 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:27:21,310 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:27:21,310 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio <unk> è, in un senso, il cuore <unk> del sistema globale.
2024-05-11 19:27:21,311 - INFO - joeynmt.training - Example #3
2024-05-11 19:27:21,311 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:27:21,311 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:27:21,311 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:27:21,312 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:27:21,312 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:27:21,312 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:27:21,312 - INFO - joeynmt.training - Example #4
2024-05-11 19:27:21,312 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:27:21,313 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:27:21,313 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', 'mostrerò', 'una', '<unk>', '<unk>', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:27:21,313 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:27:21,314 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:27:21,314 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi mostrerò una <unk> <unk> di quello che è successo negli ultimi 25 anni.
2024-05-11 19:27:24,731 - INFO - joeynmt.training - Epoch   3, Step:     8600, Batch Loss:     1.353407, Batch Acc: 0.552313, Tokens per Sec:    17505, Lr: 0.000300
2024-05-11 19:27:27,414 - INFO - joeynmt.training - Epoch   3, Step:     8700, Batch Loss:     1.489080, Batch Acc: 0.551595, Tokens per Sec:    24123, Lr: 0.000300
2024-05-11 19:27:30,051 - INFO - joeynmt.training - Epoch   3, Step:     8800, Batch Loss:     1.528244, Batch Acc: 0.550905, Tokens per Sec:    24249, Lr: 0.000300
2024-05-11 19:27:32,721 - INFO - joeynmt.training - Epoch   3, Step:     8900, Batch Loss:     1.475166, Batch Acc: 0.547001, Tokens per Sec:    24117, Lr: 0.000300
2024-05-11 19:27:35,525 - INFO - joeynmt.training - Epoch   3, Step:     9000, Batch Loss:     1.449872, Batch Acc: 0.552433, Tokens per Sec:    23207, Lr: 0.000300
2024-05-11 19:27:35,526 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:27:35,526 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:27:42,932 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.49, ppl:   4.45, acc:   0.54, generation: 7.3582[sec], evaluation: 0.0000[sec]
2024-05-11 19:27:42,933 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:27:43,160 - INFO - joeynmt.helpers - delete word_level_model_moses/6500.ckpt
2024-05-11 19:27:43,175 - INFO - joeynmt.training - Example #0
2024-05-11 19:27:43,176 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:27:43,176 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:27:43,177 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'anno', 'anno', 'ho', 'mostrato', 'queste', 'due', '<unk>', 'così', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'dei', 'tre', 'milioni', 'di', 'anni', 'è', 'stato', 'la', 'dimensione', 'del', '<unk>', '<unk>', ',', 'ha', '<unk>', 'il', '40', '%', '.', '</s>']
2024-05-11 19:27:43,177 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:27:43,177 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:27:43,178 - INFO - joeynmt.training - 	Hypothesis: L'anno anno anno ho mostrato queste due <unk> così che <unk> il ghiaccio <unk> <unk>, che per la maggior parte dei tre milioni di anni è stato la dimensione del <unk> <unk>, ha <unk> il 40%.
2024-05-11 19:27:43,178 - INFO - joeynmt.training - Example #1
2024-05-11 19:27:43,178 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:27:43,178 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:27:43,178 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', '<unk>', 'perché', 'non', '<unk>', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:27:43,179 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:27:43,179 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:27:43,179 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo problema <unk> perché non <unk> la <unk> del ghiaccio.
2024-05-11 19:27:43,180 - INFO - joeynmt.training - Example #2
2024-05-11 19:27:43,180 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:27:43,180 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:27:43,180 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'senso', ',', 'il', '<unk>', '<unk>', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:27:43,181 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:27:43,181 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:27:43,181 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> <unk> è, in un senso, il <unk> <unk> del sistema globale.
2024-05-11 19:27:43,181 - INFO - joeynmt.training - Example #3
2024-05-11 19:27:43,181 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:27:43,182 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:27:43,182 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', 'estate', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:27:43,182 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:27:43,183 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:27:43,183 - INFO - joeynmt.training - 	Hypothesis: <unk> in estate <unk> e <unk> in estate.
2024-05-11 19:27:43,183 - INFO - joeynmt.training - Example #4
2024-05-11 19:27:43,183 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:27:43,183 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:27:43,184 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', '<unk>', 'un', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', '<unk>', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:27:43,184 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:27:43,184 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:27:43,185 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi <unk> un <unk> <unk> di ciò che è <unk> negli ultimi 25 anni.
2024-05-11 19:27:46,016 - INFO - joeynmt.training - Epoch   3, Step:     9100, Batch Loss:     1.548966, Batch Acc: 0.553099, Tokens per Sec:    21180, Lr: 0.000300
2024-05-11 19:27:48,842 - INFO - joeynmt.training - Epoch   3, Step:     9200, Batch Loss:     1.410805, Batch Acc: 0.553197, Tokens per Sec:    22621, Lr: 0.000300
2024-05-11 19:27:49,965 - INFO - joeynmt.training - Epoch   3: total training loss 4510.00
2024-05-11 19:27:49,965 - INFO - joeynmt.training - EPOCH 4
2024-05-11 19:27:52,402 - INFO - joeynmt.training - Epoch   4, Step:     9300, Batch Loss:     1.453766, Batch Acc: 0.565058, Tokens per Sec:    19722, Lr: 0.000300
2024-05-11 19:27:55,008 - INFO - joeynmt.training - Epoch   4, Step:     9400, Batch Loss:     1.405989, Batch Acc: 0.568681, Tokens per Sec:    25065, Lr: 0.000300
2024-05-11 19:27:57,603 - INFO - joeynmt.training - Epoch   4, Step:     9500, Batch Loss:     1.360790, Batch Acc: 0.568342, Tokens per Sec:    25173, Lr: 0.000300
2024-05-11 19:27:57,604 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:27:57,604 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:28:04,733 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.49, ppl:   4.43, acc:   0.54, generation: 7.0337[sec], evaluation: 0.0000[sec]
2024-05-11 19:28:04,734 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:28:05,024 - INFO - joeynmt.helpers - delete word_level_model_moses/7000.ckpt
2024-05-11 19:28:05,051 - INFO - joeynmt.training - Example #0
2024-05-11 19:28:05,052 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:28:05,052 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:28:05,052 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'così', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'della', 'dimensione', 'della', '<unk>', '<unk>', ',', 'ha', '<unk>', 'il', '40', '%', '.', '</s>']
2024-05-11 19:28:05,053 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:28:05,053 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:28:05,054 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due <unk> così che <unk> il ghiaccio <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione della dimensione della <unk> <unk>, ha <unk> il 40%.
2024-05-11 19:28:05,054 - INFO - joeynmt.training - Example #1
2024-05-11 19:28:05,054 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:28:05,054 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:28:05,054 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'la', '<unk>', 'di', 'questo', 'particolare', 'problema', 'perché', 'non', '<unk>', 'il', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:28:05,055 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:28:05,055 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:28:05,056 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> la <unk> di questo particolare problema perché non <unk> il <unk> del ghiaccio.
2024-05-11 19:28:05,056 - INFO - joeynmt.training - Example #2
2024-05-11 19:28:05,056 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:28:05,056 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:28:05,056 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', '<unk>', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:28:05,057 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:28:05,058 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:28:05,059 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un senso, il cuore <unk> del sistema globale.
2024-05-11 19:28:05,059 - INFO - joeynmt.training - Example #3
2024-05-11 19:28:05,059 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:28:05,059 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:28:05,059 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:28:05,060 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:28:05,060 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:28:05,061 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:28:05,061 - INFO - joeynmt.training - Example #4
2024-05-11 19:28:05,061 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:28:05,061 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:28:05,062 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', '<unk>', '<unk>', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:28:05,062 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:28:05,062 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:28:05,063 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi <unk> <unk> di quello che è successo negli ultimi 25 anni.
2024-05-11 19:28:08,048 - INFO - joeynmt.training - Epoch   4, Step:     9600, Batch Loss:     1.476695, Batch Acc: 0.566404, Tokens per Sec:    19807, Lr: 0.000300
2024-05-11 19:28:10,817 - INFO - joeynmt.training - Epoch   4, Step:     9700, Batch Loss:     1.490334, Batch Acc: 0.567553, Tokens per Sec:    24030, Lr: 0.000300
2024-05-11 19:28:13,517 - INFO - joeynmt.training - Epoch   4, Step:     9800, Batch Loss:     1.361845, Batch Acc: 0.566319, Tokens per Sec:    23443, Lr: 0.000300
2024-05-11 19:28:16,291 - INFO - joeynmt.training - Epoch   4, Step:     9900, Batch Loss:     1.324916, Batch Acc: 0.563835, Tokens per Sec:    23280, Lr: 0.000300
2024-05-11 19:28:19,344 - INFO - joeynmt.training - Epoch   4, Step:    10000, Batch Loss:     1.497754, Batch Acc: 0.561968, Tokens per Sec:    21698, Lr: 0.000300
2024-05-11 19:28:19,345 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:28:19,345 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:28:25,937 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.49, ppl:   4.42, acc:   0.54, generation: 6.5415[sec], evaluation: 0.0000[sec]
2024-05-11 19:28:25,938 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:28:26,184 - INFO - joeynmt.helpers - delete word_level_model_moses/7500.ckpt
2024-05-11 19:28:26,201 - INFO - joeynmt.training - Example #0
2024-05-11 19:28:26,201 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:28:26,202 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:28:26,202 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'anno', 'anno', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'così', 'che', '<unk>', 'che', 'il', 'ghiaccio', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'delle', 'tre', 'milioni', 'di', 'anni', 'è', 'stato', 'la', 'dimensione', 'degli', '<unk>', '<unk>', ',', 'ha', '<unk>', 'dal', '40', '%', '.', '</s>']
2024-05-11 19:28:26,203 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:28:26,203 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:28:26,203 - INFO - joeynmt.training - 	Hypothesis: L'anno anno anno ho mostrato questi due <unk> così che <unk> che il ghiaccio <unk>, che per la maggior parte delle tre milioni di anni è stato la dimensione degli <unk> <unk>, ha <unk> dal 40%.
2024-05-11 19:28:26,204 - INFO - joeynmt.training - Example #1
2024-05-11 19:28:26,204 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:28:26,204 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:28:26,204 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', '<unk>', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:28:26,205 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:28:26,205 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:28:26,205 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo problema perché non <unk> la <unk> del ghiaccio.
2024-05-11 19:28:26,206 - INFO - joeynmt.training - Example #2
2024-05-11 19:28:26,206 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:28:26,206 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:28:26,206 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:28:26,207 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:28:26,207 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:28:26,207 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un certo senso, il cuore del sistema globale.
2024-05-11 19:28:26,207 - INFO - joeynmt.training - Example #3
2024-05-11 19:28:26,208 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:28:26,208 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:28:26,208 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', 'estate', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:28:26,208 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:28:26,209 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:28:26,209 - INFO - joeynmt.training - 	Hypothesis: <unk> in estate e <unk> in estate.
2024-05-11 19:28:26,209 - INFO - joeynmt.training - Example #4
2024-05-11 19:28:26,209 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:28:26,210 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:28:26,210 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:28:26,210 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:28:26,211 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:28:26,211 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi <unk> <unk> di ciò che è successo negli ultimi 25 anni.
2024-05-11 19:28:29,077 - INFO - joeynmt.training - Epoch   4, Step:    10100, Batch Loss:     1.438598, Batch Acc: 0.563300, Tokens per Sec:    20591, Lr: 0.000300
2024-05-11 19:28:32,501 - INFO - joeynmt.training - Epoch   4, Step:    10200, Batch Loss:     1.356444, Batch Acc: 0.565723, Tokens per Sec:    19003, Lr: 0.000300
2024-05-11 19:28:35,221 - INFO - joeynmt.training - Epoch   4, Step:    10300, Batch Loss:     1.314456, Batch Acc: 0.564658, Tokens per Sec:    23640, Lr: 0.000300
2024-05-11 19:28:37,997 - INFO - joeynmt.training - Epoch   4, Step:    10400, Batch Loss:     1.485369, Batch Acc: 0.565988, Tokens per Sec:    23462, Lr: 0.000300
2024-05-11 19:28:40,588 - INFO - joeynmt.training - Epoch   4, Step:    10500, Batch Loss:     1.314687, Batch Acc: 0.568550, Tokens per Sec:    25578, Lr: 0.000300
2024-05-11 19:28:40,589 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:28:40,589 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:28:47,682 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.47, ppl:   4.36, acc:   0.54, generation: 7.0420[sec], evaluation: 0.0000[sec]
2024-05-11 19:28:47,683 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:28:47,915 - INFO - joeynmt.helpers - delete word_level_model_moses/8000.ckpt
2024-05-11 19:28:47,933 - INFO - joeynmt.training - Example #0
2024-05-11 19:28:47,933 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:28:47,934 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:28:47,934 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'anno', 'ho', 'mostrato', 'queste', 'due', '<unk>', 'così', 'che', '<unk>', 'che', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'dei', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'dei', '<unk>', '<unk>', '<unk>', ',', 'ha', '<unk>', 'dal', '40', '%', '.', '</s>']
2024-05-11 19:28:47,934 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:28:47,935 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:28:47,935 - INFO - joeynmt.training - 	Hypothesis: <unk> anno ho mostrato queste due <unk> così che <unk> che il ghiaccio <unk> <unk>, che per la maggior parte dei tre milioni di anni è stata la dimensione dei <unk> <unk> <unk>, ha <unk> dal 40%.
2024-05-11 19:28:47,935 - INFO - joeynmt.training - Example #1
2024-05-11 19:28:47,935 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:28:47,936 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:28:47,936 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', '<unk>', 'la', '<unk>', 'di', 'questo', 'particolare', 'problema', 'perché', 'non', 'mostra', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:28:47,936 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:28:47,936 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:28:47,937 - INFO - joeynmt.training - 	Hypothesis: Ma questa <unk> la <unk> di questo particolare problema perché non mostra la <unk> del ghiaccio.
2024-05-11 19:28:47,937 - INFO - joeynmt.training - Example #2
2024-05-11 19:28:47,937 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:28:47,938 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:28:47,938 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:28:47,938 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:28:47,939 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:28:47,939 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un senso, il cuore del sistema globale.
2024-05-11 19:28:47,939 - INFO - joeynmt.training - Example #3
2024-05-11 19:28:47,939 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:28:47,940 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:28:47,940 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:28:47,940 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:28:47,940 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:28:47,941 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:28:47,941 - INFO - joeynmt.training - Example #4
2024-05-11 19:28:47,941 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:28:47,941 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:28:47,941 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'vi', 'mostrerò', 'una', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', '<unk>', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:28:47,942 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:28:47,943 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:28:47,943 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> vi mostrerò una <unk> <unk> di ciò che è <unk> negli ultimi 25 anni.
2024-05-11 19:28:50,805 - INFO - joeynmt.training - Epoch   4, Step:    10600, Batch Loss:     1.279617, Batch Acc: 0.568026, Tokens per Sec:    21277, Lr: 0.000300
2024-05-11 19:28:53,454 - INFO - joeynmt.training - Epoch   4, Step:    10700, Batch Loss:     1.465598, Batch Acc: 0.566854, Tokens per Sec:    24540, Lr: 0.000300
2024-05-11 19:28:56,240 - INFO - joeynmt.training - Epoch   4, Step:    10800, Batch Loss:     1.359407, Batch Acc: 0.562029, Tokens per Sec:    23660, Lr: 0.000300
2024-05-11 19:28:59,574 - INFO - joeynmt.training - Epoch   4, Step:    10900, Batch Loss:     1.456783, Batch Acc: 0.564093, Tokens per Sec:    19491, Lr: 0.000300
2024-05-11 19:29:02,164 - INFO - joeynmt.training - Epoch   4, Step:    11000, Batch Loss:     1.444220, Batch Acc: 0.564304, Tokens per Sec:    25121, Lr: 0.000300
2024-05-11 19:29:02,165 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:29:02,165 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:29:08,500 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.48, ppl:   4.37, acc:   0.54, generation: 6.2873[sec], evaluation: 0.0000[sec]
2024-05-11 19:29:08,719 - INFO - joeynmt.helpers - delete word_level_model_moses/8500.ckpt
2024-05-11 19:29:08,735 - INFO - joeynmt.training - Example #0
2024-05-11 19:29:08,735 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:29:08,736 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:29:08,736 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'anno', 'ho', 'mostrato', 'questi', 'due', '<unk>', '<unk>', 'così', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', '<unk>', '<unk>', ',', 'ha', '<unk>', 'dal', '40', '%', '.', '</s>']
2024-05-11 19:29:08,736 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:29:08,737 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:29:08,737 - INFO - joeynmt.training - 	Hypothesis: <unk> anno ho mostrato questi due <unk> <unk> così che <unk> il ghiaccio <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione degli <unk> <unk>, ha <unk> dal 40%.
2024-05-11 19:29:08,737 - INFO - joeynmt.training - Example #1
2024-05-11 19:29:08,738 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:29:08,738 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:29:08,738 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'la', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', '<unk>', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:29:08,738 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:29:08,739 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:29:08,739 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> la <unk> di questo problema perché non <unk> la <unk> del ghiaccio.
2024-05-11 19:29:08,739 - INFO - joeynmt.training - Example #2
2024-05-11 19:29:08,739 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:29:08,739 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:29:08,740 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', 'globale', '.', '</s>']
2024-05-11 19:29:08,740 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:29:08,740 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:29:08,741 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio <unk> <unk> è, in un certo senso, il cuore del sistema climatico globale.
2024-05-11 19:29:08,741 - INFO - joeynmt.training - Example #3
2024-05-11 19:29:08,741 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:29:08,741 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:29:08,741 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:29:08,742 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:29:08,742 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:29:08,742 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:29:08,743 - INFO - joeynmt.training - Example #4
2024-05-11 19:29:08,743 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:29:08,743 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:29:08,743 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', '<unk>', '<unk>', 'un', '<unk>', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:29:08,744 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:29:08,744 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:29:08,744 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi <unk> <unk> un <unk> di quello che è successo negli ultimi 25 anni.
2024-05-11 19:29:11,960 - INFO - joeynmt.training - Epoch   4, Step:    11100, Batch Loss:     1.424979, Batch Acc: 0.566555, Tokens per Sec:    19644, Lr: 0.000300
2024-05-11 19:29:15,017 - INFO - joeynmt.training - Epoch   4, Step:    11200, Batch Loss:     1.398187, Batch Acc: 0.564126, Tokens per Sec:    21046, Lr: 0.000300
2024-05-11 19:29:17,788 - INFO - joeynmt.training - Epoch   4, Step:    11300, Batch Loss:     1.339805, Batch Acc: 0.565189, Tokens per Sec:    23426, Lr: 0.000300
2024-05-11 19:29:20,457 - INFO - joeynmt.training - Epoch   4, Step:    11400, Batch Loss:     1.453427, Batch Acc: 0.564823, Tokens per Sec:    24726, Lr: 0.000300
2024-05-11 19:29:23,047 - INFO - joeynmt.training - Epoch   4, Step:    11500, Batch Loss:     1.356405, Batch Acc: 0.560848, Tokens per Sec:    24955, Lr: 0.000300
2024-05-11 19:29:23,048 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:29:23,048 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:29:31,973 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.46, ppl:   4.33, acc:   0.55, generation: 8.8745[sec], evaluation: 0.0000[sec]
2024-05-11 19:29:31,974 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:29:32,210 - INFO - joeynmt.helpers - delete word_level_model_moses/9000.ckpt
2024-05-11 19:29:32,223 - INFO - joeynmt.training - Example #0
2024-05-11 19:29:32,224 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:29:32,224 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:29:32,225 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'questi', 'due', '<unk>', '<unk>', 'così', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', '<unk>', 'come', '<unk>', '<unk>', '<unk>', ',', 'ha', '<unk>', 'da', '40', '%', '.', '</s>']
2024-05-11 19:29:32,225 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:29:32,226 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:29:32,226 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due <unk> <unk> così che <unk> il ghiaccio <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata <unk> come <unk> <unk> <unk>, ha <unk> da 40%.
2024-05-11 19:29:32,226 - INFO - joeynmt.training - Example #1
2024-05-11 19:29:32,226 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:29:32,227 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:29:32,227 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'la', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:29:32,227 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:29:32,228 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:29:32,228 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> la <unk> di questo problema perché non mostra la <unk> del ghiaccio.
2024-05-11 19:29:32,228 - INFO - joeynmt.training - Example #2
2024-05-11 19:29:32,228 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:29:32,228 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:29:32,229 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'del', 'sistema', '<unk>', 'globale', '.', '</s>']
2024-05-11 19:29:32,229 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:29:32,229 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:29:32,230 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un certo senso, il cuore del sistema <unk> globale.
2024-05-11 19:29:32,230 - INFO - joeynmt.training - Example #3
2024-05-11 19:29:32,230 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:29:32,230 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:29:32,230 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:29:32,231 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:29:32,231 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:29:32,231 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:29:32,231 - INFO - joeynmt.training - Example #4
2024-05-11 19:29:32,232 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:29:32,232 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:29:32,232 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', 'mostrerò', 'un', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', '<unk>', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:29:32,232 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:29:32,233 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:29:32,233 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi mostrerò un <unk> <unk> di ciò che è <unk> negli ultimi 25 anni.
2024-05-11 19:29:35,023 - INFO - joeynmt.training - Epoch   4, Step:    11600, Batch Loss:     1.424642, Batch Acc: 0.570148, Tokens per Sec:    21287, Lr: 0.000300
2024-05-11 19:29:37,734 - INFO - joeynmt.training - Epoch   4, Step:    11700, Batch Loss:     1.285627, Batch Acc: 0.562800, Tokens per Sec:    23893, Lr: 0.000300
2024-05-11 19:29:40,392 - INFO - joeynmt.training - Epoch   4, Step:    11800, Batch Loss:     1.392299, Batch Acc: 0.561579, Tokens per Sec:    23806, Lr: 0.000300
2024-05-11 19:29:43,800 - INFO - joeynmt.training - Epoch   4, Step:    11900, Batch Loss:     1.338915, Batch Acc: 0.564725, Tokens per Sec:    19189, Lr: 0.000300
2024-05-11 19:29:46,506 - INFO - joeynmt.training - Epoch   4, Step:    12000, Batch Loss:     1.428004, Batch Acc: 0.568052, Tokens per Sec:    24744, Lr: 0.000300
2024-05-11 19:29:46,507 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:29:46,507 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:29:53,427 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.46, ppl:   4.30, acc:   0.55, generation: 6.8728[sec], evaluation: 0.0000[sec]
2024-05-11 19:29:53,428 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:29:53,680 - INFO - joeynmt.helpers - delete word_level_model_moses/9500.ckpt
2024-05-11 19:29:53,697 - INFO - joeynmt.training - Example #0
2024-05-11 19:29:53,697 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:29:53,697 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:29:53,698 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'anno', 'ho', 'mostrato', 'questi', 'due', '<unk>', '<unk>', 'così', 'che', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'della', '<unk>', '<unk>', '<unk>', ',', 'ha', '<unk>', 'il', '40', '%', '.', '</s>']
2024-05-11 19:29:53,698 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:29:53,699 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:29:53,699 - INFO - joeynmt.training - 	Hypothesis: <unk> anno ho mostrato questi due <unk> <unk> così che il ghiaccio <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione della <unk> <unk> <unk>, ha <unk> il 40%.
2024-05-11 19:29:53,699 - INFO - joeynmt.training - Example #1
2024-05-11 19:29:53,700 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:29:53,700 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:29:53,700 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', '<unk>', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:29:53,700 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:29:53,701 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:29:53,701 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo problema perché non <unk> la <unk> del ghiaccio.
2024-05-11 19:29:53,701 - INFO - joeynmt.training - Example #2
2024-05-11 19:29:53,701 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:29:53,702 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:29:53,702 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', 'globale', '.', '</s>']
2024-05-11 19:29:53,702 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:29:53,702 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:29:53,703 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un certo senso, il cuore del sistema climatico globale.
2024-05-11 19:29:53,703 - INFO - joeynmt.training - Example #3
2024-05-11 19:29:53,703 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:29:53,703 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:29:53,704 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:29:53,704 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:29:53,704 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:29:53,705 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> in estate.
2024-05-11 19:29:53,705 - INFO - joeynmt.training - Example #4
2024-05-11 19:29:53,705 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:29:53,705 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:29:53,706 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', 'mostrerò', 'un', '<unk>', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:29:53,706 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:29:53,706 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:29:53,707 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi mostrerò un <unk> di ciò che è successo negli ultimi 25 anni.
2024-05-11 19:29:57,288 - INFO - joeynmt.training - Epoch   4, Step:    12100, Batch Loss:     1.382413, Batch Acc: 0.564016, Tokens per Sec:    17323, Lr: 0.000300
2024-05-11 19:30:00,055 - INFO - joeynmt.training - Epoch   4, Step:    12200, Batch Loss:     1.393204, Batch Acc: 0.564848, Tokens per Sec:    22869, Lr: 0.000300
2024-05-11 19:30:02,748 - INFO - joeynmt.training - Epoch   4: total training loss 4272.42
2024-05-11 19:30:02,749 - INFO - joeynmt.training - EPOCH 5
2024-05-11 19:30:02,858 - INFO - joeynmt.training - Epoch   5, Step:    12300, Batch Loss:     1.424907, Batch Acc: 0.579275, Tokens per Sec:    23645, Lr: 0.000300
2024-05-11 19:30:05,394 - INFO - joeynmt.training - Epoch   5, Step:    12400, Batch Loss:     1.363050, Batch Acc: 0.582931, Tokens per Sec:    24725, Lr: 0.000300
2024-05-11 19:30:08,084 - INFO - joeynmt.training - Epoch   5, Step:    12500, Batch Loss:     1.380849, Batch Acc: 0.586010, Tokens per Sec:    24513, Lr: 0.000300
2024-05-11 19:30:08,085 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:30:08,085 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:30:14,784 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.46, ppl:   4.30, acc:   0.55, generation: 6.6366[sec], evaluation: 0.0000[sec]
2024-05-11 19:30:15,005 - INFO - joeynmt.helpers - delete word_level_model_moses/10000.ckpt
2024-05-11 19:30:15,017 - INFO - joeynmt.training - Example #0
2024-05-11 19:30:15,018 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:30:15,018 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:30:15,019 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'anno', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'così', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'dell&apos;', '<unk>', '<unk>', ',', 'ha', '<unk>', 'il', '40', '%', '.', '</s>']
2024-05-11 19:30:15,020 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:30:15,020 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:30:15,020 - INFO - joeynmt.training - 	Hypothesis: <unk> anno ho mostrato questi due <unk> così che <unk> il ghiaccio <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dell' <unk> <unk>, ha <unk> il 40%.
2024-05-11 19:30:15,021 - INFO - joeynmt.training - Example #1
2024-05-11 19:30:15,021 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:30:15,021 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:30:15,021 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', '<unk>', 'la', '<unk>', 'di', 'questo', 'problema', 'particolare', 'perché', 'non', 'mostra', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:30:15,022 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:30:15,022 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:30:15,022 - INFO - joeynmt.training - 	Hypothesis: Ma questa <unk> la <unk> di questo problema particolare perché non mostra la <unk> del ghiaccio.
2024-05-11 19:30:15,023 - INFO - joeynmt.training - Example #2
2024-05-11 19:30:15,023 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:30:15,023 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:30:15,023 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:30:15,024 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:30:15,024 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:30:15,024 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un certo senso, il cuore del sistema globale.
2024-05-11 19:30:15,024 - INFO - joeynmt.training - Example #3
2024-05-11 19:30:15,025 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:30:15,025 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:30:15,025 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', 'estate', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:30:15,026 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:30:15,026 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:30:15,026 - INFO - joeynmt.training - 	Hypothesis: <unk> in estate <unk> e <unk> in estate.
2024-05-11 19:30:15,027 - INFO - joeynmt.training - Example #4
2024-05-11 19:30:15,027 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:30:15,027 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:30:15,027 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', 'mostro', '<unk>', 'un', '<unk>', '<unk>', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:30:15,028 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:30:15,028 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:30:15,028 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi mostro <unk> un <unk> <unk> di quello che è successo negli ultimi 25 anni.
2024-05-11 19:30:17,932 - INFO - joeynmt.training - Epoch   5, Step:    12600, Batch Loss:     1.435865, Batch Acc: 0.583165, Tokens per Sec:    20922, Lr: 0.000300
2024-05-11 19:30:20,673 - INFO - joeynmt.training - Epoch   5, Step:    12700, Batch Loss:     1.283635, Batch Acc: 0.583582, Tokens per Sec:    24068, Lr: 0.000300
2024-05-11 19:30:23,844 - INFO - joeynmt.training - Epoch   5, Step:    12800, Batch Loss:     1.336489, Batch Acc: 0.580267, Tokens per Sec:    20058, Lr: 0.000300
2024-05-11 19:30:26,836 - INFO - joeynmt.training - Epoch   5, Step:    12900, Batch Loss:     1.442802, Batch Acc: 0.582413, Tokens per Sec:    21861, Lr: 0.000300
2024-05-11 19:30:29,488 - INFO - joeynmt.training - Epoch   5, Step:    13000, Batch Loss:     1.337566, Batch Acc: 0.582040, Tokens per Sec:    24357, Lr: 0.000300
2024-05-11 19:30:29,489 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:30:29,489 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:30:36,487 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.46, ppl:   4.29, acc:   0.55, generation: 6.9091[sec], evaluation: 0.0000[sec]
2024-05-11 19:30:36,488 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:30:36,771 - INFO - joeynmt.helpers - delete word_level_model_moses/11000.ckpt
2024-05-11 19:30:36,799 - INFO - joeynmt.training - Example #0
2024-05-11 19:30:36,800 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:30:36,800 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:30:36,801 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'anno', 'ho', 'mostrato', 'questi', 'due', '<unk>', '<unk>', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stato', '<unk>', 'il', '40', '%', 'del', '40', '%', '.', '</s>']
2024-05-11 19:30:36,801 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:30:36,802 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:30:36,802 - INFO - joeynmt.training - 	Hypothesis: <unk> anno ho mostrato questi due <unk> <unk> che <unk> il ghiaccio <unk>, che per la maggior parte degli ultimi tre milioni di anni è stato <unk> il 40% del 40%.
2024-05-11 19:30:36,802 - INFO - joeynmt.training - Example #1
2024-05-11 19:30:36,802 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:30:36,802 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:30:36,802 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:30:36,803 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:30:36,803 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:30:36,804 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo problema perché non mostra la <unk> del ghiaccio.
2024-05-11 19:30:36,804 - INFO - joeynmt.training - Example #2
2024-05-11 19:30:36,804 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:30:36,804 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:30:36,804 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:30:36,805 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:30:36,805 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:30:36,805 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un senso, il cuore del sistema globale.
2024-05-11 19:30:36,805 - INFO - joeynmt.training - Example #3
2024-05-11 19:30:36,805 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:30:36,806 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:30:36,806 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', 'estate', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:30:36,806 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:30:36,806 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:30:36,807 - INFO - joeynmt.training - 	Hypothesis: <unk> in estate <unk> e <unk> in estate.
2024-05-11 19:30:36,807 - INFO - joeynmt.training - Example #4
2024-05-11 19:30:36,807 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:30:36,807 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:30:36,807 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', '<unk>', '<unk>', 'un', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:30:36,808 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:30:36,808 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:30:36,808 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi <unk> <unk> un <unk> <unk> di ciò che è successo negli ultimi 25 anni.
2024-05-11 19:30:40,154 - INFO - joeynmt.training - Epoch   5, Step:    13100, Batch Loss:     1.276960, Batch Acc: 0.577004, Tokens per Sec:    17735, Lr: 0.000300
2024-05-11 19:30:42,823 - INFO - joeynmt.training - Epoch   5, Step:    13200, Batch Loss:     1.378775, Batch Acc: 0.580694, Tokens per Sec:    24353, Lr: 0.000300
2024-05-11 19:30:45,478 - INFO - joeynmt.training - Epoch   5, Step:    13300, Batch Loss:     1.288687, Batch Acc: 0.578498, Tokens per Sec:    25046, Lr: 0.000300
2024-05-11 19:30:48,191 - INFO - joeynmt.training - Epoch   5, Step:    13400, Batch Loss:     1.352370, Batch Acc: 0.574649, Tokens per Sec:    24337, Lr: 0.000300
2024-05-11 19:30:51,201 - INFO - joeynmt.training - Epoch   5, Step:    13500, Batch Loss:     1.270667, Batch Acc: 0.577859, Tokens per Sec:    21922, Lr: 0.000300
2024-05-11 19:30:51,201 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:30:51,202 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:30:57,826 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.45, ppl:   4.26, acc:   0.55, generation: 6.5755[sec], evaluation: 0.0000[sec]
2024-05-11 19:30:57,827 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:30:58,066 - INFO - joeynmt.helpers - delete word_level_model_moses/10500.ckpt
2024-05-11 19:30:58,083 - INFO - joeynmt.training - Example #0
2024-05-11 19:30:58,084 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:30:58,084 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:30:58,084 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'anno', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'così', 'che', '<unk>', 'che', 'il', 'ghiaccio', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', 'stati', '<unk>', '<unk>', ',', 'ha', '<unk>', 'dal', '40', '%', '.', '</s>']
2024-05-11 19:30:58,084 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:30:58,085 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:30:58,085 - INFO - joeynmt.training - 	Hypothesis: <unk> anno ho mostrato questi due <unk> così che <unk> che il ghiaccio <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione degli stati <unk> <unk>, ha <unk> dal 40%.
2024-05-11 19:30:58,085 - INFO - joeynmt.training - Example #1
2024-05-11 19:30:58,086 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:30:58,086 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:30:58,086 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', 'problema', 'di', 'questo', 'particolare', 'problema', 'perché', 'non', 'mostra', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:30:58,087 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:30:58,087 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:30:58,087 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il problema di questo particolare problema perché non mostra la <unk> del ghiaccio.
2024-05-11 19:30:58,087 - INFO - joeynmt.training - Example #2
2024-05-11 19:30:58,088 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:30:58,088 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:30:58,088 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:30:58,088 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:30:58,089 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:30:58,089 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio <unk> <unk> è, in un senso, il cuore del sistema globale.
2024-05-11 19:30:58,089 - INFO - joeynmt.training - Example #3
2024-05-11 19:30:58,089 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:30:58,090 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:30:58,090 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:30:58,090 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:30:58,091 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:30:58,091 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:30:58,091 - INFO - joeynmt.training - Example #4
2024-05-11 19:30:58,091 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:30:58,091 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:30:58,092 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', 'mostrerò', 'un', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:30:58,092 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:30:58,092 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:30:58,093 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi mostrerò un <unk> <unk> di ciò che è successo negli ultimi 25 anni.
2024-05-11 19:31:00,893 - INFO - joeynmt.training - Epoch   5, Step:    13600, Batch Loss:     1.342777, Batch Acc: 0.576821, Tokens per Sec:    21596, Lr: 0.000300
2024-05-11 19:31:03,676 - INFO - joeynmt.training - Epoch   5, Step:    13700, Batch Loss:     1.472632, Batch Acc: 0.578806, Tokens per Sec:    23339, Lr: 0.000300
2024-05-11 19:31:07,186 - INFO - joeynmt.training - Epoch   5, Step:    13800, Batch Loss:     1.207963, Batch Acc: 0.582247, Tokens per Sec:    18851, Lr: 0.000300
2024-05-11 19:31:09,829 - INFO - joeynmt.training - Epoch   5, Step:    13900, Batch Loss:     1.453997, Batch Acc: 0.576048, Tokens per Sec:    24897, Lr: 0.000300
2024-05-11 19:31:12,408 - INFO - joeynmt.training - Epoch   5, Step:    14000, Batch Loss:     1.316062, Batch Acc: 0.576422, Tokens per Sec:    24928, Lr: 0.000300
2024-05-11 19:31:12,408 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:31:12,409 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:31:19,871 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.45, ppl:   4.28, acc:   0.55, generation: 7.3662[sec], evaluation: 0.0000[sec]
2024-05-11 19:31:20,143 - INFO - joeynmt.helpers - delete word_level_model_moses/11500.ckpt
2024-05-11 19:31:20,162 - INFO - joeynmt.training - Example #0
2024-05-11 19:31:20,163 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:31:20,163 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:31:20,163 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'così', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', 'che', 'per', 'la', 'maggior', 'parte', 'dei', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'delle', 'dimensioni', 'del', '40', '%', 'del', '<unk>', '<unk>', ',', 'ha', '<unk>', 'da', '40', '%', '.', '</s>']
2024-05-11 19:31:20,164 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:31:20,165 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:31:20,165 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due <unk> così che <unk> il ghiaccio <unk> che per la maggior parte dei tre milioni di anni è stata la dimensione delle dimensioni del 40% del <unk> <unk>, ha <unk> da 40%.
2024-05-11 19:31:20,166 - INFO - joeynmt.training - Example #1
2024-05-11 19:31:20,166 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:31:20,166 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:31:20,166 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', '<unk>', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:31:20,167 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:31:20,167 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:31:20,168 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo problema perché non <unk> la <unk> del ghiaccio.
2024-05-11 19:31:20,168 - INFO - joeynmt.training - Example #2
2024-05-11 19:31:20,168 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:31:20,168 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:31:20,168 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', '<unk>', 'del', 'sistema', 'climatico', 'globale', '.', '</s>']
2024-05-11 19:31:20,169 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:31:20,169 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:31:20,169 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio <unk> <unk> è, in un certo senso, il cuore <unk> del sistema climatico globale.
2024-05-11 19:31:20,170 - INFO - joeynmt.training - Example #3
2024-05-11 19:31:20,170 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:31:20,170 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:31:20,170 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:31:20,171 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:31:20,171 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:31:20,172 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:31:20,172 - INFO - joeynmt.training - Example #4
2024-05-11 19:31:20,172 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:31:20,172 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:31:20,173 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', '<unk>', '<unk>', 'un', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:31:20,173 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:31:20,173 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:31:20,174 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi <unk> <unk> un <unk> <unk> di ciò che è successo negli ultimi 25 anni.
2024-05-11 19:31:23,010 - INFO - joeynmt.training - Epoch   5, Step:    14100, Batch Loss:     1.301206, Batch Acc: 0.578939, Tokens per Sec:    20647, Lr: 0.000300
2024-05-11 19:31:25,716 - INFO - joeynmt.training - Epoch   5, Step:    14200, Batch Loss:     1.327125, Batch Acc: 0.579167, Tokens per Sec:    24152, Lr: 0.000300
2024-05-11 19:31:28,539 - INFO - joeynmt.training - Epoch   5, Step:    14300, Batch Loss:     1.321678, Batch Acc: 0.575549, Tokens per Sec:    23478, Lr: 0.000300
2024-05-11 19:31:31,374 - INFO - joeynmt.training - Epoch   5, Step:    14400, Batch Loss:     1.527248, Batch Acc: 0.575246, Tokens per Sec:    22471, Lr: 0.000300
2024-05-11 19:31:34,519 - INFO - joeynmt.training - Epoch   5, Step:    14500, Batch Loss:     1.248096, Batch Acc: 0.573668, Tokens per Sec:    20756, Lr: 0.000300
2024-05-11 19:31:34,520 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:31:34,520 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:31:41,081 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.45, ppl:   4.25, acc:   0.55, generation: 6.5127[sec], evaluation: 0.0000[sec]
2024-05-11 19:31:41,082 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:31:41,318 - INFO - joeynmt.helpers - delete word_level_model_moses/12500.ckpt
2024-05-11 19:31:41,334 - INFO - joeynmt.training - Example #0
2024-05-11 19:31:41,335 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:31:41,335 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:31:41,335 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'anno', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'così', 'che', '<unk>', 'il', '<unk>', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'dell&apos;', '<unk>', '<unk>', ',', 'ha', '<unk>', 'da', '40', '%', '.', '</s>']
2024-05-11 19:31:41,335 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:31:41,336 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:31:41,336 - INFO - joeynmt.training - 	Hypothesis: <unk> anno ho mostrato questi due <unk> così che <unk> il <unk> <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dell' <unk> <unk>, ha <unk> da 40%.
2024-05-11 19:31:41,336 - INFO - joeynmt.training - Example #1
2024-05-11 19:31:41,337 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:31:41,337 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:31:41,337 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'particolare', 'problema', 'perché', 'non', 'mostra', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:31:41,337 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:31:41,338 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:31:41,338 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo particolare problema perché non mostra la <unk> del ghiaccio.
2024-05-11 19:31:41,338 - INFO - joeynmt.training - Example #2
2024-05-11 19:31:41,339 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:31:41,339 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:31:41,339 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', '<unk>', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:31:41,340 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:31:41,340 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:31:41,340 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un senso, il cuore <unk> del sistema globale.
2024-05-11 19:31:41,340 - INFO - joeynmt.training - Example #3
2024-05-11 19:31:41,340 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:31:41,341 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:31:41,341 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:31:41,341 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:31:41,342 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:31:41,342 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> in <unk> e <unk> in estate.
2024-05-11 19:31:41,342 - INFO - joeynmt.training - Example #4
2024-05-11 19:31:41,342 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:31:41,343 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:31:41,343 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', '<unk>', 'un', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', '<unk>', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:31:41,343 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:31:41,344 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:31:41,344 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi <unk> un <unk> <unk> di ciò che è <unk> negli ultimi 25 anni.
2024-05-11 19:31:44,175 - INFO - joeynmt.training - Epoch   5, Step:    14600, Batch Loss:     1.146466, Batch Acc: 0.577560, Tokens per Sec:    21041, Lr: 0.000300
2024-05-11 19:31:47,531 - INFO - joeynmt.training - Epoch   5, Step:    14700, Batch Loss:     1.326445, Batch Acc: 0.574476, Tokens per Sec:    19155, Lr: 0.000300
2024-05-11 19:31:50,374 - INFO - joeynmt.training - Epoch   5, Step:    14800, Batch Loss:     1.524673, Batch Acc: 0.572259, Tokens per Sec:    22567, Lr: 0.000300
2024-05-11 19:31:53,071 - INFO - joeynmt.training - Epoch   5, Step:    14900, Batch Loss:     1.518115, Batch Acc: 0.575256, Tokens per Sec:    24782, Lr: 0.000300
2024-05-11 19:31:55,589 - INFO - joeynmt.training - Epoch   5, Step:    15000, Batch Loss:     1.476810, Batch Acc: 0.574019, Tokens per Sec:    25633, Lr: 0.000300
2024-05-11 19:31:55,589 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:31:55,590 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:32:03,011 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.44, ppl:   4.22, acc:   0.55, generation: 7.3733[sec], evaluation: 0.0000[sec]
2024-05-11 19:32:03,012 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:32:03,246 - INFO - joeynmt.helpers - delete word_level_model_moses/12000.ckpt
2024-05-11 19:32:03,263 - INFO - joeynmt.training - Example #0
2024-05-11 19:32:03,263 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:32:03,264 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:32:03,264 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'così', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'dell&apos;', '<unk>', '<unk>', ',', 'ha', '<unk>', 'dal', '40', '%', '.', '</s>']
2024-05-11 19:32:03,264 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:32:03,265 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:32:03,265 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due <unk> così che <unk> il ghiaccio <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dell' <unk> <unk>, ha <unk> dal 40%.
2024-05-11 19:32:03,265 - INFO - joeynmt.training - Example #1
2024-05-11 19:32:03,265 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:32:03,266 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:32:03,266 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:32:03,266 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:32:03,266 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:32:03,267 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo problema perché non mostra la <unk> del ghiaccio.
2024-05-11 19:32:03,267 - INFO - joeynmt.training - Example #2
2024-05-11 19:32:03,267 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:32:03,267 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:32:03,268 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', '<unk>', '<unk>', 'del', 'sistema', 'climatico', '.', '</s>']
2024-05-11 19:32:03,268 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:32:03,268 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:32:03,269 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un certo senso, il <unk> <unk> del sistema climatico.
2024-05-11 19:32:03,269 - INFO - joeynmt.training - Example #3
2024-05-11 19:32:03,269 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:32:03,269 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:32:03,269 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:32:03,270 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:32:03,270 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:32:03,270 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:32:03,271 - INFO - joeynmt.training - Example #4
2024-05-11 19:32:03,271 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:32:03,271 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:32:03,271 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', 'mostrerò', 'un', '<unk>', '<unk>', 'di', 'cosa', 'è', '<unk>', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:32:03,272 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:32:03,272 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:32:03,272 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi mostrerò un <unk> <unk> di cosa è <unk> negli ultimi 25 anni.
2024-05-11 19:32:06,007 - INFO - joeynmt.training - Epoch   5, Step:    15100, Batch Loss:     1.477301, Batch Acc: 0.570437, Tokens per Sec:    21254, Lr: 0.000300
2024-05-11 19:32:08,697 - INFO - joeynmt.training - Epoch   5, Step:    15200, Batch Loss:     1.401726, Batch Acc: 0.575180, Tokens per Sec:    24823, Lr: 0.000300
2024-05-11 19:32:11,486 - INFO - joeynmt.training - Epoch   5, Step:    15300, Batch Loss:     1.371863, Batch Acc: 0.571442, Tokens per Sec:    22784, Lr: 0.000300
2024-05-11 19:32:13,878 - INFO - joeynmt.training - Epoch   5: total training loss 4124.48
2024-05-11 19:32:13,881 - INFO - joeynmt.training - EPOCH 6
2024-05-11 19:32:14,891 - INFO - joeynmt.training - Epoch   6, Step:    15400, Batch Loss:     1.423870, Batch Acc: 0.583549, Tokens per Sec:    16885, Lr: 0.000300
2024-05-11 19:32:17,403 - INFO - joeynmt.training - Epoch   6, Step:    15500, Batch Loss:     1.195337, Batch Acc: 0.598098, Tokens per Sec:    25874, Lr: 0.000300
2024-05-11 19:32:17,404 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:32:17,404 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:32:23,495 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.44, ppl:   4.23, acc:   0.55, generation: 6.0404[sec], evaluation: 0.0000[sec]
2024-05-11 19:32:23,724 - INFO - joeynmt.helpers - delete word_level_model_moses/13000.ckpt
2024-05-11 19:32:23,739 - INFO - joeynmt.training - Example #0
2024-05-11 19:32:23,740 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:32:23,740 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:32:23,740 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'anno', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'così', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'dei', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'delle', '<unk>', '<unk>', ',', 'ha', '<unk>', 'dal', '40', '%', '.', '</s>']
2024-05-11 19:32:23,741 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:32:23,741 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:32:23,742 - INFO - joeynmt.training - 	Hypothesis: L'anno anno ho mostrato questi due <unk> così che <unk> il ghiaccio <unk> <unk>, che per la maggior parte dei tre milioni di anni è stata la dimensione delle <unk> <unk>, ha <unk> dal 40%.
2024-05-11 19:32:23,742 - INFO - joeynmt.training - Example #1
2024-05-11 19:32:23,742 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:32:23,742 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:32:23,742 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', '<unk>', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:32:23,743 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:32:23,743 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:32:23,743 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo problema perché non <unk> la <unk> del ghiaccio.
2024-05-11 19:32:23,744 - INFO - joeynmt.training - Example #2
2024-05-11 19:32:23,744 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:32:23,744 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:32:23,744 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2024-05-11 19:32:23,745 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:32:23,745 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:32:23,745 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un certo senso, il cuore del sistema climatico.
2024-05-11 19:32:23,746 - INFO - joeynmt.training - Example #3
2024-05-11 19:32:23,746 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:32:23,746 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:32:23,746 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:32:23,747 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:32:23,747 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:32:23,747 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:32:23,747 - INFO - joeynmt.training - Example #4
2024-05-11 19:32:23,748 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:32:23,748 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:32:23,748 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', '<unk>', 'un', '<unk>', '<unk>', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:32:23,748 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:32:23,749 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:32:23,749 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi <unk> un <unk> <unk> di quello che è successo negli ultimi 25 anni.
2024-05-11 19:32:26,938 - INFO - joeynmt.training - Epoch   6, Step:    15600, Batch Loss:     1.278683, Batch Acc: 0.598999, Tokens per Sec:    19214, Lr: 0.000300
2024-05-11 19:32:30,079 - INFO - joeynmt.training - Epoch   6, Step:    15700, Batch Loss:     1.169551, Batch Acc: 0.595247, Tokens per Sec:    21200, Lr: 0.000300
2024-05-11 19:32:32,834 - INFO - joeynmt.training - Epoch   6, Step:    15800, Batch Loss:     1.254384, Batch Acc: 0.595715, Tokens per Sec:    24030, Lr: 0.000300
2024-05-11 19:32:35,419 - INFO - joeynmt.training - Epoch   6, Step:    15900, Batch Loss:     1.336001, Batch Acc: 0.586314, Tokens per Sec:    24750, Lr: 0.000300
2024-05-11 19:32:37,976 - INFO - joeynmt.training - Epoch   6, Step:    16000, Batch Loss:     1.214426, Batch Acc: 0.591394, Tokens per Sec:    25028, Lr: 0.000300
2024-05-11 19:32:37,977 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:32:37,977 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:32:46,648 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.44, ppl:   4.23, acc:   0.55, generation: 8.6203[sec], evaluation: 0.0000[sec]
2024-05-11 19:32:46,880 - INFO - joeynmt.helpers - delete word_level_model_moses/14000.ckpt
2024-05-11 19:32:46,894 - INFO - joeynmt.training - Example #0
2024-05-11 19:32:46,895 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:32:46,895 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:32:46,895 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'anno', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'così', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stato', 'la', 'dimensione', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', '.', '</s>']
2024-05-11 19:32:46,896 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:32:46,897 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:32:46,897 - INFO - joeynmt.training - 	Hypothesis: L'anno anno ho mostrato questi due <unk> così che <unk> il ghiaccio <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stato la dimensione degli ultimi tre milioni di anni.
2024-05-11 19:32:46,897 - INFO - joeynmt.training - Example #1
2024-05-11 19:32:46,898 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:32:46,898 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:32:46,898 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', '<unk>', '<unk>', 'il', 'problema', 'perché', 'non', 'mostra', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:32:46,898 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:32:46,899 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:32:46,899 - INFO - joeynmt.training - 	Hypothesis: Ma questa <unk> <unk> il problema perché non mostra la <unk> del ghiaccio.
2024-05-11 19:32:46,899 - INFO - joeynmt.training - Example #2
2024-05-11 19:32:46,899 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:32:46,900 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:32:46,900 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', '<unk>', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', '<unk>', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:32:46,900 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:32:46,900 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:32:46,901 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> <unk>, in un certo senso, il cuore <unk> del sistema globale.
2024-05-11 19:32:46,901 - INFO - joeynmt.training - Example #3
2024-05-11 19:32:46,901 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:32:46,901 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:32:46,901 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'nel', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:32:46,902 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:32:46,902 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:32:46,903 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> nel <unk> e <unk> in estate.
2024-05-11 19:32:46,903 - INFO - joeynmt.training - Example #4
2024-05-11 19:32:46,903 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:32:46,903 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:32:46,903 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', 'mostrerò', 'sarà', 'una', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:32:46,904 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:32:46,904 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:32:46,904 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi mostrerò sarà una <unk> <unk> di ciò che è successo negli ultimi 25 anni.
2024-05-11 19:32:49,749 - INFO - joeynmt.training - Epoch   6, Step:    16100, Batch Loss:     1.251986, Batch Acc: 0.590912, Tokens per Sec:    21536, Lr: 0.000300
2024-05-11 19:32:52,443 - INFO - joeynmt.training - Epoch   6, Step:    16200, Batch Loss:     1.252928, Batch Acc: 0.594423, Tokens per Sec:    23810, Lr: 0.000300
2024-05-11 19:32:55,126 - INFO - joeynmt.training - Epoch   6, Step:    16300, Batch Loss:     1.229383, Batch Acc: 0.593186, Tokens per Sec:    24442, Lr: 0.000300
2024-05-11 19:32:58,661 - INFO - joeynmt.training - Epoch   6, Step:    16400, Batch Loss:     1.375524, Batch Acc: 0.590034, Tokens per Sec:    18774, Lr: 0.000300
2024-05-11 19:33:01,268 - INFO - joeynmt.training - Epoch   6, Step:    16500, Batch Loss:     1.231472, Batch Acc: 0.587475, Tokens per Sec:    24537, Lr: 0.000300
2024-05-11 19:33:01,268 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:33:01,269 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:33:07,884 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.44, ppl:   4.23, acc:   0.55, generation: 6.5650[sec], evaluation: 0.0000[sec]
2024-05-11 19:33:08,102 - INFO - joeynmt.helpers - delete word_level_model_moses/13500.ckpt
2024-05-11 19:33:08,116 - INFO - joeynmt.training - Example #0
2024-05-11 19:33:08,116 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:33:08,117 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:33:08,117 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'così', '<unk>', 'che', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', '.', '</s>']
2024-05-11 19:33:08,118 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:33:08,118 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:33:08,118 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due <unk> così <unk> che il ghiaccio <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione degli ultimi tre milioni di anni.
2024-05-11 19:33:08,119 - INFO - joeynmt.training - Example #1
2024-05-11 19:33:08,119 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:33:08,119 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:33:08,119 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'la', '<unk>', 'di', 'questo', 'particolare', 'problema', 'perché', 'non', 'mostra', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:33:08,120 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:33:08,120 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:33:08,120 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> la <unk> di questo particolare problema perché non mostra la <unk> del ghiaccio.
2024-05-11 19:33:08,120 - INFO - joeynmt.training - Example #2
2024-05-11 19:33:08,121 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:33:08,121 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:33:08,121 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', '<unk>', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:33:08,121 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:33:08,122 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:33:08,122 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un certo senso, il cuore <unk> del sistema globale.
2024-05-11 19:33:08,122 - INFO - joeynmt.training - Example #3
2024-05-11 19:33:08,122 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:33:08,123 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:33:08,123 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:33:08,123 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:33:08,123 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:33:08,124 - INFO - joeynmt.training - 	Hypothesis: <unk> in estate.
2024-05-11 19:33:08,124 - INFO - joeynmt.training - Example #4
2024-05-11 19:33:08,124 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:33:08,124 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:33:08,124 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', '<unk>', '<unk>', 'una', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:33:08,125 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:33:08,125 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:33:08,125 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi <unk> <unk> una <unk> <unk> di ciò che è successo negli ultimi 25 anni.
2024-05-11 19:33:11,601 - INFO - joeynmt.training - Epoch   6, Step:    16600, Batch Loss:     1.271822, Batch Acc: 0.586019, Tokens per Sec:    17827, Lr: 0.000300
2024-05-11 19:33:14,535 - INFO - joeynmt.training - Epoch   6, Step:    16700, Batch Loss:     1.195168, Batch Acc: 0.588907, Tokens per Sec:    22671, Lr: 0.000300
2024-05-11 19:33:17,310 - INFO - joeynmt.training - Epoch   6, Step:    16800, Batch Loss:     1.196623, Batch Acc: 0.585972, Tokens per Sec:    22716, Lr: 0.000300
2024-05-11 19:33:19,878 - INFO - joeynmt.training - Epoch   6, Step:    16900, Batch Loss:     1.393013, Batch Acc: 0.587989, Tokens per Sec:    25292, Lr: 0.000300
2024-05-11 19:33:22,455 - INFO - joeynmt.training - Epoch   6, Step:    17000, Batch Loss:     1.264921, Batch Acc: 0.585500, Tokens per Sec:    25016, Lr: 0.000300
2024-05-11 19:33:22,455 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:33:22,455 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:33:29,932 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.43, ppl:   4.19, acc:   0.55, generation: 7.4188[sec], evaluation: 0.0000[sec]
2024-05-11 19:33:29,933 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:33:30,170 - INFO - joeynmt.helpers - delete word_level_model_moses/14500.ckpt
2024-05-11 19:33:30,199 - INFO - joeynmt.training - Example #0
2024-05-11 19:33:30,200 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:33:30,200 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:33:30,200 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'così', 'che', '<unk>', 'la', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', '<unk>', '<unk>', ',', '<unk>', 'da', '40', '%', '.', '</s>']
2024-05-11 19:33:30,201 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:33:30,201 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:33:30,201 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due <unk> così che <unk> la <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione degli <unk> <unk>, <unk> da 40%.
2024-05-11 19:33:30,202 - INFO - joeynmt.training - Example #1
2024-05-11 19:33:30,202 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:33:30,202 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:33:30,202 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', '<unk>', 'perché', 'non', 'mostra', 'il', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:33:30,203 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:33:30,203 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:33:30,203 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo problema <unk> perché non mostra il <unk> del ghiaccio.
2024-05-11 19:33:30,204 - INFO - joeynmt.training - Example #2
2024-05-11 19:33:30,204 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:33:30,204 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:33:30,204 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:33:30,205 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:33:30,205 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:33:30,205 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> <unk> è, in un certo senso, il cuore del sistema globale.
2024-05-11 19:33:30,205 - INFO - joeynmt.training - Example #3
2024-05-11 19:33:30,206 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:33:30,206 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:33:30,206 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:33:30,206 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:33:30,207 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:33:30,207 - INFO - joeynmt.training - 	Hypothesis: <unk> in estate.
2024-05-11 19:33:30,207 - INFO - joeynmt.training - Example #4
2024-05-11 19:33:30,207 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:33:30,208 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:33:30,208 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', '<unk>', '<unk>', 'che', 'vi', 'mostrerò', 'un', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', '<unk>', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:33:30,208 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:33:30,208 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:33:30,209 - INFO - joeynmt.training - 	Hypothesis: La <unk> <unk> che vi mostrerò un <unk> <unk> di ciò che è <unk> negli ultimi 25 anni.
2024-05-11 19:33:33,086 - INFO - joeynmt.training - Epoch   6, Step:    17100, Batch Loss:     1.315688, Batch Acc: 0.589409, Tokens per Sec:    21547, Lr: 0.000300
2024-05-11 19:33:35,893 - INFO - joeynmt.training - Epoch   6, Step:    17200, Batch Loss:     1.277504, Batch Acc: 0.579503, Tokens per Sec:    23176, Lr: 0.000300
2024-05-11 19:33:39,179 - INFO - joeynmt.training - Epoch   6, Step:    17300, Batch Loss:     1.155211, Batch Acc: 0.585242, Tokens per Sec:    19270, Lr: 0.000300
2024-05-11 19:33:41,986 - INFO - joeynmt.training - Epoch   6, Step:    17400, Batch Loss:     1.336903, Batch Acc: 0.584396, Tokens per Sec:    23085, Lr: 0.000300
2024-05-11 19:33:44,576 - INFO - joeynmt.training - Epoch   6, Step:    17500, Batch Loss:     1.444979, Batch Acc: 0.580225, Tokens per Sec:    24864, Lr: 0.000300
2024-05-11 19:33:44,576 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:33:44,577 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:33:51,172 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.43, ppl:   4.17, acc:   0.55, generation: 6.5030[sec], evaluation: 0.0000[sec]
2024-05-11 19:33:51,173 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:33:51,442 - INFO - joeynmt.helpers - delete word_level_model_moses/16500.ckpt
2024-05-11 19:33:51,461 - INFO - joeynmt.training - Example #0
2024-05-11 19:33:51,461 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:33:51,461 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:33:51,461 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'così', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stato', 'la', 'dimensione', 'dell&apos;', '<unk>', '<unk>', ',', 'ha', '<unk>', 'dal', '40', '%', '.', '</s>']
2024-05-11 19:33:51,462 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:33:51,464 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:33:51,464 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due <unk> così che <unk> il ghiaccio <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stato la dimensione dell' <unk> <unk>, ha <unk> dal 40%.
2024-05-11 19:33:51,464 - INFO - joeynmt.training - Example #1
2024-05-11 19:33:51,465 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:33:51,465 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:33:51,465 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'la', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:33:51,465 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:33:51,466 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:33:51,466 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> la <unk> di questo problema perché non mostra la <unk> del ghiaccio.
2024-05-11 19:33:51,466 - INFO - joeynmt.training - Example #2
2024-05-11 19:33:51,466 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:33:51,466 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:33:51,466 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', '<unk>', '<unk>', 'di', 'ghiaccio', '<unk>', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', '<unk>', '<unk>', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:33:51,467 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:33:51,467 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:33:51,467 - INFO - joeynmt.training - 	Hypothesis: La <unk> <unk> di ghiaccio <unk> è, in un certo senso, il <unk> <unk> del sistema globale.
2024-05-11 19:33:51,468 - INFO - joeynmt.training - Example #3
2024-05-11 19:33:51,468 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:33:51,468 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:33:51,468 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:33:51,468 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:33:51,469 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:33:51,469 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:33:51,469 - INFO - joeynmt.training - Example #4
2024-05-11 19:33:51,469 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:33:51,469 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:33:51,469 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', 'mostro', 'una', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:33:51,470 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:33:51,470 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:33:51,470 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi mostro una <unk> <unk> di ciò che è successo negli ultimi 25 anni.
2024-05-11 19:33:54,791 - INFO - joeynmt.training - Epoch   6, Step:    17600, Batch Loss:     1.413528, Batch Acc: 0.583653, Tokens per Sec:    18243, Lr: 0.000300
2024-05-11 19:33:57,417 - INFO - joeynmt.training - Epoch   6, Step:    17700, Batch Loss:     1.303039, Batch Acc: 0.590739, Tokens per Sec:    24521, Lr: 0.000300
2024-05-11 19:34:00,046 - INFO - joeynmt.training - Epoch   6, Step:    17800, Batch Loss:     1.348045, Batch Acc: 0.584332, Tokens per Sec:    25395, Lr: 0.000300
2024-05-11 19:34:02,737 - INFO - joeynmt.training - Epoch   6, Step:    17900, Batch Loss:     1.181140, Batch Acc: 0.591889, Tokens per Sec:    23996, Lr: 0.000300
2024-05-11 19:34:05,679 - INFO - joeynmt.training - Epoch   6, Step:    18000, Batch Loss:     1.389371, Batch Acc: 0.587025, Tokens per Sec:    21981, Lr: 0.000300
2024-05-11 19:34:05,680 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:34:05,680 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:34:12,281 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.43, ppl:   4.18, acc:   0.56, generation: 6.5502[sec], evaluation: 0.0000[sec]
2024-05-11 19:34:12,503 - INFO - joeynmt.helpers - delete word_level_model_moses/16000.ckpt
2024-05-11 19:34:12,519 - INFO - joeynmt.training - Example #0
2024-05-11 19:34:12,520 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:34:12,520 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:34:12,520 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', '<unk>', 'a', '<unk>', 'così', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stato', '<unk>', 'la', 'dimensione', 'degli', '<unk>', '<unk>', '<unk>', ',', 'ha', '<unk>', 'il', '40', '%', '.', '</s>']
2024-05-11 19:34:12,521 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:34:12,521 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:34:12,521 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho <unk> a <unk> così che <unk> il ghiaccio <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stato <unk> la dimensione degli <unk> <unk> <unk>, ha <unk> il 40%.
2024-05-11 19:34:12,522 - INFO - joeynmt.training - Example #1
2024-05-11 19:34:12,522 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:34:12,522 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:34:12,522 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'la', '<unk>', 'di', 'questo', 'particolare', 'problema', 'perché', 'non', 'mostra', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:34:12,523 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:34:12,523 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:34:12,523 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> la <unk> di questo particolare problema perché non mostra la <unk> del ghiaccio.
2024-05-11 19:34:12,524 - INFO - joeynmt.training - Example #2
2024-05-11 19:34:12,524 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:34:12,524 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:34:12,524 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:34:12,525 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:34:12,525 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:34:12,525 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> <unk> è, in un senso, il cuore del sistema globale.
2024-05-11 19:34:12,525 - INFO - joeynmt.training - Example #3
2024-05-11 19:34:12,526 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:34:12,526 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:34:12,526 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:34:12,526 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:34:12,527 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:34:12,527 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> <unk> e <unk> in estate.
2024-05-11 19:34:12,527 - INFO - joeynmt.training - Example #4
2024-05-11 19:34:12,527 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:34:12,528 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:34:12,528 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', 'mostrerò', 'un', '<unk>', '<unk>', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:34:12,528 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:34:12,528 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:34:12,529 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi mostrerò un <unk> <unk> di quello che è successo negli ultimi 25 anni.
2024-05-11 19:34:15,312 - INFO - joeynmt.training - Epoch   6, Step:    18100, Batch Loss:     1.302349, Batch Acc: 0.583640, Tokens per Sec:    21764, Lr: 0.000300
2024-05-11 19:34:18,165 - INFO - joeynmt.training - Epoch   6, Step:    18200, Batch Loss:     1.427747, Batch Acc: 0.591831, Tokens per Sec:    22810, Lr: 0.000300
2024-05-11 19:34:21,543 - INFO - joeynmt.training - Epoch   6, Step:    18300, Batch Loss:     1.419746, Batch Acc: 0.588841, Tokens per Sec:    19472, Lr: 0.000300
2024-05-11 19:34:24,189 - INFO - joeynmt.training - Epoch   6, Step:    18400, Batch Loss:     1.337638, Batch Acc: 0.586098, Tokens per Sec:    24397, Lr: 0.000300
2024-05-11 19:34:25,378 - INFO - joeynmt.training - Epoch   6: total training loss 3988.92
2024-05-11 19:34:25,378 - INFO - joeynmt.training - EPOCH 7
2024-05-11 19:34:26,801 - INFO - joeynmt.training - Epoch   7, Step:    18500, Batch Loss:     1.272750, Batch Acc: 0.610098, Tokens per Sec:    24853, Lr: 0.000300
2024-05-11 19:34:26,801 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:34:26,802 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:34:34,232 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.42, ppl:   4.15, acc:   0.56, generation: 7.3374[sec], evaluation: 0.0000[sec]
2024-05-11 19:34:34,233 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:34:34,522 - INFO - joeynmt.helpers - delete word_level_model_moses/15500.ckpt
2024-05-11 19:34:34,539 - INFO - joeynmt.training - Example #0
2024-05-11 19:34:34,539 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:34:34,540 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:34:34,540 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'così', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', 'stati', '<unk>', '<unk>', ',', 'il', '40', '%', '.', '</s>']
2024-05-11 19:34:34,540 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:34:34,541 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:34:34,541 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due <unk> così che <unk> il ghiaccio <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione degli stati <unk> <unk>, il 40%.
2024-05-11 19:34:34,541 - INFO - joeynmt.training - Example #1
2024-05-11 19:34:34,541 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:34:34,542 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:34:34,542 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', '<unk>', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:34:34,542 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:34:34,543 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:34:34,543 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo problema perché non <unk> la <unk> del ghiaccio.
2024-05-11 19:34:34,543 - INFO - joeynmt.training - Example #2
2024-05-11 19:34:34,543 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:34:34,543 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:34:34,544 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', '<unk>', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:34:34,544 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:34:34,544 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:34:34,545 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio <unk> <unk> è, in un certo senso, il cuore <unk> del sistema globale.
2024-05-11 19:34:34,545 - INFO - joeynmt.training - Example #3
2024-05-11 19:34:34,545 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:34:34,545 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:34:34,546 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:34:34,546 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:34:34,546 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:34:34,547 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:34:34,547 - INFO - joeynmt.training - Example #4
2024-05-11 19:34:34,547 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:34:34,547 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:34:34,547 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', 'mostro', '<unk>', 'un', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', '<unk>', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:34:34,548 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:34:34,548 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:34:34,548 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi mostro <unk> un <unk> <unk> di ciò che è <unk> negli ultimi 25 anni.
2024-05-11 19:34:37,390 - INFO - joeynmt.training - Epoch   7, Step:    18600, Batch Loss:     1.177999, Batch Acc: 0.609082, Tokens per Sec:    20760, Lr: 0.000300
2024-05-11 19:34:40,107 - INFO - joeynmt.training - Epoch   7, Step:    18700, Batch Loss:     1.181486, Batch Acc: 0.609131, Tokens per Sec:    24286, Lr: 0.000300
2024-05-11 19:34:42,902 - INFO - joeynmt.training - Epoch   7, Step:    18800, Batch Loss:     1.339618, Batch Acc: 0.604847, Tokens per Sec:    23189, Lr: 0.000300
2024-05-11 19:34:45,699 - INFO - joeynmt.training - Epoch   7, Step:    18900, Batch Loss:     1.328459, Batch Acc: 0.602043, Tokens per Sec:    23145, Lr: 0.000300
2024-05-11 19:34:48,811 - INFO - joeynmt.training - Epoch   7, Step:    19000, Batch Loss:     1.338923, Batch Acc: 0.601764, Tokens per Sec:    21246, Lr: 0.000300
2024-05-11 19:34:48,811 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:34:48,812 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:34:55,544 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.42, ppl:   4.16, acc:   0.56, generation: 6.6829[sec], evaluation: 0.0000[sec]
2024-05-11 19:34:55,763 - INFO - joeynmt.helpers - delete word_level_model_moses/15000.ckpt
2024-05-11 19:34:55,779 - INFO - joeynmt.training - Example #0
2024-05-11 19:34:55,780 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:34:55,780 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:34:55,780 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'questi', 'due', '<unk>', '<unk>', 'così', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', '<unk>', '<unk>', '<unk>', ',', 'ha', '<unk>', 'dal', '40', '%', '.', '</s>']
2024-05-11 19:34:55,781 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:34:55,781 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:34:55,781 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due <unk> <unk> così che <unk> il ghiaccio <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione degli <unk> <unk> <unk>, ha <unk> dal 40%.
2024-05-11 19:34:55,781 - INFO - joeynmt.training - Example #1
2024-05-11 19:34:55,782 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:34:55,782 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:34:55,782 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:34:55,783 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:34:55,783 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:34:55,783 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo problema perché non mostra la <unk> del ghiaccio.
2024-05-11 19:34:55,783 - INFO - joeynmt.training - Example #2
2024-05-11 19:34:55,784 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:34:55,784 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:34:55,784 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', '<unk>', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:34:55,784 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:34:55,785 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:34:55,785 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un certo senso, il cuore <unk> del sistema globale.
2024-05-11 19:34:55,785 - INFO - joeynmt.training - Example #3
2024-05-11 19:34:55,785 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:34:55,786 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:34:55,786 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:34:55,786 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:34:55,787 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:34:55,787 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:34:55,787 - INFO - joeynmt.training - Example #4
2024-05-11 19:34:55,787 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:34:55,788 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:34:55,788 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', 'mostrerò', 'un', '<unk>', '<unk>', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:34:55,788 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:34:55,788 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:34:55,789 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi mostrerò un <unk> <unk> di quello che è successo negli ultimi 25 anni.
2024-05-11 19:34:58,793 - INFO - joeynmt.training - Epoch   7, Step:    19100, Batch Loss:     1.349026, Batch Acc: 0.597471, Tokens per Sec:    20455, Lr: 0.000300
2024-05-11 19:35:02,097 - INFO - joeynmt.training - Epoch   7, Step:    19200, Batch Loss:     1.290817, Batch Acc: 0.601358, Tokens per Sec:    19620, Lr: 0.000300
2024-05-11 19:35:04,728 - INFO - joeynmt.training - Epoch   7, Step:    19300, Batch Loss:     1.325999, Batch Acc: 0.597025, Tokens per Sec:    24798, Lr: 0.000300
2024-05-11 19:35:07,519 - INFO - joeynmt.training - Epoch   7, Step:    19400, Batch Loss:     1.231567, Batch Acc: 0.603792, Tokens per Sec:    22871, Lr: 0.000300
2024-05-11 19:35:10,037 - INFO - joeynmt.training - Epoch   7, Step:    19500, Batch Loss:     1.117030, Batch Acc: 0.599161, Tokens per Sec:    25587, Lr: 0.000300
2024-05-11 19:35:10,037 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:35:10,038 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:35:17,322 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.43, ppl:   4.19, acc:   0.55, generation: 7.2357[sec], evaluation: 0.0000[sec]
2024-05-11 19:35:17,539 - INFO - joeynmt.helpers - delete word_level_model_moses/17000.ckpt
2024-05-11 19:35:17,552 - INFO - joeynmt.training - Example #0
2024-05-11 19:35:17,552 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:35:17,552 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:35:17,553 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'così', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stato', 'la', 'dimensione', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stato', 'la', 'dimensione', 'degli', 'stati', '<unk>', 'del', '40', '%', '.', '</s>']
2024-05-11 19:35:17,553 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:35:17,554 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:35:17,554 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due <unk> così che <unk> il ghiaccio <unk> che per la maggior parte degli ultimi tre milioni di anni è stato la dimensione degli ultimi tre milioni di anni è stato la dimensione degli stati <unk> del 40%.
2024-05-11 19:35:17,554 - INFO - joeynmt.training - Example #1
2024-05-11 19:35:17,554 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:35:17,555 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:35:17,555 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'il', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:35:17,555 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:35:17,556 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:35:17,556 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo problema perché non mostra il <unk> del ghiaccio.
2024-05-11 19:35:17,556 - INFO - joeynmt.training - Example #2
2024-05-11 19:35:17,556 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:35:17,556 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:35:17,557 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', '<unk>', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:35:17,557 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:35:17,557 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:35:17,558 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un certo senso, il cuore <unk> del sistema globale.
2024-05-11 19:35:17,558 - INFO - joeynmt.training - Example #3
2024-05-11 19:35:17,558 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:35:17,558 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:35:17,558 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:35:17,559 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:35:17,559 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:35:17,559 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:35:17,560 - INFO - joeynmt.training - Example #4
2024-05-11 19:35:17,560 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:35:17,560 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:35:17,560 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', 'mostrerò', 'una', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:35:17,561 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:35:17,561 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:35:17,561 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi mostrerò una <unk> <unk> di ciò che è successo negli ultimi 25 anni.
2024-05-11 19:35:20,465 - INFO - joeynmt.training - Epoch   7, Step:    19600, Batch Loss:     1.321446, Batch Acc: 0.602175, Tokens per Sec:    20751, Lr: 0.000300
2024-05-11 19:35:23,191 - INFO - joeynmt.training - Epoch   7, Step:    19700, Batch Loss:     1.273767, Batch Acc: 0.594696, Tokens per Sec:    24299, Lr: 0.000300
2024-05-11 19:35:25,951 - INFO - joeynmt.training - Epoch   7, Step:    19800, Batch Loss:     1.217757, Batch Acc: 0.594479, Tokens per Sec:    22954, Lr: 0.000300
2024-05-11 19:35:29,267 - INFO - joeynmt.training - Epoch   7, Step:    19900, Batch Loss:     1.345555, Batch Acc: 0.594871, Tokens per Sec:    19670, Lr: 0.000300
2024-05-11 19:35:31,929 - INFO - joeynmt.training - Epoch   7, Step:    20000, Batch Loss:     1.182233, Batch Acc: 0.597041, Tokens per Sec:    23976, Lr: 0.000300
2024-05-11 19:35:31,930 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:35:31,930 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:35:38,142 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.43, ppl:   4.19, acc:   0.55, generation: 6.1627[sec], evaluation: 0.0000[sec]
2024-05-11 19:35:38,147 - INFO - joeynmt.training - Example #0
2024-05-11 19:35:38,147 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:35:38,148 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:35:38,148 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'così', '<unk>', 'che', 'il', '<unk>', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', 'stati', '<unk>', '<unk>', ',', '<unk>', 'dal', '40', '%', '.', '</s>']
2024-05-11 19:35:38,149 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:35:38,149 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:35:38,149 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due <unk> così <unk> che il <unk> <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione degli stati <unk> <unk>, <unk> dal 40%.
2024-05-11 19:35:38,150 - INFO - joeynmt.training - Example #1
2024-05-11 19:35:38,150 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:35:38,150 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:35:38,151 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:35:38,151 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:35:38,151 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:35:38,152 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo problema perché non mostra la <unk> del ghiaccio.
2024-05-11 19:35:38,152 - INFO - joeynmt.training - Example #2
2024-05-11 19:35:38,152 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:35:38,152 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:35:38,152 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', '<unk>', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:35:38,153 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:35:38,153 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:35:38,153 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un certo senso, il cuore <unk> del sistema globale.
2024-05-11 19:35:38,154 - INFO - joeynmt.training - Example #3
2024-05-11 19:35:38,154 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:35:38,154 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:35:38,154 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:35:38,155 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:35:38,155 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:35:38,155 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:35:38,155 - INFO - joeynmt.training - Example #4
2024-05-11 19:35:38,156 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:35:38,156 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:35:38,156 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', 'mostro', 'un', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:35:38,157 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:35:38,157 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:35:38,157 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi mostro un <unk> <unk> di ciò che è successo negli ultimi 25 anni.
2024-05-11 19:35:41,356 - INFO - joeynmt.training - Epoch   7, Step:    20100, Batch Loss:     1.252891, Batch Acc: 0.594760, Tokens per Sec:    20143, Lr: 0.000300
2024-05-11 19:35:44,521 - INFO - joeynmt.training - Epoch   7, Step:    20200, Batch Loss:     1.346849, Batch Acc: 0.598407, Tokens per Sec:    20590, Lr: 0.000300
2024-05-11 19:35:47,193 - INFO - joeynmt.training - Epoch   7, Step:    20300, Batch Loss:     1.335777, Batch Acc: 0.593266, Tokens per Sec:    24623, Lr: 0.000300
2024-05-11 19:35:49,814 - INFO - joeynmt.training - Epoch   7, Step:    20400, Batch Loss:     1.192163, Batch Acc: 0.594573, Tokens per Sec:    25333, Lr: 0.000300
2024-05-11 19:35:52,434 - INFO - joeynmt.training - Epoch   7, Step:    20500, Batch Loss:     1.221270, Batch Acc: 0.596434, Tokens per Sec:    24929, Lr: 0.000300
2024-05-11 19:35:52,434 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:35:52,435 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:36:00,705 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.42, ppl:   4.13, acc:   0.55, generation: 8.2195[sec], evaluation: 0.0000[sec]
2024-05-11 19:36:00,705 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:36:00,938 - INFO - joeynmt.helpers - delete word_level_model_moses/19500.ckpt
2024-05-11 19:36:00,954 - INFO - joeynmt.training - Example #0
2024-05-11 19:36:00,955 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:36:00,955 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:36:00,955 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', '<unk>', '<unk>', 'così', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'delle', '<unk>', '<unk>', ',', 'è', '<unk>', 'dal', '40', '%', '.', '</s>']
2024-05-11 19:36:00,956 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:36:00,956 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:36:00,956 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due <unk> <unk> così che <unk> il ghiaccio <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione delle <unk> <unk>, è <unk> dal 40%.
2024-05-11 19:36:00,957 - INFO - joeynmt.training - Example #1
2024-05-11 19:36:00,957 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:36:00,957 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:36:00,957 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'la', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', 'mostrano', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:36:00,958 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:36:00,958 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:36:00,958 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> la <unk> di questo problema perché non mostrano la <unk> del ghiaccio.
2024-05-11 19:36:00,958 - INFO - joeynmt.training - Example #2
2024-05-11 19:36:00,959 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:36:00,959 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:36:00,959 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', '<unk>', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:36:00,959 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:36:00,959 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:36:00,960 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un certo senso, il cuore <unk> del sistema globale.
2024-05-11 19:36:00,960 - INFO - joeynmt.training - Example #3
2024-05-11 19:36:00,960 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:36:00,960 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:36:00,960 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:36:00,960 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:36:00,960 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:36:00,961 - INFO - joeynmt.training - 	Hypothesis: <unk> in estate.
2024-05-11 19:36:00,961 - INFO - joeynmt.training - Example #4
2024-05-11 19:36:00,961 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:36:00,961 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:36:00,961 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', 'mostrerò', 'un', '<unk>', '<unk>', 'di', 'quello', 'che', 'è', '<unk>', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:36:00,961 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:36:00,961 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:36:00,962 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi mostrerò un <unk> <unk> di quello che è <unk> negli ultimi 25 anni.
2024-05-11 19:36:03,796 - INFO - joeynmt.training - Epoch   7, Step:    20600, Batch Loss:     1.251951, Batch Acc: 0.592252, Tokens per Sec:    20805, Lr: 0.000300
2024-05-11 19:36:06,546 - INFO - joeynmt.training - Epoch   7, Step:    20700, Batch Loss:     1.456483, Batch Acc: 0.588115, Tokens per Sec:    23825, Lr: 0.000300
2024-05-11 19:36:09,381 - INFO - joeynmt.training - Epoch   7, Step:    20800, Batch Loss:     1.184097, Batch Acc: 0.595868, Tokens per Sec:    22510, Lr: 0.000300
2024-05-11 19:36:12,767 - INFO - joeynmt.training - Epoch   7, Step:    20900, Batch Loss:     1.276240, Batch Acc: 0.591732, Tokens per Sec:    19671, Lr: 0.000300
2024-05-11 19:36:15,401 - INFO - joeynmt.training - Epoch   7, Step:    21000, Batch Loss:     1.276619, Batch Acc: 0.590454, Tokens per Sec:    24199, Lr: 0.000300
2024-05-11 19:36:15,402 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:36:15,402 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:36:21,744 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.42, ppl:   4.15, acc:   0.56, generation: 6.2925[sec], evaluation: 0.0000[sec]
2024-05-11 19:36:21,962 - INFO - joeynmt.helpers - delete word_level_model_moses/18000.ckpt
2024-05-11 19:36:21,980 - INFO - joeynmt.training - Example #0
2024-05-11 19:36:21,980 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:36:21,980 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:36:21,981 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'anno', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'in', 'modo', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stato', 'la', 'dimensione', 'degli', 'stati', '<unk>', '<unk>', ',', 'ha', '<unk>', 'dal', '40', '%', '.', '</s>']
2024-05-11 19:36:21,981 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:36:21,981 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:36:21,982 - INFO - joeynmt.training - 	Hypothesis: L'anno anno ho mostrato questi due <unk> in modo che <unk> il ghiaccio <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stato la dimensione degli stati <unk> <unk>, ha <unk> dal 40%.
2024-05-11 19:36:21,982 - INFO - joeynmt.training - Example #1
2024-05-11 19:36:21,982 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:36:21,982 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:36:21,983 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', 'in', 'particolare', 'perché', 'non', 'mostra', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:36:21,983 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:36:21,983 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:36:21,984 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo problema in particolare perché non mostra la <unk> del ghiaccio.
2024-05-11 19:36:21,984 - INFO - joeynmt.training - Example #2
2024-05-11 19:36:21,984 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:36:21,984 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:36:21,985 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', '<unk>', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:36:21,985 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:36:21,985 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:36:21,986 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un certo senso, il cuore <unk> del sistema globale.
2024-05-11 19:36:21,986 - INFO - joeynmt.training - Example #3
2024-05-11 19:36:21,986 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:36:21,986 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:36:21,986 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:36:21,987 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:36:21,987 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:36:21,987 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:36:21,987 - INFO - joeynmt.training - Example #4
2024-05-11 19:36:21,988 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:36:21,988 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:36:21,988 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', 'mostrerò', 'una', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:36:21,988 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:36:21,989 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:36:21,989 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi mostrerò una <unk> <unk> di ciò che è successo negli ultimi 25 anni.
2024-05-11 19:36:25,665 - INFO - joeynmt.training - Epoch   7, Step:    21100, Batch Loss:     1.240782, Batch Acc: 0.593173, Tokens per Sec:    17104, Lr: 0.000300
2024-05-11 19:36:28,470 - INFO - joeynmt.training - Epoch   7, Step:    21200, Batch Loss:     1.268363, Batch Acc: 0.592418, Tokens per Sec:    23081, Lr: 0.000300
2024-05-11 19:36:31,122 - INFO - joeynmt.training - Epoch   7, Step:    21300, Batch Loss:     1.266080, Batch Acc: 0.590256, Tokens per Sec:    24684, Lr: 0.000300
2024-05-11 19:36:33,917 - INFO - joeynmt.training - Epoch   7, Step:    21400, Batch Loss:     1.380112, Batch Acc: 0.588599, Tokens per Sec:    23060, Lr: 0.000300
2024-05-11 19:36:36,662 - INFO - joeynmt.training - Epoch   7, Step:    21500, Batch Loss:     1.238956, Batch Acc: 0.598456, Tokens per Sec:    24505, Lr: 0.000300
2024-05-11 19:36:36,662 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:36:36,662 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:36:43,762 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.41, ppl:   4.11, acc:   0.56, generation: 7.0483[sec], evaluation: 0.0000[sec]
2024-05-11 19:36:43,763 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:36:44,007 - INFO - joeynmt.helpers - delete word_level_model_moses/17500.ckpt
2024-05-11 19:36:44,023 - INFO - joeynmt.training - Example #0
2024-05-11 19:36:44,024 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:36:44,024 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:36:44,025 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'così', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'delle', '<unk>', '<unk>', ',', 'ha', '<unk>', 'il', '40', '%', '.', '</s>']
2024-05-11 19:36:44,025 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:36:44,026 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:36:44,026 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due <unk> così che <unk> il ghiaccio <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione delle <unk> <unk>, ha <unk> il 40%.
2024-05-11 19:36:44,026 - INFO - joeynmt.training - Example #1
2024-05-11 19:36:44,026 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:36:44,027 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:36:44,027 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'particolare', 'problema', 'perché', 'non', 'mostra', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:36:44,027 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:36:44,027 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:36:44,028 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo particolare problema perché non mostra la <unk> del ghiaccio.
2024-05-11 19:36:44,028 - INFO - joeynmt.training - Example #2
2024-05-11 19:36:44,028 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:36:44,028 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:36:44,028 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', '<unk>', 'del', 'sistema', 'climatico', 'globale', '.', '</s>']
2024-05-11 19:36:44,029 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:36:44,029 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:36:44,029 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un senso, il cuore <unk> del sistema climatico globale.
2024-05-11 19:36:44,030 - INFO - joeynmt.training - Example #3
2024-05-11 19:36:44,030 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:36:44,030 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:36:44,030 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:36:44,031 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:36:44,031 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:36:44,031 - INFO - joeynmt.training - 	Hypothesis: <unk> in estate.
2024-05-11 19:36:44,031 - INFO - joeynmt.training - Example #4
2024-05-11 19:36:44,031 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:36:44,032 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:36:44,032 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', '<unk>', '<unk>', 'che', 'vi', '<unk>', 'sarà', 'un', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:36:44,032 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:36:44,033 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:36:44,033 - INFO - joeynmt.training - 	Hypothesis: La <unk> <unk> che vi <unk> sarà un <unk> <unk> di ciò che è successo negli ultimi 25 anni.
2024-05-11 19:36:44,585 - INFO - joeynmt.training - Epoch   7: total training loss 3885.84
2024-05-11 19:36:44,586 - INFO - joeynmt.training - EPOCH 8
2024-05-11 19:36:46,942 - INFO - joeynmt.training - Epoch   8, Step:    21600, Batch Loss:     1.211125, Batch Acc: 0.611803, Tokens per Sec:    23181, Lr: 0.000300
2024-05-11 19:36:49,690 - INFO - joeynmt.training - Epoch   8, Step:    21700, Batch Loss:     1.200974, Batch Acc: 0.611146, Tokens per Sec:    23087, Lr: 0.000300
2024-05-11 19:36:53,229 - INFO - joeynmt.training - Epoch   8, Step:    21800, Batch Loss:     1.315995, Batch Acc: 0.611906, Tokens per Sec:    19103, Lr: 0.000300
2024-05-11 19:36:55,924 - INFO - joeynmt.training - Epoch   8, Step:    21900, Batch Loss:     1.182302, Batch Acc: 0.614863, Tokens per Sec:    24207, Lr: 0.000300
2024-05-11 19:36:58,542 - INFO - joeynmt.training - Epoch   8, Step:    22000, Batch Loss:     1.080371, Batch Acc: 0.608431, Tokens per Sec:    25099, Lr: 0.000300
2024-05-11 19:36:58,542 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:36:58,543 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:37:05,049 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.43, ppl:   4.18, acc:   0.56, generation: 6.4179[sec], evaluation: 0.0000[sec]
2024-05-11 19:37:05,056 - INFO - joeynmt.training - Example #0
2024-05-11 19:37:05,057 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:37:05,057 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:37:05,057 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', '<unk>', 'così', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'sono', 'stati', 'le', 'dimensioni', 'del', '40', '%', 'del', '<unk>', '<unk>', ',', 'ha', '<unk>', 'il', '40', '%', '.', '</s>']
2024-05-11 19:37:05,058 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:37:05,058 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:37:05,058 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due <unk> così che <unk> il ghiaccio <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni sono stati le dimensioni del 40% del <unk> <unk>, ha <unk> il 40%.
2024-05-11 19:37:05,059 - INFO - joeynmt.training - Example #1
2024-05-11 19:37:05,059 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:37:05,059 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:37:05,059 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'la', '<unk>', 'di', 'questo', 'problema', 'in', 'particolare', 'perché', 'non', 'mostra', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:37:05,060 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:37:05,060 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:37:05,060 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> la <unk> di questo problema in particolare perché non mostra la <unk> del ghiaccio.
2024-05-11 19:37:05,060 - INFO - joeynmt.training - Example #2
2024-05-11 19:37:05,060 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:37:05,061 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:37:05,061 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'si', 'trova', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:37:05,061 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:37:05,062 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:37:05,062 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> si trova, in un senso, il cuore del sistema globale.
2024-05-11 19:37:05,062 - INFO - joeynmt.training - Example #3
2024-05-11 19:37:05,062 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:37:05,062 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:37:05,062 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'nel', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:37:05,063 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:37:05,063 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:37:05,063 - INFO - joeynmt.training - 	Hypothesis: <unk> nel <unk> e <unk> in estate.
2024-05-11 19:37:05,063 - INFO - joeynmt.training - Example #4
2024-05-11 19:37:05,064 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:37:05,064 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:37:05,064 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', 'mostrerò', 'sarà', 'una', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', '<unk>', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:37:05,064 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:37:05,065 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:37:05,065 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi mostrerò sarà una <unk> <unk> di ciò che è <unk> negli ultimi 25 anni.
2024-05-11 19:37:08,344 - INFO - joeynmt.training - Epoch   8, Step:    22100, Batch Loss:     1.299445, Batch Acc: 0.608994, Tokens per Sec:    20166, Lr: 0.000300
2024-05-11 19:37:11,048 - INFO - joeynmt.training - Epoch   8, Step:    22200, Batch Loss:     1.270840, Batch Acc: 0.612047, Tokens per Sec:    24153, Lr: 0.000300
2024-05-11 19:37:13,771 - INFO - joeynmt.training - Epoch   8, Step:    22300, Batch Loss:     1.203116, Batch Acc: 0.610036, Tokens per Sec:    24111, Lr: 0.000300
2024-05-11 19:37:16,402 - INFO - joeynmt.training - Epoch   8, Step:    22400, Batch Loss:     1.239741, Batch Acc: 0.605702, Tokens per Sec:    25464, Lr: 0.000300
2024-05-11 19:37:19,415 - INFO - joeynmt.training - Epoch   8, Step:    22500, Batch Loss:     1.352276, Batch Acc: 0.608581, Tokens per Sec:    21867, Lr: 0.000300
2024-05-11 19:37:19,415 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:37:19,415 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:37:26,195 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.42, ppl:   4.15, acc:   0.55, generation: 6.7186[sec], evaluation: 0.0000[sec]
2024-05-11 19:37:26,413 - INFO - joeynmt.helpers - delete word_level_model_moses/19000.ckpt
2024-05-11 19:37:26,428 - INFO - joeynmt.training - Example #0
2024-05-11 19:37:26,429 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:37:26,429 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:37:26,430 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'in', 'modo', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', '<unk>', '<unk>', '<unk>', ',', '<unk>', 'dal', '40', '%', '.', '</s>']
2024-05-11 19:37:26,430 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:37:26,430 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:37:26,431 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due <unk> in modo che <unk> il ghiaccio <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione degli <unk> <unk> <unk>, <unk> dal 40%.
2024-05-11 19:37:26,431 - INFO - joeynmt.training - Example #1
2024-05-11 19:37:26,431 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:37:26,431 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:37:26,431 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'il', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:37:26,432 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:37:26,432 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:37:26,432 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo problema perché non mostra il <unk> del ghiaccio.
2024-05-11 19:37:26,433 - INFO - joeynmt.training - Example #2
2024-05-11 19:37:26,433 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:37:26,433 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:37:26,433 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', '<unk>', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:37:26,434 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:37:26,434 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:37:26,434 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un certo senso, il cuore <unk> del sistema globale.
2024-05-11 19:37:26,434 - INFO - joeynmt.training - Example #3
2024-05-11 19:37:26,434 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:37:26,435 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:37:26,435 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', 'estate', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:37:26,435 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:37:26,435 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:37:26,435 - INFO - joeynmt.training - 	Hypothesis: <unk> in estate <unk> e <unk> in estate.
2024-05-11 19:37:26,435 - INFO - joeynmt.training - Example #4
2024-05-11 19:37:26,435 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:37:26,436 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:37:26,436 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', 'mostrerò', 'un', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:37:26,436 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:37:26,436 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:37:26,436 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi mostrerò un <unk> <unk> di ciò che è successo negli ultimi 25 anni.
2024-05-11 19:37:29,293 - INFO - joeynmt.training - Epoch   8, Step:    22600, Batch Loss:     1.272087, Batch Acc: 0.605739, Tokens per Sec:    20894, Lr: 0.000300
2024-05-11 19:37:32,251 - INFO - joeynmt.training - Epoch   8, Step:    22700, Batch Loss:     1.343974, Batch Acc: 0.611720, Tokens per Sec:    22260, Lr: 0.000300
2024-05-11 19:37:35,429 - INFO - joeynmt.training - Epoch   8, Step:    22800, Batch Loss:     1.244611, Batch Acc: 0.601613, Tokens per Sec:    20417, Lr: 0.000300
2024-05-11 19:37:38,166 - INFO - joeynmt.training - Epoch   8, Step:    22900, Batch Loss:     1.124086, Batch Acc: 0.606898, Tokens per Sec:    24158, Lr: 0.000300
2024-05-11 19:37:40,782 - INFO - joeynmt.training - Epoch   8, Step:    23000, Batch Loss:     1.174028, Batch Acc: 0.604127, Tokens per Sec:    25383, Lr: 0.000300
2024-05-11 19:37:40,783 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:37:40,783 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:37:47,916 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.42, ppl:   4.15, acc:   0.55, generation: 7.0714[sec], evaluation: 0.0000[sec]
2024-05-11 19:37:48,152 - INFO - joeynmt.helpers - delete word_level_model_moses/22500.ckpt
2024-05-11 19:37:48,163 - INFO - joeynmt.helpers - delete /content/drive/MyDrive/mt5/mt-exercise-5/word_level_model_moses/22500.ckpt
2024-05-11 19:37:48,163 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /content/drive/MyDrive/mt5/mt-exercise-5/word_level_model_moses/22500.ckpt but file does not exist. ([Errno 2] No such file or directory: '/content/drive/MyDrive/mt5/mt-exercise-5/word_level_model_moses/22500.ckpt')
2024-05-11 19:37:48,168 - INFO - joeynmt.training - Example #0
2024-05-11 19:37:48,169 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:37:48,169 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:37:48,170 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'in', 'modo', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', '<unk>', '<unk>', '<unk>', ',', '<unk>', 'da', '40', '%', '.', '</s>']
2024-05-11 19:37:48,170 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:37:48,170 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:37:48,171 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due <unk> in modo che <unk> il ghiaccio <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione degli <unk> <unk> <unk>, <unk> da 40%.
2024-05-11 19:37:48,171 - INFO - joeynmt.training - Example #1
2024-05-11 19:37:48,171 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:37:48,171 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:37:48,172 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:37:48,172 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:37:48,172 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:37:48,173 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo problema perché non mostra la <unk> del ghiaccio.
2024-05-11 19:37:48,173 - INFO - joeynmt.training - Example #2
2024-05-11 19:37:48,173 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:37:48,173 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:37:48,173 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', '<unk>', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:37:48,174 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:37:48,174 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:37:48,174 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un certo senso, il cuore <unk> del sistema globale.
2024-05-11 19:37:48,174 - INFO - joeynmt.training - Example #3
2024-05-11 19:37:48,175 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:37:48,175 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:37:48,175 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:37:48,175 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:37:48,176 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:37:48,176 - INFO - joeynmt.training - 	Hypothesis: <unk> in estate.
2024-05-11 19:37:48,176 - INFO - joeynmt.training - Example #4
2024-05-11 19:37:48,176 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:37:48,177 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:37:48,177 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', 'mostrerò', 'un', '<unk>', '<unk>', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:37:48,177 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:37:48,177 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:37:48,178 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi mostrerò un <unk> <unk> di quello che è successo negli ultimi 25 anni.
2024-05-11 19:37:51,021 - INFO - joeynmt.training - Epoch   8, Step:    23100, Batch Loss:     1.469737, Batch Acc: 0.608414, Tokens per Sec:    21048, Lr: 0.000300
2024-05-11 19:37:53,641 - INFO - joeynmt.training - Epoch   8, Step:    23200, Batch Loss:     1.273783, Batch Acc: 0.601811, Tokens per Sec:    24540, Lr: 0.000300
2024-05-11 19:37:56,385 - INFO - joeynmt.training - Epoch   8, Step:    23300, Batch Loss:     1.221110, Batch Acc: 0.605870, Tokens per Sec:    23897, Lr: 0.000300
2024-05-11 19:37:59,384 - INFO - joeynmt.training - Epoch   8, Step:    23400, Batch Loss:     1.271059, Batch Acc: 0.602345, Tokens per Sec:    21654, Lr: 0.000300
2024-05-11 19:38:02,447 - INFO - joeynmt.training - Epoch   8, Step:    23500, Batch Loss:     1.162402, Batch Acc: 0.603743, Tokens per Sec:    20749, Lr: 0.000300
2024-05-11 19:38:02,447 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:38:02,448 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:38:08,489 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.42, ppl:   4.13, acc:   0.56, generation: 5.9945[sec], evaluation: 0.0000[sec]
2024-05-11 19:38:08,710 - INFO - joeynmt.helpers - delete word_level_model_moses/18500.ckpt
2024-05-11 19:38:08,726 - INFO - joeynmt.training - Example #0
2024-05-11 19:38:08,727 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:38:08,727 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:38:08,727 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'così', 'che', '<unk>', 'che', 'il', '<unk>', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', '<unk>', '<unk>', ',', 'ha', '<unk>', 'il', '40', '%', '.', '</s>']
2024-05-11 19:38:08,728 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:38:08,728 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:38:08,728 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due <unk> così che <unk> che il <unk> <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione degli <unk> <unk>, ha <unk> il 40%.
2024-05-11 19:38:08,729 - INFO - joeynmt.training - Example #1
2024-05-11 19:38:08,729 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:38:08,729 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:38:08,729 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', 'in', 'particolare', 'perché', 'non', 'mostra', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:38:08,730 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:38:08,730 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:38:08,730 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo problema in particolare perché non mostra la <unk> del ghiaccio.
2024-05-11 19:38:08,730 - INFO - joeynmt.training - Example #2
2024-05-11 19:38:08,731 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:38:08,731 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:38:08,731 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', '<unk>', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:38:08,731 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:38:08,732 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:38:08,732 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un certo senso, il cuore <unk> del sistema globale.
2024-05-11 19:38:08,732 - INFO - joeynmt.training - Example #3
2024-05-11 19:38:08,732 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:38:08,733 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:38:08,733 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', 'estate', '<unk>', 'e', '<unk>', '.', '</s>']
2024-05-11 19:38:08,733 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:38:08,733 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:38:08,734 - INFO - joeynmt.training - 	Hypothesis: <unk> in estate <unk> e <unk>.
2024-05-11 19:38:08,734 - INFO - joeynmt.training - Example #4
2024-05-11 19:38:08,734 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:38:08,734 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:38:08,734 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', '<unk>', 'che', 'vi', 'mostrerò', 'una', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:38:08,735 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:38:08,735 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:38:08,735 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> <unk> che vi mostrerò una <unk> <unk> di ciò che è successo negli ultimi 25 anni.
2024-05-11 19:38:11,641 - INFO - joeynmt.training - Epoch   8, Step:    23600, Batch Loss:     1.396124, Batch Acc: 0.603751, Tokens per Sec:    20249, Lr: 0.000300
2024-05-11 19:38:15,011 - INFO - joeynmt.training - Epoch   8, Step:    23700, Batch Loss:     1.253789, Batch Acc: 0.600187, Tokens per Sec:    19339, Lr: 0.000300
2024-05-11 19:38:17,900 - INFO - joeynmt.training - Epoch   8, Step:    23800, Batch Loss:     1.158312, Batch Acc: 0.603417, Tokens per Sec:    22635, Lr: 0.000300
2024-05-11 19:38:20,469 - INFO - joeynmt.training - Epoch   8, Step:    23900, Batch Loss:     1.235017, Batch Acc: 0.597652, Tokens per Sec:    24586, Lr: 0.000300
2024-05-11 19:38:23,076 - INFO - joeynmt.training - Epoch   8, Step:    24000, Batch Loss:     1.178802, Batch Acc: 0.601082, Tokens per Sec:    25450, Lr: 0.000300
2024-05-11 19:38:23,076 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:38:23,077 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:38:30,130 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.42, ppl:   4.13, acc:   0.56, generation: 7.0030[sec], evaluation: 0.0000[sec]
2024-05-11 19:38:30,349 - INFO - joeynmt.helpers - delete word_level_model_moses/23000.ckpt
2024-05-11 19:38:30,362 - INFO - joeynmt.training - Example #0
2024-05-11 19:38:30,363 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:38:30,363 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:38:30,363 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'così', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', '<unk>', 'di', '<unk>', '<unk>', ',', 'ha', '<unk>', 'il', '40', '%', '.', '</s>']
2024-05-11 19:38:30,364 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:38:30,365 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:38:30,365 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due <unk> così che <unk> il ghiaccio <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata la <unk> di <unk> <unk>, ha <unk> il 40%.
2024-05-11 19:38:30,365 - INFO - joeynmt.training - Example #1
2024-05-11 19:38:30,365 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:38:30,366 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:38:30,366 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'la', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:38:30,366 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:38:30,367 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:38:30,367 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> la <unk> di questo problema perché non mostra la <unk> del ghiaccio.
2024-05-11 19:38:30,367 - INFO - joeynmt.training - Example #2
2024-05-11 19:38:30,367 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:38:30,367 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:38:30,368 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', '<unk>', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:38:30,368 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:38:30,368 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:38:30,369 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un certo senso, il cuore <unk> del sistema globale.
2024-05-11 19:38:30,369 - INFO - joeynmt.training - Example #3
2024-05-11 19:38:30,369 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:38:30,369 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:38:30,370 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:38:30,370 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:38:30,370 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:38:30,371 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:38:30,371 - INFO - joeynmt.training - Example #4
2024-05-11 19:38:30,371 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:38:30,371 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:38:30,371 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', '<unk>', 'che', 'vi', '<unk>', 'sarà', 'un', '<unk>', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:38:30,372 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:38:30,372 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:38:30,373 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> <unk> che vi <unk> sarà un <unk> di quello che è successo negli ultimi 25 anni.
2024-05-11 19:38:33,228 - INFO - joeynmt.training - Epoch   8, Step:    24100, Batch Loss:     1.296155, Batch Acc: 0.605185, Tokens per Sec:    21505, Lr: 0.000300
2024-05-11 19:38:35,942 - INFO - joeynmt.training - Epoch   8, Step:    24200, Batch Loss:     1.298586, Batch Acc: 0.596309, Tokens per Sec:    24539, Lr: 0.000300
2024-05-11 19:38:38,678 - INFO - joeynmt.training - Epoch   8, Step:    24300, Batch Loss:     1.303492, Batch Acc: 0.598416, Tokens per Sec:    23645, Lr: 0.000300
2024-05-11 19:38:41,788 - INFO - joeynmt.training - Epoch   8, Step:    24400, Batch Loss:     1.380470, Batch Acc: 0.602071, Tokens per Sec:    20405, Lr: 0.000300
2024-05-11 19:38:44,672 - INFO - joeynmt.training - Epoch   8, Step:    24500, Batch Loss:     1.199055, Batch Acc: 0.594412, Tokens per Sec:    22466, Lr: 0.000300
2024-05-11 19:38:44,673 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:38:44,673 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:38:51,139 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.41, ppl:   4.09, acc:   0.56, generation: 6.4155[sec], evaluation: 0.0000[sec]
2024-05-11 19:38:51,139 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:38:51,399 - INFO - joeynmt.helpers - delete word_level_model_moses/21000.ckpt
2024-05-11 19:38:51,417 - INFO - joeynmt.training - Example #0
2024-05-11 19:38:51,418 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:38:51,418 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:38:51,418 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'così', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', 'stati', '<unk>', '<unk>', ',', 'ha', '<unk>', 'il', '40', '%', '.', '</s>']
2024-05-11 19:38:51,419 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:38:51,419 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:38:51,420 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due <unk> così che <unk> il ghiaccio <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione degli stati <unk> <unk>, ha <unk> il 40%.
2024-05-11 19:38:51,420 - INFO - joeynmt.training - Example #1
2024-05-11 19:38:51,420 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:38:51,420 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:38:51,421 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:38:51,421 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:38:51,421 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:38:51,422 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo problema perché non mostra la <unk> del ghiaccio.
2024-05-11 19:38:51,422 - INFO - joeynmt.training - Example #2
2024-05-11 19:38:51,422 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:38:51,423 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:38:51,423 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:38:51,423 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:38:51,424 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:38:51,424 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un certo senso, il cuore del sistema globale.
2024-05-11 19:38:51,424 - INFO - joeynmt.training - Example #3
2024-05-11 19:38:51,425 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:38:51,425 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:38:51,425 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:38:51,425 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:38:51,426 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:38:51,426 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:38:51,426 - INFO - joeynmt.training - Example #4
2024-05-11 19:38:51,427 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:38:51,427 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:38:51,427 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', '<unk>', 'che', 'vi', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:38:51,428 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:38:51,428 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:38:51,428 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> <unk> che vi <unk> <unk> di ciò che è successo negli ultimi 25 anni.
2024-05-11 19:38:54,099 - INFO - joeynmt.training - Epoch   8: total training loss 3784.07
2024-05-11 19:38:54,100 - INFO - joeynmt.training - EPOCH 9
2024-05-11 19:38:54,677 - INFO - joeynmt.training - Epoch   9, Step:    24600, Batch Loss:     1.334216, Batch Acc: 0.626503, Tokens per Sec:    18771, Lr: 0.000300
2024-05-11 19:38:57,752 - INFO - joeynmt.training - Epoch   9, Step:    24700, Batch Loss:     1.146171, Batch Acc: 0.626101, Tokens per Sec:    20907, Lr: 0.000300
2024-05-11 19:39:00,386 - INFO - joeynmt.training - Epoch   9, Step:    24800, Batch Loss:     1.196115, Batch Acc: 0.619735, Tokens per Sec:    24508, Lr: 0.000300
2024-05-11 19:39:03,125 - INFO - joeynmt.training - Epoch   9, Step:    24900, Batch Loss:     1.266518, Batch Acc: 0.620204, Tokens per Sec:    23268, Lr: 0.000300
2024-05-11 19:39:05,659 - INFO - joeynmt.training - Epoch   9, Step:    25000, Batch Loss:     1.126811, Batch Acc: 0.621490, Tokens per Sec:    26576, Lr: 0.000300
2024-05-11 19:39:05,660 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:39:05,660 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:39:13,598 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.42, ppl:   4.15, acc:   0.56, generation: 7.8446[sec], evaluation: 0.0000[sec]
2024-05-11 19:39:13,604 - INFO - joeynmt.training - Example #0
2024-05-11 19:39:13,604 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:39:13,605 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:39:13,605 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'così', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', '.', '</s>']
2024-05-11 19:39:13,605 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:39:13,606 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:39:13,606 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due <unk> così che <unk> il ghiaccio <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione degli ultimi tre milioni di anni.
2024-05-11 19:39:13,606 - INFO - joeynmt.training - Example #1
2024-05-11 19:39:13,606 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:39:13,606 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:39:13,607 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'la', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:39:13,607 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:39:13,607 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:39:13,608 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> la <unk> di questo problema perché non mostra la <unk> del ghiaccio.
2024-05-11 19:39:13,608 - INFO - joeynmt.training - Example #2
2024-05-11 19:39:13,608 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:39:13,608 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:39:13,608 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', '<unk>', 'globale', '.', '</s>']
2024-05-11 19:39:13,609 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:39:13,609 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:39:13,609 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> <unk> è, in un senso, il cuore del sistema <unk> globale.
2024-05-11 19:39:13,609 - INFO - joeynmt.training - Example #3
2024-05-11 19:39:13,610 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:39:13,610 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:39:13,610 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:39:13,610 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:39:13,611 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:39:13,611 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:39:13,611 - INFO - joeynmt.training - Example #4
2024-05-11 19:39:13,611 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:39:13,611 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:39:13,611 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', 'mostrerò', '<unk>', 'un', '<unk>', 'di', 'ciò', 'che', 'è', '<unk>', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:39:13,612 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:39:13,612 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:39:13,612 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi mostrerò <unk> un <unk> di ciò che è <unk> negli ultimi 25 anni.
2024-05-11 19:39:17,029 - INFO - joeynmt.training - Epoch   9, Step:    25100, Batch Loss:     1.173043, Batch Acc: 0.619308, Tokens per Sec:    19235, Lr: 0.000300
2024-05-11 19:39:19,700 - INFO - joeynmt.training - Epoch   9, Step:    25200, Batch Loss:     1.256699, Batch Acc: 0.618255, Tokens per Sec:    24245, Lr: 0.000300
2024-05-11 19:39:22,797 - INFO - joeynmt.training - Epoch   9, Step:    25300, Batch Loss:     1.295649, Batch Acc: 0.609520, Tokens per Sec:    20205, Lr: 0.000300
2024-05-11 19:39:25,607 - INFO - joeynmt.training - Epoch   9, Step:    25400, Batch Loss:     1.252994, Batch Acc: 0.613159, Tokens per Sec:    23005, Lr: 0.000300
2024-05-11 19:39:28,129 - INFO - joeynmt.training - Epoch   9, Step:    25500, Batch Loss:     1.233655, Batch Acc: 0.617452, Tokens per Sec:    25678, Lr: 0.000300
2024-05-11 19:39:28,130 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:39:28,130 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:39:34,527 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.43, ppl:   4.17, acc:   0.56, generation: 6.2931[sec], evaluation: 0.0000[sec]
2024-05-11 19:39:34,534 - INFO - joeynmt.training - Example #0
2024-05-11 19:39:34,534 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:39:34,534 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:39:34,534 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'in', 'modo', 'che', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', '.', '</s>']
2024-05-11 19:39:34,535 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:39:34,535 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:39:34,536 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due <unk> in modo che il ghiaccio <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione degli ultimi tre milioni di anni.
2024-05-11 19:39:34,536 - INFO - joeynmt.training - Example #1
2024-05-11 19:39:34,536 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:39:34,536 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:39:34,536 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'la', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:39:34,537 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:39:34,537 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:39:34,538 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> la <unk> di questo problema perché non mostra la <unk> del ghiaccio.
2024-05-11 19:39:34,538 - INFO - joeynmt.training - Example #2
2024-05-11 19:39:34,538 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:39:34,538 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:39:34,538 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', '<unk>', 'del', 'clima', 'globale', '.', '</s>']
2024-05-11 19:39:34,539 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:39:34,539 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:39:34,539 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un senso, il cuore del sistema <unk> del clima globale.
2024-05-11 19:39:34,540 - INFO - joeynmt.training - Example #3
2024-05-11 19:39:34,540 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:39:34,540 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:39:34,540 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:39:34,541 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:39:34,541 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:39:34,541 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:39:34,541 - INFO - joeynmt.training - Example #4
2024-05-11 19:39:34,542 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:39:34,542 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:39:34,542 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', 'mostrerò', 'sarà', 'un', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', '<unk>', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:39:34,543 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:39:34,543 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:39:34,543 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi mostrerò sarà un <unk> <unk> di ciò che è <unk> negli ultimi 25 anni.
2024-05-11 19:39:38,072 - INFO - joeynmt.training - Epoch   9, Step:    25600, Batch Loss:     1.066399, Batch Acc: 0.616398, Tokens per Sec:    18502, Lr: 0.000300
2024-05-11 19:39:40,741 - INFO - joeynmt.training - Epoch   9, Step:    25700, Batch Loss:     1.222672, Batch Acc: 0.613856, Tokens per Sec:    24614, Lr: 0.000300
2024-05-11 19:39:43,383 - INFO - joeynmt.training - Epoch   9, Step:    25800, Batch Loss:     1.288951, Batch Acc: 0.612201, Tokens per Sec:    24183, Lr: 0.000300
2024-05-11 19:39:46,029 - INFO - joeynmt.training - Epoch   9, Step:    25900, Batch Loss:     1.279428, Batch Acc: 0.614913, Tokens per Sec:    24829, Lr: 0.000300
2024-05-11 19:39:48,840 - INFO - joeynmt.training - Epoch   9, Step:    26000, Batch Loss:     1.121789, Batch Acc: 0.612309, Tokens per Sec:    22409, Lr: 0.000300
2024-05-11 19:39:48,840 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:39:48,841 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:39:55,396 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.42, ppl:   4.15, acc:   0.55, generation: 6.5055[sec], evaluation: 0.0000[sec]
2024-05-11 19:39:55,401 - INFO - joeynmt.training - Example #0
2024-05-11 19:39:55,402 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:39:55,402 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:39:55,402 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'in', 'modo', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'di', '<unk>', '<unk>', 'gli', 'stati', '<unk>', ',', 'dal', '40', '%', '.', '</s>']
2024-05-11 19:39:55,403 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:39:55,403 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:39:55,404 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due <unk> in modo che <unk> il ghiaccio <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione di <unk> <unk> gli stati <unk>, dal 40%.
2024-05-11 19:39:55,404 - INFO - joeynmt.training - Example #1
2024-05-11 19:39:55,404 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:39:55,404 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:39:55,405 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'particolare', 'problema', 'perché', 'non', 'mostra', 'le', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:39:55,405 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:39:55,406 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:39:55,406 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo particolare problema perché non mostra le <unk> del ghiaccio.
2024-05-11 19:39:55,406 - INFO - joeynmt.training - Example #2
2024-05-11 19:39:55,406 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:39:55,407 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:39:55,407 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', '<unk>', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:39:55,407 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:39:55,408 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:39:55,408 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un senso, il cuore <unk> del sistema globale.
2024-05-11 19:39:55,408 - INFO - joeynmt.training - Example #3
2024-05-11 19:39:55,408 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:39:55,408 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:39:55,409 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:39:55,409 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:39:55,409 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:39:55,410 - INFO - joeynmt.training - 	Hypothesis: <unk> in estate.
2024-05-11 19:39:55,410 - INFO - joeynmt.training - Example #4
2024-05-11 19:39:55,410 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:39:55,410 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:39:55,411 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', '<unk>', 'sarà', 'un', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', '<unk>', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:39:55,411 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:39:55,411 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:39:55,412 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che <unk> sarà un <unk> <unk> di ciò che è <unk> negli ultimi 25 anni.
2024-05-11 19:39:58,219 - INFO - joeynmt.training - Epoch   9, Step:    26100, Batch Loss:     1.224077, Batch Acc: 0.613970, Tokens per Sec:    23505, Lr: 0.000300
2024-05-11 19:40:00,895 - INFO - joeynmt.training - Epoch   9, Step:    26200, Batch Loss:     1.439088, Batch Acc: 0.615028, Tokens per Sec:    24639, Lr: 0.000300
2024-05-11 19:40:04,021 - INFO - joeynmt.training - Epoch   9, Step:    26300, Batch Loss:     1.243202, Batch Acc: 0.613739, Tokens per Sec:    21368, Lr: 0.000300
2024-05-11 19:40:06,860 - INFO - joeynmt.training - Epoch   9, Step:    26400, Batch Loss:     1.284273, Batch Acc: 0.611158, Tokens per Sec:    23193, Lr: 0.000300
2024-05-11 19:40:09,426 - INFO - joeynmt.training - Epoch   9, Step:    26500, Batch Loss:     1.355052, Batch Acc: 0.604938, Tokens per Sec:    24427, Lr: 0.000300
2024-05-11 19:40:09,427 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:40:09,427 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:40:15,827 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.42, ppl:   4.16, acc:   0.56, generation: 6.3054[sec], evaluation: 0.0000[sec]
2024-05-11 19:40:15,832 - INFO - joeynmt.training - Example #0
2024-05-11 19:40:15,833 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:40:15,833 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:40:15,833 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'in', 'modo', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', 'stati', '<unk>', ',', 'ha', '<unk>', 'il', '40', '%', '.', '</s>']
2024-05-11 19:40:15,834 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:40:15,834 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:40:15,834 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due <unk> in modo che <unk> il ghiaccio <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione degli stati <unk>, ha <unk> il 40%.
2024-05-11 19:40:15,834 - INFO - joeynmt.training - Example #1
2024-05-11 19:40:15,835 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:40:15,835 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:40:15,835 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:40:15,836 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:40:15,836 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:40:15,836 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo problema perché non mostra la <unk> del ghiaccio.
2024-05-11 19:40:15,836 - INFO - joeynmt.training - Example #2
2024-05-11 19:40:15,836 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:40:15,836 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:40:15,837 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:40:15,837 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:40:15,837 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:40:15,838 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un certo senso, il cuore del sistema globale.
2024-05-11 19:40:15,838 - INFO - joeynmt.training - Example #3
2024-05-11 19:40:15,838 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:40:15,838 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:40:15,838 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:40:15,839 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:40:15,839 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:40:15,839 - INFO - joeynmt.training - 	Hypothesis: <unk> in estate.
2024-05-11 19:40:15,839 - INFO - joeynmt.training - Example #4
2024-05-11 19:40:15,839 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:40:15,840 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:40:15,840 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', 'mostro', 'sarà', 'una', '<unk>', '<unk>', 'di', 'quello', 'che', 'è', '<unk>', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:40:15,840 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:40:15,840 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:40:15,841 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi mostro sarà una <unk> <unk> di quello che è <unk> negli ultimi 25 anni.
2024-05-11 19:40:19,364 - INFO - joeynmt.training - Epoch   9, Step:    26600, Batch Loss:     1.218665, Batch Acc: 0.609486, Tokens per Sec:    18867, Lr: 0.000300
2024-05-11 19:40:22,019 - INFO - joeynmt.training - Epoch   9, Step:    26700, Batch Loss:     1.204147, Batch Acc: 0.606701, Tokens per Sec:    24780, Lr: 0.000300
2024-05-11 19:40:24,647 - INFO - joeynmt.training - Epoch   9, Step:    26800, Batch Loss:     1.183765, Batch Acc: 0.607117, Tokens per Sec:    24764, Lr: 0.000300
2024-05-11 19:40:27,236 - INFO - joeynmt.training - Epoch   9, Step:    26900, Batch Loss:     1.165130, Batch Acc: 0.606393, Tokens per Sec:    25232, Lr: 0.000300
2024-05-11 19:40:30,025 - INFO - joeynmt.training - Epoch   9, Step:    27000, Batch Loss:     1.158544, Batch Acc: 0.612579, Tokens per Sec:    22808, Lr: 0.000300
2024-05-11 19:40:30,026 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:40:30,026 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:40:36,757 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.41, ppl:   4.10, acc:   0.56, generation: 6.6824[sec], evaluation: 0.0000[sec]
2024-05-11 19:40:37,001 - INFO - joeynmt.helpers - delete word_level_model_moses/23500.ckpt
2024-05-11 19:40:37,017 - INFO - joeynmt.training - Example #0
2024-05-11 19:40:37,017 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:40:37,018 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:40:37,018 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'così', '<unk>', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', '<unk>', '<unk>', ',', 'ha', '<unk>', 'il', '40', '%', '.', '</s>']
2024-05-11 19:40:37,018 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:40:37,019 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:40:37,019 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due <unk> così <unk> che <unk> il ghiaccio <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione degli <unk> <unk>, ha <unk> il 40%.
2024-05-11 19:40:37,019 - INFO - joeynmt.training - Example #1
2024-05-11 19:40:37,019 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:40:37,020 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:40:37,020 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'il', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:40:37,020 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:40:37,021 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:40:37,021 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo problema perché non mostra il <unk> del ghiaccio.
2024-05-11 19:40:37,021 - INFO - joeynmt.training - Example #2
2024-05-11 19:40:37,022 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:40:37,022 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:40:37,022 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', '<unk>', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:40:37,022 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:40:37,023 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:40:37,023 - INFO - joeynmt.training - 	Hypothesis: La <unk> <unk> <unk> è, in un senso, il cuore del sistema globale.
2024-05-11 19:40:37,023 - INFO - joeynmt.training - Example #3
2024-05-11 19:40:37,024 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:40:37,024 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:40:37,024 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:40:37,024 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:40:37,025 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:40:37,025 - INFO - joeynmt.training - 	Hypothesis: <unk> in estate.
2024-05-11 19:40:37,025 - INFO - joeynmt.training - Example #4
2024-05-11 19:40:37,025 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:40:37,025 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:40:37,026 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', '<unk>', 'un', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:40:37,026 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:40:37,026 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:40:37,027 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi <unk> un <unk> <unk> di ciò che è successo negli ultimi 25 anni.
2024-05-11 19:40:39,889 - INFO - joeynmt.training - Epoch   9, Step:    27100, Batch Loss:     1.319867, Batch Acc: 0.606080, Tokens per Sec:    20948, Lr: 0.000300
2024-05-11 19:40:42,629 - INFO - joeynmt.training - Epoch   9, Step:    27200, Batch Loss:     1.226515, Batch Acc: 0.608347, Tokens per Sec:    23603, Lr: 0.000300
2024-05-11 19:40:45,960 - INFO - joeynmt.training - Epoch   9, Step:    27300, Batch Loss:     1.259163, Batch Acc: 0.604466, Tokens per Sec:    19239, Lr: 0.000300
2024-05-11 19:40:48,718 - INFO - joeynmt.training - Epoch   9, Step:    27400, Batch Loss:     1.265643, Batch Acc: 0.605464, Tokens per Sec:    23190, Lr: 0.000300
2024-05-11 19:40:51,421 - INFO - joeynmt.training - Epoch   9, Step:    27500, Batch Loss:     1.194990, Batch Acc: 0.608823, Tokens per Sec:    24169, Lr: 0.000300
2024-05-11 19:40:51,421 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:40:51,422 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:40:57,890 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.41, ppl:   4.11, acc:   0.56, generation: 6.3813[sec], evaluation: 0.0000[sec]
2024-05-11 19:40:58,136 - INFO - joeynmt.helpers - delete word_level_model_moses/20500.ckpt
2024-05-11 19:40:58,166 - INFO - joeynmt.training - Example #0
2024-05-11 19:40:58,166 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:40:58,166 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:40:58,167 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', '<unk>', 'così', 'che', '<unk>', 'che', 'il', '<unk>', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', '<unk>', '<unk>', '<unk>', ',', 'ha', '<unk>', 'dal', '40', '%', '.', '</s>']
2024-05-11 19:40:58,167 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:40:58,167 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:40:58,168 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due <unk> così che <unk> che il <unk> <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione degli <unk> <unk> <unk>, ha <unk> dal 40%.
2024-05-11 19:40:58,168 - INFO - joeynmt.training - Example #1
2024-05-11 19:40:58,168 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:40:58,168 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:40:58,168 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', '<unk>', 'la', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:40:58,169 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:40:58,169 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:40:58,169 - INFO - joeynmt.training - 	Hypothesis: Ma questa <unk> la <unk> di questo problema perché non mostra la <unk> del ghiaccio.
2024-05-11 19:40:58,170 - INFO - joeynmt.training - Example #2
2024-05-11 19:40:58,170 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:40:58,170 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:40:58,170 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:40:58,171 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:40:58,171 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:40:58,171 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> <unk> è, in un certo senso, il cuore del sistema globale.
2024-05-11 19:40:58,171 - INFO - joeynmt.training - Example #3
2024-05-11 19:40:58,171 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:40:58,172 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:40:58,172 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:40:58,172 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:40:58,172 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:40:58,173 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:40:58,173 - INFO - joeynmt.training - Example #4
2024-05-11 19:40:58,173 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:40:58,173 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:40:58,173 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', 'mostrerò', 'una', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:40:58,174 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:40:58,174 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:40:58,174 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi mostrerò una <unk> <unk> di ciò che è successo negli ultimi 25 anni.
2024-05-11 19:41:01,429 - INFO - joeynmt.training - Epoch   9, Step:    27600, Batch Loss:     1.153492, Batch Acc: 0.609618, Tokens per Sec:    19077, Lr: 0.000300
2024-05-11 19:41:03,108 - INFO - joeynmt.training - Epoch   9: total training loss 3721.75
2024-05-11 19:41:03,109 - INFO - joeynmt.training - EPOCH 10
2024-05-11 19:41:04,083 - INFO - joeynmt.training - Epoch  10, Step:    27700, Batch Loss:     1.208170, Batch Acc: 0.628247, Tokens per Sec:    24351, Lr: 0.000300
2024-05-11 19:41:06,904 - INFO - joeynmt.training - Epoch  10, Step:    27800, Batch Loss:     1.199082, Batch Acc: 0.631299, Tokens per Sec:    23340, Lr: 0.000300
2024-05-11 19:41:09,568 - INFO - joeynmt.training - Epoch  10, Step:    27900, Batch Loss:     1.104695, Batch Acc: 0.629898, Tokens per Sec:    23947, Lr: 0.000300
2024-05-11 19:41:12,635 - INFO - joeynmt.training - Epoch  10, Step:    28000, Batch Loss:     1.118596, Batch Acc: 0.624326, Tokens per Sec:    21065, Lr: 0.000300
2024-05-11 19:41:12,635 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:41:12,635 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:41:18,948 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.42, ppl:   4.13, acc:   0.56, generation: 6.2634[sec], evaluation: 0.0000[sec]
2024-05-11 19:41:18,953 - INFO - joeynmt.training - Example #0
2024-05-11 19:41:18,954 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:41:18,954 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:41:18,954 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'questi', 'due', '<unk>', '<unk>', 'così', 'che', '<unk>', 'che', 'il', '<unk>', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', 'stati', '<unk>', '<unk>', ',', 'il', '40', '%', 'del', '<unk>', '.', '</s>']
2024-05-11 19:41:18,955 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:41:18,955 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:41:18,955 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due <unk> <unk> così che <unk> che il <unk> <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione degli stati <unk> <unk>, il 40% del <unk>.
2024-05-11 19:41:18,956 - INFO - joeynmt.training - Example #1
2024-05-11 19:41:18,956 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:41:18,956 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:41:18,956 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:41:18,957 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:41:18,957 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:41:18,957 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo problema perché non mostra la <unk> del ghiaccio.
2024-05-11 19:41:18,958 - INFO - joeynmt.training - Example #2
2024-05-11 19:41:18,958 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:41:18,958 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:41:18,958 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2024-05-11 19:41:18,959 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:41:18,959 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:41:18,959 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un certo senso, il cuore del sistema climatico.
2024-05-11 19:41:18,959 - INFO - joeynmt.training - Example #3
2024-05-11 19:41:18,960 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:41:18,960 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:41:18,960 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:41:18,960 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:41:18,961 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:41:18,961 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:41:18,961 - INFO - joeynmt.training - Example #4
2024-05-11 19:41:18,962 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:41:18,962 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:41:18,962 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', 'mostrerò', 'sarà', 'una', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:41:18,962 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:41:18,963 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:41:18,963 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi mostrerò sarà una <unk> <unk> di ciò che è successo negli ultimi 25 anni.
2024-05-11 19:41:21,773 - INFO - joeynmt.training - Epoch  10, Step:    28100, Batch Loss:     1.162806, Batch Acc: 0.627968, Tokens per Sec:    23312, Lr: 0.000300
2024-05-11 19:41:24,670 - INFO - joeynmt.training - Epoch  10, Step:    28200, Batch Loss:     1.283809, Batch Acc: 0.627959, Tokens per Sec:    22731, Lr: 0.000300
2024-05-11 19:41:27,840 - INFO - joeynmt.training - Epoch  10, Step:    28300, Batch Loss:     1.249127, Batch Acc: 0.620085, Tokens per Sec:    21189, Lr: 0.000300
2024-05-11 19:41:30,416 - INFO - joeynmt.training - Epoch  10, Step:    28400, Batch Loss:     1.136474, Batch Acc: 0.624717, Tokens per Sec:    24912, Lr: 0.000300
2024-05-11 19:41:33,024 - INFO - joeynmt.training - Epoch  10, Step:    28500, Batch Loss:     1.161188, Batch Acc: 0.622796, Tokens per Sec:    24773, Lr: 0.000300
2024-05-11 19:41:33,025 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:41:33,025 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:41:39,917 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.42, ppl:   4.14, acc:   0.56, generation: 6.8022[sec], evaluation: 0.0000[sec]
2024-05-11 19:41:39,926 - INFO - joeynmt.training - Example #0
2024-05-11 19:41:39,927 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:41:39,927 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:41:39,927 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'così', '<unk>', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'delle', '<unk>', '<unk>', ',', 'ha', '<unk>', 'dal', '40', '%', '.', '</s>']
2024-05-11 19:41:39,928 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:41:39,928 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:41:39,928 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due <unk> così <unk> che <unk> il ghiaccio <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione delle <unk> <unk>, ha <unk> dal 40%.
2024-05-11 19:41:39,928 - INFO - joeynmt.training - Example #1
2024-05-11 19:41:39,929 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:41:39,929 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:41:39,929 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', ',', 'perché', 'non', 'mostra', 'le', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:41:39,930 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:41:39,930 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:41:39,930 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo problema, perché non mostra le <unk> del ghiaccio.
2024-05-11 19:41:39,930 - INFO - joeynmt.training - Example #2
2024-05-11 19:41:39,931 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:41:39,931 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:41:39,931 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:41:39,932 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:41:39,932 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:41:39,932 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un certo senso, il cuore del sistema globale.
2024-05-11 19:41:39,932 - INFO - joeynmt.training - Example #3
2024-05-11 19:41:39,933 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:41:39,933 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:41:39,933 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:41:39,933 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:41:39,934 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:41:39,934 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:41:39,934 - INFO - joeynmt.training - Example #4
2024-05-11 19:41:39,934 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:41:39,935 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:41:39,935 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', 'mostrerò', 'sarà', 'una', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:41:39,935 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:41:39,936 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:41:39,936 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi mostrerò sarà una <unk> <unk> di ciò che è successo negli ultimi 25 anni.
2024-05-11 19:41:43,076 - INFO - joeynmt.training - Epoch  10, Step:    28600, Batch Loss:     1.093741, Batch Acc: 0.619579, Tokens per Sec:    20370, Lr: 0.000300
2024-05-11 19:41:45,705 - INFO - joeynmt.training - Epoch  10, Step:    28700, Batch Loss:     1.262941, Batch Acc: 0.625924, Tokens per Sec:    24953, Lr: 0.000300
2024-05-11 19:41:48,382 - INFO - joeynmt.training - Epoch  10, Step:    28800, Batch Loss:     1.217402, Batch Acc: 0.622040, Tokens per Sec:    25231, Lr: 0.000300
2024-05-11 19:41:51,001 - INFO - joeynmt.training - Epoch  10, Step:    28900, Batch Loss:     1.097681, Batch Acc: 0.625170, Tokens per Sec:    25603, Lr: 0.000300
2024-05-11 19:41:54,166 - INFO - joeynmt.training - Epoch  10, Step:    29000, Batch Loss:     1.210909, Batch Acc: 0.623332, Tokens per Sec:    20778, Lr: 0.000300
2024-05-11 19:41:54,166 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:41:54,167 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:42:00,961 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.41, ppl:   4.11, acc:   0.56, generation: 6.7426[sec], evaluation: 0.0000[sec]
2024-05-11 19:42:01,194 - INFO - joeynmt.helpers - delete word_level_model_moses/24000.ckpt
2024-05-11 19:42:01,211 - INFO - joeynmt.training - Example #0
2024-05-11 19:42:01,212 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:42:01,212 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:42:01,212 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'così', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'ha', '<unk>', 'il', '40', '%', '.', '</s>']
2024-05-11 19:42:01,213 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:42:01,213 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:42:01,213 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due <unk> così che <unk> il ghiaccio <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione degli <unk> <unk> <unk> <unk>, ha <unk> il 40%.
2024-05-11 19:42:01,214 - INFO - joeynmt.training - Example #1
2024-05-11 19:42:01,214 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:42:01,214 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:42:01,214 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'la', '<unk>', 'di', 'questo', 'problema', 'in', 'particolare', 'perché', 'non', 'mostra', 'le', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:42:01,215 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:42:01,215 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:42:01,215 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> la <unk> di questo problema in particolare perché non mostra le <unk> del ghiaccio.
2024-05-11 19:42:01,216 - INFO - joeynmt.training - Example #2
2024-05-11 19:42:01,216 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:42:01,216 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:42:01,216 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', '<unk>', 'del', 'sistema', '<unk>', 'globale', '.', '</s>']
2024-05-11 19:42:01,217 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:42:01,217 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:42:01,217 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un certo senso, il cuore <unk> del sistema <unk> globale.
2024-05-11 19:42:01,217 - INFO - joeynmt.training - Example #3
2024-05-11 19:42:01,218 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:42:01,218 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:42:01,218 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', 'estate', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:42:01,218 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:42:01,219 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:42:01,219 - INFO - joeynmt.training - 	Hypothesis: <unk> in estate e <unk> in estate.
2024-05-11 19:42:01,219 - INFO - joeynmt.training - Example #4
2024-05-11 19:42:01,220 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:42:01,220 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:42:01,220 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', 'mostrerò', 'una', '<unk>', '<unk>', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:42:01,220 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:42:01,221 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:42:01,221 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi mostrerò una <unk> <unk> di quello che è successo negli ultimi 25 anni.
2024-05-11 19:42:04,052 - INFO - joeynmt.training - Epoch  10, Step:    29100, Batch Loss:     1.040176, Batch Acc: 0.619940, Tokens per Sec:    21095, Lr: 0.000210
2024-05-11 19:42:07,141 - INFO - joeynmt.training - Epoch  10, Step:    29200, Batch Loss:     1.199517, Batch Acc: 0.622722, Tokens per Sec:    20699, Lr: 0.000210
2024-05-11 19:42:10,166 - INFO - joeynmt.training - Epoch  10, Step:    29300, Batch Loss:     1.117303, Batch Acc: 0.623542, Tokens per Sec:    21430, Lr: 0.000210
2024-05-11 19:42:12,903 - INFO - joeynmt.training - Epoch  10, Step:    29400, Batch Loss:     1.226483, Batch Acc: 0.628948, Tokens per Sec:    24286, Lr: 0.000210
2024-05-11 19:42:15,509 - INFO - joeynmt.training - Epoch  10, Step:    29500, Batch Loss:     1.253874, Batch Acc: 0.628490, Tokens per Sec:    24715, Lr: 0.000210
2024-05-11 19:42:15,510 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:42:15,510 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:42:22,212 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.40, ppl:   4.06, acc:   0.56, generation: 6.6270[sec], evaluation: 0.0000[sec]
2024-05-11 19:42:22,213 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:42:22,446 - INFO - joeynmt.helpers - delete word_level_model_moses/29000.ckpt
2024-05-11 19:42:22,459 - INFO - joeynmt.training - Example #0
2024-05-11 19:42:22,460 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:42:22,460 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:42:22,460 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'questi', 'due', '<unk>', '<unk>', '<unk>', 'che', 'il', '<unk>', '<unk>', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'delle', '<unk>', '<unk>', ',', 'è', '<unk>', 'da', '40', '%', '.', '</s>']
2024-05-11 19:42:22,461 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:42:22,461 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:42:22,461 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due <unk> <unk> <unk> che il <unk> <unk> <unk>, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione delle <unk> <unk>, è <unk> da 40%.
2024-05-11 19:42:22,462 - INFO - joeynmt.training - Example #1
2024-05-11 19:42:22,462 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:42:22,462 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:42:22,462 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'particolare', 'problema', 'perché', 'non', 'mostra', 'l&apos;', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:42:22,463 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:42:22,463 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:42:22,463 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo particolare problema perché non mostra l' <unk> del ghiaccio.
2024-05-11 19:42:22,464 - INFO - joeynmt.training - Example #2
2024-05-11 19:42:22,464 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:42:22,464 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:42:22,464 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', '<unk>', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:42:22,465 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:42:22,465 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:42:22,465 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un certo senso, il cuore <unk> del sistema globale.
2024-05-11 19:42:22,465 - INFO - joeynmt.training - Example #3
2024-05-11 19:42:22,466 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:42:22,466 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:42:22,466 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'd&apos;', 'estate', '.', '</s>']
2024-05-11 19:42:22,466 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:42:22,467 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:42:22,467 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> d'estate.
2024-05-11 19:42:22,467 - INFO - joeynmt.training - Example #4
2024-05-11 19:42:22,467 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:42:22,468 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:42:22,468 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', 'mostrerò', 'sarà', 'una', '<unk>', '<unk>', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:42:22,468 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:42:22,469 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:42:22,469 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi mostrerò sarà una <unk> <unk> di quello che è successo negli ultimi 25 anni.
2024-05-11 19:42:25,354 - INFO - joeynmt.training - Epoch  10, Step:    29600, Batch Loss:     1.231545, Batch Acc: 0.620734, Tokens per Sec:    20889, Lr: 0.000210
2024-05-11 19:42:27,990 - INFO - joeynmt.training - Epoch  10, Step:    29700, Batch Loss:     1.197027, Batch Acc: 0.628052, Tokens per Sec:    24447, Lr: 0.000210
2024-05-11 19:42:31,153 - INFO - joeynmt.training - Epoch  10, Step:    29800, Batch Loss:     1.100613, Batch Acc: 0.626273, Tokens per Sec:    20934, Lr: 0.000210
2024-05-11 19:42:34,681 - INFO - joeynmt.training - Epoch  10, Step:    29900, Batch Loss:     1.329942, Batch Acc: 0.627682, Tokens per Sec:    18684, Lr: 0.000210
2024-05-11 19:42:37,635 - INFO - joeynmt.training - Epoch  10, Step:    30000, Batch Loss:     1.272364, Batch Acc: 0.627467, Tokens per Sec:    22189, Lr: 0.000210
2024-05-11 19:42:37,635 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:42:37,636 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:42:43,664 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.39, ppl:   4.03, acc:   0.57, generation: 5.9808[sec], evaluation: 0.0000[sec]
2024-05-11 19:42:43,665 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-11 19:42:43,888 - INFO - joeynmt.helpers - delete word_level_model_moses/27500.ckpt
2024-05-11 19:42:43,915 - INFO - joeynmt.training - Example #0
2024-05-11 19:42:43,915 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:42:43,916 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:42:43,916 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'così', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stato', 'la', 'dimensione', 'dell&apos;', '<unk>', '<unk>', ',', '<unk>', 'dal', '40', '%', '.', '</s>']
2024-05-11 19:42:43,916 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:42:43,917 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:42:43,917 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due <unk> così che <unk> il ghiaccio <unk>, che per la maggior parte degli ultimi tre milioni di anni è stato la dimensione dell' <unk> <unk>, <unk> dal 40%.
2024-05-11 19:42:43,918 - INFO - joeynmt.training - Example #1
2024-05-11 19:42:43,918 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:42:43,918 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:42:43,919 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'il', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:42:43,919 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:42:43,919 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:42:43,920 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo problema perché non mostra il <unk> del ghiaccio.
2024-05-11 19:42:43,920 - INFO - joeynmt.training - Example #2
2024-05-11 19:42:43,920 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:42:43,920 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:42:43,920 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', '<unk>', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:42:43,921 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:42:43,921 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:42:43,921 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un senso, il cuore <unk> del sistema globale.
2024-05-11 19:42:43,922 - INFO - joeynmt.training - Example #3
2024-05-11 19:42:43,922 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:42:43,922 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:42:43,922 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', '<unk>', 'e', '<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:42:43,923 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:42:43,923 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:42:43,923 - INFO - joeynmt.training - 	Hypothesis: <unk> in <unk> e <unk> in estate.
2024-05-11 19:42:43,923 - INFO - joeynmt.training - Example #4
2024-05-11 19:42:43,924 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:42:43,924 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:42:43,924 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', 'mostrerò', 'una', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:42:43,924 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:42:43,925 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:42:43,925 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi mostrerò una <unk> <unk> di ciò che è successo negli ultimi 25 anni.
2024-05-11 19:42:46,873 - INFO - joeynmt.training - Epoch  10, Step:    30100, Batch Loss:     1.224500, Batch Acc: 0.627191, Tokens per Sec:    20096, Lr: 0.000210
2024-05-11 19:42:50,209 - INFO - joeynmt.training - Epoch  10, Step:    30200, Batch Loss:     1.170048, Batch Acc: 0.621845, Tokens per Sec:    19055, Lr: 0.000210
2024-05-11 19:42:52,998 - INFO - joeynmt.training - Epoch  10, Step:    30300, Batch Loss:     1.147311, Batch Acc: 0.625432, Tokens per Sec:    23877, Lr: 0.000210
2024-05-11 19:42:55,633 - INFO - joeynmt.training - Epoch  10, Step:    30400, Batch Loss:     1.264473, Batch Acc: 0.623546, Tokens per Sec:    24891, Lr: 0.000210
2024-05-11 19:42:58,198 - INFO - joeynmt.training - Epoch  10, Step:    30500, Batch Loss:     1.292455, Batch Acc: 0.620296, Tokens per Sec:    25172, Lr: 0.000210
2024-05-11 19:42:58,199 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:42:58,199 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:43:05,209 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.40, ppl:   4.04, acc:   0.56, generation: 6.9576[sec], evaluation: 0.0000[sec]
2024-05-11 19:43:05,427 - INFO - joeynmt.helpers - delete word_level_model_moses/21500.ckpt
2024-05-11 19:43:05,443 - INFO - joeynmt.training - Example #0
2024-05-11 19:43:05,444 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-11 19:43:05,444 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-11 19:43:05,444 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'questi', 'due', '<unk>', 'in', 'modo', 'che', '<unk>', 'il', 'ghiaccio', '<unk>', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', '<unk>', '<unk>', '<unk>', ',', 'è', '<unk>', 'dal', '40', '%', '.', '</s>']
2024-05-11 19:43:05,445 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-11 19:43:05,445 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-11 19:43:05,445 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due <unk> in modo che <unk> il ghiaccio <unk> che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione degli <unk> <unk> <unk>, è <unk> dal 40%.
2024-05-11 19:43:05,445 - INFO - joeynmt.training - Example #1
2024-05-11 19:43:05,446 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-11 19:43:05,446 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-11 19:43:05,446 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', '<unk>', 'il', '<unk>', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', '<unk>', 'del', 'ghiaccio', '.', '</s>']
2024-05-11 19:43:05,446 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-11 19:43:05,447 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-11 19:43:05,447 - INFO - joeynmt.training - 	Hypothesis: Ma questo <unk> il <unk> di questo problema perché non mostra la <unk> del ghiaccio.
2024-05-11 19:43:05,447 - INFO - joeynmt.training - Example #2
2024-05-11 19:43:05,447 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-11 19:43:05,448 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-11 19:43:05,448 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '<unk>', '<unk>', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', '<unk>', 'del', 'sistema', 'globale', '.', '</s>']
2024-05-11 19:43:05,448 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-11 19:43:05,449 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-11 19:43:05,449 - INFO - joeynmt.training - 	Hypothesis: Il <unk> <unk> è, in un certo senso, il cuore <unk> del sistema globale.
2024-05-11 19:43:05,450 - INFO - joeynmt.training - Example #3
2024-05-11 19:43:05,450 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-11 19:43:05,450 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-11 19:43:05,450 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'in', 'estate', '.', '</s>']
2024-05-11 19:43:05,451 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-11 19:43:05,451 - INFO - joeynmt.training - 	Reference:  Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-11 19:43:05,452 - INFO - joeynmt.training - 	Hypothesis: <unk> in estate.
2024-05-11 19:43:05,452 - INFO - joeynmt.training - Example #4
2024-05-11 19:43:05,452 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-11 19:43:05,452 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-11 19:43:05,452 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', '<unk>', 'che', 'vi', 'mostrerò', 'una', '<unk>', '<unk>', 'di', 'ciò', 'che', 'è', '<unk>', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2024-05-11 19:43:05,453 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-11 19:43:05,453 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-11 19:43:05,453 - INFO - joeynmt.training - 	Hypothesis: La prossima <unk> che vi mostrerò una <unk> <unk> di ciò che è <unk> negli ultimi 25 anni.
2024-05-11 19:43:08,282 - INFO - joeynmt.training - Epoch  10, Step:    30600, Batch Loss:     1.132445, Batch Acc: 0.622219, Tokens per Sec:    21203, Lr: 0.000210
2024-05-11 19:43:11,013 - INFO - joeynmt.training - Epoch  10, Step:    30700, Batch Loss:     1.193284, Batch Acc: 0.625818, Tokens per Sec:    23907, Lr: 0.000210
2024-05-11 19:43:11,855 - INFO - joeynmt.training - Epoch  10: total training loss 3580.56
2024-05-11 19:43:11,856 - INFO - joeynmt.training - Training ended after  10 epochs.
2024-05-11 19:43:11,856 - INFO - joeynmt.training - Best validation result (greedy) at step    30000:   4.03 ppl.
2024-05-11 19:43:11,880 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-11 19:43:11,975 - INFO - joeynmt.model - Enc-dec model built.
2024-05-11 19:43:12,152 - INFO - joeynmt.helpers - Load model from /content/drive/MyDrive/mt5/mt-exercise-5/word_level_model_moses/30000.ckpt.
2024-05-11 19:43:12,177 - INFO - joeynmt.prediction - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=2004),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=2004),
	loss_function=None)
2024-05-11 19:43:12,178 - INFO - joeynmt.prediction - Decoding on dev set...
2024-05-11 19:43:12,179 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:43:12,179 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:43:22,249 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 10.0217[sec], evaluation: 0.0000[sec]
2024-05-11 19:43:22,255 - INFO - joeynmt.prediction - Translations saved to: /content/drive/MyDrive/mt5/mt-exercise-5/word_level_model_moses/00030000.hyps.dev.
2024-05-11 19:43:22,256 - INFO - joeynmt.prediction - Decoding on test set...
2024-05-11 19:43:22,256 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-11 19:43:22,257 - INFO - joeynmt.prediction - Predicting 1566 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-11 19:43:36,191 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 13.8527[sec], evaluation: 0.0000[sec]
2024-05-11 19:43:36,200 - INFO - joeynmt.prediction - Translations saved to: /content/drive/MyDrive/mt5/mt-exercise-5/word_level_model_moses/00030000.hyps.test.
