2024-05-19 11:30:06,303 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 11:30:06,307 - INFO - joeynmt.helpers -                           cfg.name : bpe_model_2000
2024-05-19 11:30:06,307 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0
2024-05-19 11:30:06,308 - INFO - joeynmt.helpers -                     cfg.data.train : data2/train
2024-05-19 11:30:06,308 - INFO - joeynmt.helpers -                       cfg.data.dev : data2/dev
2024-05-19 11:30:06,308 - INFO - joeynmt.helpers -                      cfg.data.test : data2/test
2024-05-19 11:30:06,308 - INFO - joeynmt.helpers -              cfg.data.dataset_type : plain
2024-05-19 11:30:06,309 - INFO - joeynmt.helpers -                  cfg.data.src.lang : en
2024-05-19 11:30:06,309 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe
2024-05-19 11:30:06,309 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False
2024-05-19 11:30:06,309 - INFO - joeynmt.helpers -       cfg.data.src.max_sent_length : 100
2024-05-19 11:30:06,309 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : data2/bpe_vocab2000.en
2024-05-19 11:30:06,310 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : subword-nmt
2024-05-19 11:30:06,310 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.pretokenizer : moses
2024-05-19 11:30:06,310 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.num_merges : 2000
2024-05-19 11:30:06,310 - INFO - joeynmt.helpers -   cfg.data.src.tokenizer_cfg.codes : data2/code2000.bpe
2024-05-19 11:30:06,311 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : it
2024-05-19 11:30:06,311 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe
2024-05-19 11:30:06,311 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False
2024-05-19 11:30:06,311 - INFO - joeynmt.helpers -       cfg.data.trg.max_sent_length : 100
2024-05-19 11:30:06,311 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : data2/bpe_vocab2000.it
2024-05-19 11:30:06,311 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : subword-nmt
2024-05-19 11:30:06,311 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.pretokenizer : moses
2024-05-19 11:30:06,311 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.num_merges : 2000
2024-05-19 11:30:06,312 - INFO - joeynmt.helpers -   cfg.data.trg.tokenizer_cfg.codes : data2/code2000.bpe
2024-05-19 11:30:06,312 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5
2024-05-19 11:30:06,312 - INFO - joeynmt.helpers -                  cfg.testing.alpha : 1.0
2024-05-19 11:30:06,312 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42
2024-05-19 11:30:06,312 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam
2024-05-19 11:30:06,312 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens
2024-05-19 11:30:06,312 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003
2024-05-19 11:30:06,312 - INFO - joeynmt.helpers -            cfg.training.batch_size : 2048
2024-05-19 11:30:06,312 - INFO - joeynmt.helpers -            cfg.training.batch_type : token
2024-05-19 11:30:06,312 - INFO - joeynmt.helpers -       cfg.training.eval_batch_size : 1024
2024-05-19 11:30:06,313 - INFO - joeynmt.helpers -       cfg.training.eval_batch_type : token
2024-05-19 11:30:06,313 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau
2024-05-19 11:30:06,313 - INFO - joeynmt.helpers -              cfg.training.patience : 8
2024-05-19 11:30:06,313 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0
2024-05-19 11:30:06,313 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7
2024-05-19 11:30:06,313 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2024-05-19 11:30:06,313 - INFO - joeynmt.helpers -                cfg.training.epochs : 10
2024-05-19 11:30:06,313 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 500
2024-05-19 11:30:06,313 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100
2024-05-19 11:30:06,314 - INFO - joeynmt.helpers -           cfg.training.eval_metric : bleu
2024-05-19 11:30:06,314 - INFO - joeynmt.helpers -             cfg.training.model_dir : word_level_model
2024-05-19 11:30:06,314 - INFO - joeynmt.helpers -             cfg.training.overwrite : True
2024-05-19 11:30:06,314 - INFO - joeynmt.helpers -               cfg.training.shuffle : True
2024-05-19 11:30:06,314 - INFO - joeynmt.helpers -              cfg.training.use_cuda : True
2024-05-19 11:30:06,314 - INFO - joeynmt.helpers -     cfg.training.max_output_length : 100
2024-05-19 11:30:06,314 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3, 4]
2024-05-19 11:30:06,314 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.3
2024-05-19 11:30:06,314 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier_uniform
2024-05-19 11:30:06,315 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros
2024-05-19 11:30:06,315 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0
2024-05-19 11:30:06,315 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier_uniform
2024-05-19 11:30:06,315 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0
2024-05-19 11:30:06,315 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : False
2024-05-19 11:30:06,315 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True
2024-05-19 11:30:06,316 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer
2024-05-19 11:30:06,316 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 4
2024-05-19 11:30:06,316 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 2
2024-05-19 11:30:06,316 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256
2024-05-19 11:30:06,316 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2024-05-19 11:30:06,316 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0
2024-05-19 11:30:06,316 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256
2024-05-19 11:30:06,316 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 512
2024-05-19 11:30:06,316 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0
2024-05-19 11:30:06,316 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer
2024-05-19 11:30:06,317 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 1
2024-05-19 11:30:06,317 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 2
2024-05-19 11:30:06,317 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256
2024-05-19 11:30:06,317 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2024-05-19 11:30:06,317 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0
2024-05-19 11:30:06,317 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256
2024-05-19 11:30:06,317 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 512
2024-05-19 11:30:06,317 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0
2024-05-19 11:30:06,368 - INFO - joeynmt.data - Building tokenizer...
2024-05-19 11:30:06,748 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 11:30:06,748 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 11:30:06,749 - INFO - joeynmt.data - Loading train set...
2024-05-19 11:30:39,716 - INFO - joeynmt.data - Building vocabulary...
2024-05-19 11:30:39,768 - INFO - joeynmt.data - Loading dev set...
2024-05-19 11:30:39,998 - INFO - joeynmt.data - Loading test set...
2024-05-19 11:30:40,392 - INFO - joeynmt.data - Data loaded.
2024-05-19 11:30:40,392 - INFO - joeynmt.data - Train dataset: PlaintextDataset(split=train, len=100000, src_lang=en, trg_lang=it, has_trg=True, random_subset=-1)
2024-05-19 11:30:40,393 - INFO - joeynmt.data - Valid dataset: PlaintextDataset(split=dev, len=929, src_lang=en, trg_lang=it, has_trg=True, random_subset=-1)
2024-05-19 11:30:40,393 - INFO - joeynmt.data -  Test dataset: PlaintextDataset(split=test, len=1566, src_lang=en, trg_lang=it, has_trg=True, random_subset=-1)
2024-05-19 11:30:40,394 - INFO - joeynmt.data - First training example:
	[SRC] A@@ l G@@ ore : A@@ ver@@ ting the c@@ lim@@ ate c@@ ri@@ si@@ s
	[TRG] A@@ l G@@ ore : ar@@ rest@@ iamo il ri@@ sc@@ al@@ d@@ amento glob@@ ale
2024-05-19 11:30:40,394 - INFO - joeynmt.data - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) to (6) of (7) a (8) and (9) in
2024-05-19 11:30:40,394 - INFO - joeynmt.data - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) a (5) di (6) e (7) che (8) i (9) o
2024-05-19 11:30:40,395 - INFO - joeynmt.data - Number of unique Src tokens (vocab_size): 1883
2024-05-19 11:30:40,395 - INFO - joeynmt.data - Number of unique Trg tokens (vocab_size): 1840
2024-05-19 11:30:40,405 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 11:30:40,488 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 11:30:41,696 - DEBUG - tensorflow - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2024-05-19 11:30:41,898 - DEBUG - h5py._conv - Creating converter from 7 to 5
2024-05-19 11:30:41,898 - DEBUG - h5py._conv - Creating converter from 5 to 7
2024-05-19 11:30:41,899 - DEBUG - h5py._conv - Creating converter from 7 to 5
2024-05-19 11:30:41,899 - DEBUG - h5py._conv - Creating converter from 5 to 7
2024-05-19 11:30:42,301 - DEBUG - jax._src.path - etils.epath found. Using etils.epath for file I/O.
2024-05-19 11:30:43,351 - INFO - joeynmt.model - Total params: 3852288
2024-05-19 11:30:43,352 - DEBUG - joeynmt.model - Trainable parameters: ['decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2024-05-19 11:30:43,353 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=1883),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=1840),
	loss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.3))
2024-05-19 11:30:44,453 - INFO - joeynmt.builders - Adam(lr=0.0003, weight_decay=0.0, betas=(0.9, 0.999))
2024-05-19 11:30:44,453 - INFO - joeynmt.builders - ReduceLROnPlateau(mode=min, verbose=False, threshold_mode=abs, eps=0.0, factor=0.7, patience=8)
2024-05-19 11:30:44,454 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 2048
	effective batch size (w. parallel & accumulation): 2048
2024-05-19 11:30:44,454 - INFO - joeynmt.training - EPOCH 1
2024-05-19 11:30:48,396 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     3.733884, Batch Acc: 0.055550, Tokens per Sec:    18890, Lr: 0.000300
2024-05-19 11:30:52,960 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     3.650264, Batch Acc: 0.114193, Tokens per Sec:    16422, Lr: 0.000300
2024-05-19 11:30:56,397 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     3.442580, Batch Acc: 0.134121, Tokens per Sec:    21776, Lr: 0.000300
2024-05-19 11:30:59,882 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     3.381505, Batch Acc: 0.148553, Tokens per Sec:    21759, Lr: 0.000300
2024-05-19 11:31:03,762 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     3.277943, Batch Acc: 0.161518, Tokens per Sec:    19413, Lr: 0.000300
2024-05-19 11:31:03,763 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:31:03,763 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:31:23,046 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.31, ppl:  27.43, acc:   0.16, generation: 19.1162[sec], evaluation: 0.0000[sec]
2024-05-19 11:31:23,047 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:31:23,268 - INFO - joeynmt.training - Example #0
2024-05-19 11:31:23,269 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:31:23,269 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:31:23,270 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['P@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@']
2024-05-19 11:31:23,270 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:31:23,271 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:31:23,271 - INFO - joeynmt.training - 	Hypothesis: Piiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii
2024-05-19 11:31:23,271 - INFO - joeynmt.training - Example #1
2024-05-19 11:31:23,271 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:31:23,272 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:31:23,272 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'un', 'mondo', 'che', 'un', 'mondo', 'di', 'un', 'un', 'mondo', 'di', 'un', 'mondo', 'di', 'un', 'mondo', 'di', 'un', 'mondo', 'di', 'un', 'mondo', '.', '</s>']
2024-05-19 11:31:23,272 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:31:23,273 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:31:23,273 - INFO - joeynmt.training - 	Hypothesis: Ma un mondo che un mondo di un un mondo di un mondo di un mondo di un mondo di un mondo.
2024-05-19 11:31:23,273 - INFO - joeynmt.training - Example #2
2024-05-19 11:31:23,273 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:31:23,274 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:31:23,274 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'mondo', ',', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', '.', '</s>']
2024-05-19 11:31:23,274 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:31:23,275 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:31:23,275 - INFO - joeynmt.training - 	Hypothesis: Il mondo, è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è.
2024-05-19 11:31:23,275 - INFO - joeynmt.training - Example #3
2024-05-19 11:31:23,275 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:31:23,275 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:31:23,276 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&@@', 'a@@', 'pos@@', ';', 'un', 'un', 'un', 'un', 'un', 'mondo', '.', '</s>']
2024-05-19 11:31:23,276 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:31:23,276 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:31:23,277 - INFO - joeynmt.training - 	Hypothesis: E 'un un un un un mondo.
2024-05-19 11:31:23,277 - INFO - joeynmt.training - Example #4
2024-05-19 11:31:23,277 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:31:23,277 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:31:23,277 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'mondo', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'un', 'mondo', '.', '</s>']
2024-05-19 11:31:23,278 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:31:23,278 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:31:23,279 - INFO - joeynmt.training - 	Hypothesis: Il mondo è è è è è è è è è è è è è è è è è è è è è è è è è è che che che che che che che che che che un mondo.
2024-05-19 11:31:26,657 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     3.323210, Batch Acc: 0.168185, Tokens per Sec:    21137, Lr: 0.000300
2024-05-19 11:31:29,985 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     3.216744, Batch Acc: 0.180515, Tokens per Sec:    22866, Lr: 0.000300
2024-05-19 11:31:34,179 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     3.111062, Batch Acc: 0.191729, Tokens per Sec:    17544, Lr: 0.000300
2024-05-19 11:31:37,888 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     3.058590, Batch Acc: 0.207403, Tokens per Sec:    20480, Lr: 0.000300
2024-05-19 11:31:41,247 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     2.726624, Batch Acc: 0.220873, Tokens per Sec:    22211, Lr: 0.000300
2024-05-19 11:31:41,248 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:31:41,248 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:32:00,558 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.92, ppl:  18.60, acc:   0.23, generation: 19.0379[sec], evaluation: 0.0000[sec]
2024-05-19 11:32:00,559 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:32:00,875 - INFO - joeynmt.training - Example #0
2024-05-19 11:32:00,876 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:32:00,877 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:32:00,877 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['P@@', 'oi', 'oi', 'sono', 'sono', 'sono', 'un', 'po', 'di', 'cui', 'sono', 'sono', 'sono', 'sono', 'sono', 'un', 'po', ',', 'e', 'sono', 'sono', 'un', 'po', ',', 'il', 'mondo', ',', 'il', 'mondo', ',', 'il', 'mondo', ',', 'il', 'mondo', ',', 'il', 'mondo', ',', 'il', 'mondo', ',', 'il', 'mondo', ',', 'il', 'mondo', ',', 'il', 'mondo', ',', 'il', 'mondo', ',', 'il', 'mondo', ',', 'il', 'mondo', '.', '</s>']
2024-05-19 11:32:00,878 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:32:00,880 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:32:00,880 - INFO - joeynmt.training - 	Hypothesis: Poi oi sono sono sono un po di cui sono sono sono sono sono un po, e sono sono un po, il mondo, il mondo, il mondo, il mondo, il mondo, il mondo, il mondo, il mondo, il mondo, il mondo, il mondo, il mondo.
2024-05-19 11:32:00,880 - INFO - joeynmt.training - Example #1
2024-05-19 11:32:00,883 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:32:00,883 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:32:00,883 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'la', 'la', 'la', 'la', 'la', 'la', 'la', 'la', 'la', 'la', 'la', 'la', 'la', 'la', 'la', 'la', 'la', 'la', 'la', 'la', 'la', 'la', 'la', 'la', 'la', 'la', 'la', 'la', '.', '</s>']
2024-05-19 11:32:00,884 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:32:00,886 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:32:00,886 - INFO - joeynmt.training - 	Hypothesis: Ma la la la la la la la la la la la la la la la la la la la la la la la la la la la la.
2024-05-19 11:32:00,886 - INFO - joeynmt.training - Example #2
2024-05-19 11:32:00,886 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:32:00,887 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:32:00,887 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'è', 'un', 'po', 'di', 'un', 'po', ',', 'un', 'po', ',', 'un', 'po', ',', 'un', 'po', ',', 'il', 'mondo', 'è', 'un', 'po', ',', 'il', 'nostro', 'nostro', 'nostro', 'nostro', 'nostro', 'nostro', 'nostro', 'nostro', 'nostro', 'nostro', 'nostro', '.', '</s>']
2024-05-19 11:32:00,888 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:32:00,888 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:32:00,888 - INFO - joeynmt.training - 	Hypothesis: La è un po di un po, un po, un po, un po, il mondo è un po, il nostro nostro nostro nostro nostro nostro nostro nostro nostro nostro nostro.
2024-05-19 11:32:00,889 - INFO - joeynmt.training - Example #3
2024-05-19 11:32:00,889 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:32:00,889 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:32:00,889 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&@@', 'a@@', 'pos@@', ';', 'è', 'il', 'nostro', 'nostro', 'nostro', 'nostro', 'nostro', 'nostro', 'nostro', 'nostro', 'nostro', 'nostro', 'nostro', 'nostro', 'nostro', '.', '</s>']
2024-05-19 11:32:00,890 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:32:00,892 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:32:00,892 - INFO - joeynmt.training - 	Hypothesis: E 'è il nostro nostro nostro nostro nostro nostro nostro nostro nostro nostro nostro nostro nostro.
2024-05-19 11:32:00,892 - INFO - joeynmt.training - Example #4
2024-05-19 11:32:00,893 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:32:00,893 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:32:00,893 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 's@@', 'so', 'che', 'che', 'che', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', '.', '</s>']
2024-05-19 11:32:00,894 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:32:00,894 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:32:00,894 - INFO - joeynmt.training - 	Hypothesis: La sso che che che si si si si si si si si si si si si si si si si si si si si si si si si si si si.
2024-05-19 11:32:05,002 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     2.959472, Batch Acc: 0.235960, Tokens per Sec:    17032, Lr: 0.000300
2024-05-19 11:32:08,341 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     2.750271, Batch Acc: 0.246950, Tokens per Sec:    21831, Lr: 0.000300
2024-05-19 11:32:11,694 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     2.656111, Batch Acc: 0.256388, Tokens per Sec:    21985, Lr: 0.000300
2024-05-19 11:32:15,460 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     2.603181, Batch Acc: 0.263423, Tokens per Sec:    19827, Lr: 0.000300
2024-05-19 11:32:19,408 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     2.638371, Batch Acc: 0.273030, Tokens per Sec:    18714, Lr: 0.000300
2024-05-19 11:32:19,409 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:32:19,409 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:32:38,242 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.65, ppl:  14.20, acc:   0.27, generation: 18.4407[sec], evaluation: 0.0000[sec]
2024-05-19 11:32:38,243 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:32:38,480 - INFO - joeynmt.training - Example #0
2024-05-19 11:32:38,481 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:32:38,481 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:32:38,481 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'ulti@@', 'mi', 'mi', 'mi', 'mi', 'mi', 'mi', 'sono', 'due', 'anni', 'che', 'sono', 'sono', 'un', 'po', 'di', 'un', 'po', 'di', 'anni', ',', 'che', 'ha', 'fatto', 'di', 'un', 'po', 'di', 'anni', ',', 'che', 'ha', 'fatto', 'di', 'anni', 'anni', ',', 'un', 'po', '&@@', 'a@@', 'pos@@', ';', 'è', 'di', '1@@', '5', 'anni', ',', 'in', 'cui', 'un', 'mili@@', 'ar@@', 'di', 'di', 'anni', '.', '</s>']
2024-05-19 11:32:38,482 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:32:38,483 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:32:38,483 - INFO - joeynmt.training - 	Hypothesis: L'ultimi mi mi mi mi mi sono due anni che sono sono un po di un po di anni, che ha fatto di un po di anni, che ha fatto di anni anni, un po 'è di 15 anni, in cui un miliardi di anni.
2024-05-19 11:32:38,483 - INFO - joeynmt.training - Example #1
2024-05-19 11:32:38,484 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:32:38,484 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:32:38,484 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'è', 'una', 'volta', 'che', 'non', 'è', 'il', 'problema', 'di', 'questo', 'che', 'non', 'è', 'che', 'non', 'è', 'che', 'non', 'è', 'che', 'non', 'è', 'il', 'modo', 'di', 'ri@@', 'sul@@', 't@@', 'ato', '.', '</s>']
2024-05-19 11:32:38,485 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:32:38,485 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:32:38,485 - INFO - joeynmt.training - 	Hypothesis: Ma questo è una volta che non è il problema di questo che non è che non è che non è che non è il modo di risultato.
2024-05-19 11:32:38,485 - INFO - joeynmt.training - Example #2
2024-05-19 11:32:38,486 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:32:38,486 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:32:38,486 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'c@@', '&@@', 'a@@', 'pos@@', ';', 'è', 'una', 'parte', 'di', 'un', 'po', '&@@', 'a@@', 'pos@@', ';', 'è', 'una', 'di', 'un', 'po', 'di', 'un', 'po', 'di', 'un', 'po', 'di', 'ri@@', 'ri@@', 'ri@@', 'ri@@', 'va', '.', '</s>']
2024-05-19 11:32:38,487 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:32:38,487 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:32:38,487 - INFO - joeynmt.training - 	Hypothesis: Il c'è una parte di un po 'è una di un po di un po di un po di ririririva.
2024-05-19 11:32:38,487 - INFO - joeynmt.training - Example #3
2024-05-19 11:32:38,488 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:32:38,488 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:32:38,488 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'forma@@', 'zioni', 'e', 'un', 'po', '&@@', 'a@@', 'pos@@', ';', 'di', 's@@', 's@@', 's@@', 's@@', 's@@', 'ano', '.', '</s>']
2024-05-19 11:32:38,488 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:32:38,489 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:32:38,489 - INFO - joeynmt.training - 	Hypothesis: E 'informazioni e un po' di sssssano.
2024-05-19 11:32:38,489 - INFO - joeynmt.training - Example #4
2024-05-19 11:32:38,489 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:32:38,489 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:32:38,490 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'punto', 'che', 'ho', 'fatto', 'un', 'po', '&@@', 'a@@', 'pos@@', ';', 'è', 'un', 'po', '&@@', 'a@@', 'pos@@', ';', 'è', 'che', 'è', 'che', 'è', 'stato', 'anni', 'anni', 'anni', 'anni', 'anni', 'anni', 'anni', 'anni', 'anni', 'anni', 'anni', 'anni', 'anni', 'anni', '.', '</s>']
2024-05-19 11:32:38,490 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:32:38,490 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:32:38,491 - INFO - joeynmt.training - 	Hypothesis: Il punto che ho fatto un po 'è un po' è che è che è stato anni anni anni anni anni anni anni anni anni anni anni anni anni anni.
2024-05-19 11:32:41,832 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     2.595552, Batch Acc: 0.285667, Tokens per Sec:    20994, Lr: 0.000300
2024-05-19 11:32:46,300 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     2.634763, Batch Acc: 0.288211, Tokens per Sec:    17356, Lr: 0.000300
2024-05-19 11:32:49,681 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     2.658032, Batch Acc: 0.292802, Tokens per Sec:    21766, Lr: 0.000300
2024-05-19 11:32:53,055 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     2.556941, Batch Acc: 0.305412, Tokens per Sec:    21701, Lr: 0.000300
2024-05-19 11:32:56,526 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     2.408104, Batch Acc: 0.311962, Tokens per Sec:    21307, Lr: 0.000300
2024-05-19 11:32:56,527 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:32:56,527 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:33:14,760 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.50, ppl:  12.18, acc:   0.30, generation: 18.0797[sec], evaluation: 0.0000[sec]
2024-05-19 11:33:14,761 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:33:14,990 - INFO - joeynmt.training - Example #0
2024-05-19 11:33:14,991 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:33:14,991 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:33:14,991 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', 'he', 'anno', 'ho', 'parl@@', 'ato', 'di', 'questi', 'due', 'anni', 'fa', 'fa', 'fa', 'che', 'il', '1@@', '0', 'anni', 'che', 'la', 'sua', 'sua', 'sua', 'sua', 'sua', 'sua', 'sua', 'sua', 'sua', 'sua', 'sua', 'sua', 'sua', 'sua', 'sua', 'sua', 'sua', 'sua', 'sua', 'sua', 'sua', 'sua', 'sua', 'vita', ',', 'in', '1@@', '1@@', '5', 'anni', 'fa', ',', 'il', '1@@', '5', 'anni', '.', '</s>']
2024-05-19 11:33:14,992 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:33:14,992 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:33:14,993 - INFO - joeynmt.training - 	Hypothesis: Che anno ho parlato di questi due anni fa fa fa che il 10 anni che la sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua vita, in 115 anni fa, il 15 anni.
2024-05-19 11:33:14,993 - INFO - joeynmt.training - Example #1
2024-05-19 11:33:14,993 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:33:14,993 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:33:14,994 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'modo', 'che', 'si', 'si', 'si', 'si', 'si', 'può', 'essere', 'la', 'della', 'vita', 'che', 'è', 'che', 'non', 'è', 'che', 'è', 'che', 'la', 'st@@', 'essa', 'cosa', 'che', 'la', 'sua', 'sua', 'sua', 'sua', 'sua', 's@@', 'ità', '.', '</s>']
2024-05-19 11:33:14,994 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:33:14,994 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:33:14,995 - INFO - joeynmt.training - 	Hypothesis: Ma questo modo che si si si si si può essere la della vita che è che non è che è che la stessa cosa che la sua sua sua sua sua sità.
2024-05-19 11:33:14,995 - INFO - joeynmt.training - Example #2
2024-05-19 11:33:14,995 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:33:14,995 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:33:14,996 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'c@@', '&@@', 'a@@', 'pos@@', ';', 'è', 'un', 'po', '&@@', 'a@@', 'pos@@', ';', 'è', 'un', 'po', '&@@', 'a@@', 'pos@@', ';', 'è', 'la', 'della', 'vita', 'della', 'con@@', 'c@@', 'ità', '.', '</s>']
2024-05-19 11:33:14,996 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:33:14,996 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:33:14,997 - INFO - joeynmt.training - 	Hypothesis: La c'è un po 'è un po' è la della vita della concità.
2024-05-19 11:33:14,997 - INFO - joeynmt.training - Example #3
2024-05-19 11:33:14,997 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:33:14,997 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:33:14,997 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'forma@@', 'zione', 'e', 'in', 'gra@@', 'do', 'e', 'in', 'gra@@', 'do', 'e', '.', '</s>']
2024-05-19 11:33:14,998 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:33:14,998 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:33:14,998 - INFO - joeynmt.training - 	Hypothesis: E 'informazione e in grado e in grado e.
2024-05-19 11:33:14,998 - INFO - joeynmt.training - Example #4
2024-05-19 11:33:14,999 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:33:14,999 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:33:14,999 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'primo', 'primo', 'primo', 'che', 'ho', 'parl@@', 'ato', 'di', 'un', 'po', '&@@', 'a@@', 'pos@@', ';', 'è', 'che', 'si', 'può', 'essere', 'un', 'po', '&@@', 'a@@', 'pos@@', ';', 'è', 'che', 'ha', 'fatto', 'che', 'ha', 'fatto', 'che', 'mi', 'anni', '.', '</s>']
2024-05-19 11:33:15,000 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:33:15,000 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:33:15,000 - INFO - joeynmt.training - 	Hypothesis: Il primo primo primo che ho parlato di un po 'è che si può essere un po' è che ha fatto che ha fatto che mi anni.
2024-05-19 11:33:18,370 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     2.470908, Batch Acc: 0.321907, Tokens per Sec:    20738, Lr: 0.000300
2024-05-19 11:33:21,714 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     2.487994, Batch Acc: 0.324607, Tokens per Sec:    22149, Lr: 0.000300
2024-05-19 11:33:25,201 - INFO - joeynmt.training - Epoch   1, Step:     2300, Batch Loss:     2.357760, Batch Acc: 0.329982, Tokens per Sec:    21565, Lr: 0.000300
2024-05-19 11:33:29,503 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     2.430049, Batch Acc: 0.339638, Tokens per Sec:    17158, Lr: 0.000300
2024-05-19 11:33:32,918 - INFO - joeynmt.training - Epoch   1, Step:     2500, Batch Loss:     2.271058, Batch Acc: 0.342937, Tokens per Sec:    21736, Lr: 0.000300
2024-05-19 11:33:32,919 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:33:32,919 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:33:48,951 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.36, ppl:  10.58, acc:   0.34, generation: 15.9144[sec], evaluation: 0.0000[sec]
2024-05-19 11:33:48,952 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:33:49,197 - INFO - joeynmt.training - Example #0
2024-05-19 11:33:49,198 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:33:49,198 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:33:49,199 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'queste', 'due', 'due', 'che', 'ho', 'parl@@', 'ato', 'che', 'la', 's@@', 'itu@@', 'azione', 'che', 'la', 'c@@', '&@@', 'a@@', 'pos@@', ';', 'è', 'il', 'c@@', 'ento', 'di', 'tre', 'anni', ',', 'che', 'sono', 'tre', 'anni', ',', 'che', 'sono', 'sono', 'i', 'tre', 'anni', ',', 'il', '1@@', '1@@', '2', '%', 'di', '4@@', '0', '%', 'di', '4@@', '0', '%', 'di', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:33:49,200 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:33:49,200 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:33:49,200 - INFO - joeynmt.training - 	Hypothesis: L'anno queste due due che ho parlato che la situazione che la c'è il cento di tre anni, che sono tre anni, che sono sono i tre anni, il 112% di 40% di 40% di 40%.
2024-05-19 11:33:49,200 - INFO - joeynmt.training - Example #1
2024-05-19 11:33:49,201 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:33:49,201 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:33:49,201 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'pot@@', 'rebbe', 'essere', 'la', 'parte', 'di', 'questa', 'è', 'la', 'parte', 'perché', 'non', 'è', 'la', 'parte', 'perché', 'non', 'è', 'il', 'problema', 'che', 'non', 'è', 'il', 'problema', 'che', 'la', 'gu@@', 'er@@', 'ra', '.', '</s>']
2024-05-19 11:33:49,202 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:33:49,202 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:33:49,202 - INFO - joeynmt.training - 	Hypothesis: Ma questo potrebbe essere la parte di questa è la parte perché non è la parte perché non è il problema che non è il problema che la guerra.
2024-05-19 11:33:49,202 - INFO - joeynmt.training - Example #2
2024-05-19 11:33:49,203 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:33:49,203 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:33:49,203 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'c@@', '&@@', 'a@@', 'pos@@', ';', 'è', 'un', 'c@@', 'lu@@', 'og@@', 'o', ',', 'la', 's@@', 'itu@@', 'azione', ',', 'la', 'l@@', '&@@', 'a@@', 'pos@@', ';', 'acqu@@', 'a', '.', '</s>']
2024-05-19 11:33:49,204 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:33:49,204 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:33:49,204 - INFO - joeynmt.training - 	Hypothesis: Il c'è un cluogo, la situazione, la l'acqua.
2024-05-19 11:33:49,204 - INFO - joeynmt.training - Example #3
2024-05-19 11:33:49,205 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:33:49,205 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:33:49,205 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&@@', 'a@@', 'pos@@', ';', 'un', 'm@@', 'esso', 'e', 'si', 'si', 's@@', 'b@@', 'agli@@', 'o', 'e', 'la', 's@@', 'etti@@', 'man@@', 'a', '.', '</s>']
2024-05-19 11:33:49,206 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:33:49,206 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:33:49,206 - INFO - joeynmt.training - 	Hypothesis: E 'un messo e si si sbaglio e la settimana.
2024-05-19 11:33:49,206 - INFO - joeynmt.training - Example #4
2024-05-19 11:33:49,207 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:33:49,207 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:33:49,207 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'primo', 'primo', 'che', 'ho', 'mostr@@', 'ato', 'un', 'po', '&@@', 'a@@', 'pos@@', ';', 'è', 'che', 'la', 'for@@', 'z@@', 'a', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'che', 'ha', 'fatto', 'che', 'ha', 'fatto', 'che', 'ha', 'fatto', 'che', 'anni', '.', '</s>']
2024-05-19 11:33:49,208 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:33:49,208 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:33:49,208 - INFO - joeynmt.training - 	Hypothesis: Il primo primo che ho mostrato un po 'è che la forza di quello che è successo che ha fatto che ha fatto che ha fatto che anni.
2024-05-19 11:33:52,577 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     2.161379, Batch Acc: 0.349356, Tokens per Sec:    21114, Lr: 0.000300
2024-05-19 11:33:56,832 - INFO - joeynmt.training - Epoch   1, Step:     2700, Batch Loss:     2.235556, Batch Acc: 0.359512, Tokens per Sec:    17464, Lr: 0.000300
2024-05-19 11:34:00,309 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     2.231016, Batch Acc: 0.366728, Tokens per Sec:    21360, Lr: 0.000300
2024-05-19 11:34:03,717 - INFO - joeynmt.training - Epoch   1, Step:     2900, Batch Loss:     2.222700, Batch Acc: 0.372267, Tokens per Sec:    22708, Lr: 0.000300
2024-05-19 11:34:07,019 - INFO - joeynmt.training - Epoch   1, Step:     3000, Batch Loss:     2.348516, Batch Acc: 0.375758, Tokens per Sec:    21824, Lr: 0.000300
2024-05-19 11:34:07,020 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:34:07,020 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:34:21,534 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.25, ppl:   9.46, acc:   0.37, generation: 14.3969[sec], evaluation: 0.0000[sec]
2024-05-19 11:34:21,535 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:34:21,806 - INFO - joeynmt.helpers - delete word_level_model/500.ckpt
2024-05-19 11:34:21,819 - INFO - joeynmt.training - Example #0
2024-05-19 11:34:21,820 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:34:21,820 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:34:21,820 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'anno', 'due', 'm@@', 'b@@', 'atter@@', 'i', 'che', 'ha', 'fatto', 'che', 'la', 'm@@', 'p@@', 'a', 'di', 'cui', 'la', 'c@@', 'en@@', 'zione', ',', 'che', 'la', 'maggi@@', 'or', 'parte', 'di', 'tre', 'anni', ',', 'che', 'la', 'maggi@@', 'or', 'parte', 'di', '4@@', '0', 'anni', ',', 'che', 'la', 'maggi@@', 'or', 'parte', 'della', 'c@@', 'ina', ',', 'è', '4@@', '0', '%', 'della', 'c@@', 'uc@@', 'ina', '.', '</s>']
2024-05-19 11:34:21,821 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:34:21,822 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:34:21,822 - INFO - joeynmt.training - 	Hypothesis: L'anno anno due mbatteri che ha fatto che la mpa di cui la cenzione, che la maggior parte di tre anni, che la maggior parte di 40 anni, che la maggior parte della cina, è 40% della cucina.
2024-05-19 11:34:21,822 - INFO - joeynmt.training - Example #1
2024-05-19 11:34:21,822 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:34:21,822 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:34:21,823 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'è', 'la', 'capi@@', 're', 'la', 'cosa', 'di', 'questa', 'idea', 'di', 'questa', 'idea', 'di', 'questa', 'non', 'è', 'la', 'pro@@', 'du@@', 'zione', '.', '</s>']
2024-05-19 11:34:21,823 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:34:21,824 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:34:21,824 - INFO - joeynmt.training - 	Hypothesis: Ma questo è la capire la cosa di questa idea di questa idea di questa non è la produzione.
2024-05-19 11:34:21,824 - INFO - joeynmt.training - Example #2
2024-05-19 11:34:21,824 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:34:21,824 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:34:21,825 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'c@@', 'uc@@', 'ci@@', 'o', 'è', 'in', 'un', 'sen@@', 'so', ',', 'in', 'un', 'sen@@', 'so', ',', 'la', 'pro@@', 'gett@@', 'azione', 'della', 'po@@', 'sta', 'del', 'sistema', '.', '</s>']
2024-05-19 11:34:21,825 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:34:21,826 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:34:21,826 - INFO - joeynmt.training - 	Hypothesis: Il cuccio è in un senso, in un senso, la progettazione della posta del sistema.
2024-05-19 11:34:21,826 - INFO - joeynmt.training - Example #3
2024-05-19 11:34:21,826 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:34:21,826 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:34:21,826 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&@@', 'a@@', 'pos@@', ';', 'pi@@', 'ut@@', 'to@@', 'sto', 'in', 's@@', 'es@@', 'att@@', 'amente', 'in', 'm@@', 'ano', '.', '</s>']
2024-05-19 11:34:21,827 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:34:21,827 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:34:21,827 - INFO - joeynmt.training - 	Hypothesis: E 'piuttosto in sesattamente in mano.
2024-05-19 11:34:21,828 - INFO - joeynmt.training - Example #4
2024-05-19 11:34:21,828 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:34:21,828 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:34:21,828 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'pro@@', 'ssi@@', 'mo', 'mo', 'mostr@@', 'o', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'pi@@', 'ac@@', 'o', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'in', 'cui', 'è', 'succ@@', 'esso', 'succ@@', 'esso', ',', 'il', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:34:21,829 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:34:21,829 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:34:21,829 - INFO - joeynmt.training - 	Hypothesis: Il prossimo mo mostro che vi mostrerà un piaco di ciò che è successo in cui è successo successo, il 25 anni.
2024-05-19 11:34:26,157 - INFO - joeynmt.training - Epoch   1, Step:     3100, Batch Loss:     2.254803, Batch Acc: 0.376103, Tokens per Sec:    15865, Lr: 0.000300
2024-05-19 11:34:29,459 - INFO - joeynmt.training - Epoch   1, Step:     3200, Batch Loss:     2.184362, Batch Acc: 0.390805, Tokens per Sec:    22038, Lr: 0.000300
2024-05-19 11:34:32,752 - INFO - joeynmt.training - Epoch   1, Step:     3300, Batch Loss:     2.050991, Batch Acc: 0.393570, Tokens per Sec:    22332, Lr: 0.000300
2024-05-19 11:34:36,163 - INFO - joeynmt.training - Epoch   1, Step:     3400, Batch Loss:     2.329805, Batch Acc: 0.397586, Tokens per Sec:    21715, Lr: 0.000300
2024-05-19 11:34:40,471 - INFO - joeynmt.training - Epoch   1, Step:     3500, Batch Loss:     2.133571, Batch Acc: 0.400011, Tokens per Sec:    16824, Lr: 0.000300
2024-05-19 11:34:40,472 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:34:40,472 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:34:51,137 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.15, ppl:   8.57, acc:   0.39, generation: 10.4762[sec], evaluation: 0.0000[sec]
2024-05-19 11:34:51,138 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:34:51,411 - INFO - joeynmt.helpers - delete word_level_model/1000.ckpt
2024-05-19 11:34:51,422 - INFO - joeynmt.training - Example #0
2024-05-19 11:34:51,423 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:34:51,423 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:34:51,423 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'mostr@@', 'ato', 'questi', 'due', 'anni', ',', 'che', 'ha', 'mostr@@', 'ato', 'che', 'il', 'c@@', '&@@', 'a@@', 'pos@@', ';', 'è', 'stato', 'il', 'c@@', 'en@@', 'o', 'di', 'anni', ',', 'che', 'per', 'la', 'maggi@@', 'ore', 'di', 'anni', ',', 'per', 'la', 'maggi@@', 'ore', 'di', 'anni', ',', 'ha', 'l@@', '&@@', 'a@@', 'pos@@', ';', 'anno', ',', 'ha', 'ha', 'det@@', 'to', 'che', 'la', 't@@', 'est', 'di', '4@@', '8', '%', '.', '</s>']
2024-05-19 11:34:51,424 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:34:51,425 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:34:51,425 - INFO - joeynmt.training - 	Hypothesis: L'anno mostrato questi due anni, che ha mostrato che il c'è stato il ceno di anni, che per la maggiore di anni, per la maggiore di anni, ha l'anno, ha ha detto che la test di 48%.
2024-05-19 11:34:51,425 - INFO - joeynmt.training - Example #1
2024-05-19 11:34:51,425 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:34:51,426 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:34:51,426 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'è', 'la', 'capi@@', 're', 'le', 'persone', 'che', 'si', 'mostr@@', 'a', 'perché', 'non', 'è', 'la', 'pro@@', 'ssi@@', 'ma', 'non', 'è', 'la', 'po@@', 'ver@@', 'ità', '.', '</s>']
2024-05-19 11:34:51,426 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:34:51,427 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:34:51,427 - INFO - joeynmt.training - 	Hypothesis: Ma questo è la capire le persone che si mostra perché non è la prossima non è la poverità.
2024-05-19 11:34:51,427 - INFO - joeynmt.training - Example #2
2024-05-19 11:34:51,427 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:34:51,428 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:34:51,428 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'c@@', 'li@@', 'mat@@', 'ore', 'è', ',', 'in', 'un', 'altro', 'altro', 'altro', ',', 'il', 'com@@', 'pi@@', 'to', 'di', 'vi@@', 'sta', 'del', 'sistema', 'di', 'cambi@@', 'amento', 'c@@', 'li@@', 'mat@@', 'ico', '.', '</s>']
2024-05-19 11:34:51,428 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:34:51,429 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:34:51,429 - INFO - joeynmt.training - 	Hypothesis: Il climatore è, in un altro altro altro, il compito di vista del sistema di cambiamento climatico.
2024-05-19 11:34:51,429 - INFO - joeynmt.training - Example #3
2024-05-19 11:34:51,429 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:34:51,430 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:34:51,430 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'contr@@', 'o', 'e', 'in', 'gra@@', 'do', 'di', 's@@', 'es@@', 'p@@', 'are', 'in', 'm@@', 'essi', '.', '</s>']
2024-05-19 11:34:51,430 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:34:51,431 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:34:51,431 - INFO - joeynmt.training - 	Hypothesis: E 'incontro e in grado di sespare in messi.
2024-05-19 11:34:51,431 - INFO - joeynmt.training - Example #4
2024-05-19 11:34:51,431 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:34:51,431 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:34:51,431 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'pro@@', 'ssi@@', 'mo', 'li@@', 'li@@', 'ber@@', 'i', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'pi@@', 'ac@@', 'ere@@', 'o', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'in', 'cui', 'è', 'succ@@', 'esso', 'il', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:34:51,432 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:34:51,433 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:34:51,433 - INFO - joeynmt.training - 	Hypothesis: Il prossimo liliberi che vi mostrerà un piacereo di ciò che è successo in cui è successo il 25 anni.
2024-05-19 11:34:55,420 - INFO - joeynmt.training - Epoch   1, Step:     3600, Batch Loss:     2.070566, Batch Acc: 0.405752, Tokens per Sec:    17440, Lr: 0.000300
2024-05-19 11:34:58,780 - INFO - joeynmt.training - Epoch   1, Step:     3700, Batch Loss:     2.104021, Batch Acc: 0.407237, Tokens per Sec:    21876, Lr: 0.000300
2024-05-19 11:35:02,216 - INFO - joeynmt.training - Epoch   1, Step:     3800, Batch Loss:     1.988688, Batch Acc: 0.417070, Tokens per Sec:    22096, Lr: 0.000300
2024-05-19 11:35:06,032 - INFO - joeynmt.training - Epoch   1, Step:     3900, Batch Loss:     2.062535, Batch Acc: 0.415783, Tokens per Sec:    19446, Lr: 0.000300
2024-05-19 11:35:09,918 - INFO - joeynmt.training - Epoch   1, Step:     4000, Batch Loss:     1.997262, Batch Acc: 0.424916, Tokens per Sec:    18993, Lr: 0.000300
2024-05-19 11:35:09,918 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:35:09,919 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:35:23,481 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.08, ppl:   8.00, acc:   0.41, generation: 13.4497[sec], evaluation: 0.0000[sec]
2024-05-19 11:35:23,482 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:35:23,720 - INFO - joeynmt.helpers - delete word_level_model/1500.ckpt
2024-05-19 11:35:23,729 - INFO - joeynmt.training - Example #0
2024-05-19 11:35:23,730 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:35:23,730 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:35:23,730 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'mostr@@', 'ato', 'questi', 'due', 's@@', 'b@@', 'li@@', 'zz@@', 'i', 'che', 'il', 'sol@@', 'uto', 'di', 'che', 'il', 'c@@', 'ro@@', 'b@@', 'a', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'del', 'm@@', 'esso', ',', 'che', 'è', 'stato', 'il', 'm@@', 'esso', 'di', '4@@', '8', 'milioni', 'di', 'anni', ',', 'è', 'stato', 'il', '4@@', '8', 'milioni', 'di', 'anni', ',', 'è', 'che', 'ha', 'ha', 'det@@', 'to', 'che', 'il', '9@@', '0', '%', ',', 'è', 'stato', 'il', '4@@', '8', 'milioni', 'di', 'anni', '.', '</s>']
2024-05-19 11:35:23,731 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:35:23,731 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:35:23,732 - INFO - joeynmt.training - 	Hypothesis: L'anno mostrato questi due sblizzi che il soluto di che il croba, che per la maggior parte del messo, che è stato il messo di 48 milioni di anni, è stato il 48 milioni di anni, è che ha ha detto che il 90%, è stato il 48 milioni di anni.
2024-05-19 11:35:23,732 - INFO - joeynmt.training - Example #1
2024-05-19 11:35:23,732 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:35:23,732 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:35:23,733 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'tipo', 'di', 'capi@@', 're', 'che', 'il', 'problema', 'di', 'questo', 'problema', 'perché', 'non', 'ci', 'sar@@', 'à', 'il', 'problema', 'perché', 'non', 'è', 'che', 'mostr@@', 'a', 'il', 'b@@', 'ello', '.', '</s>']
2024-05-19 11:35:23,733 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:35:23,733 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:35:23,734 - INFO - joeynmt.training - 	Hypothesis: Ma questo tipo di capire che il problema di questo problema perché non ci sarà il problema perché non è che mostra il bello.
2024-05-19 11:35:23,734 - INFO - joeynmt.training - Example #2
2024-05-19 11:35:23,734 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:35:23,734 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:35:23,734 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'ar@@', 'te', 'c@@', '&@@', 'a@@', 'pos@@', ';', 'è', ',', 'in', 'un', 'sen@@', 'so', 'del', 'sistema', 'di', 'cambi@@', 'amento', 'c@@', 'li@@', 'mat@@', 'ico', 'del', 'sistema', 'di', 'cambi@@', 'are', 'il', 'sistema', '.', '</s>']
2024-05-19 11:35:23,735 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:35:23,735 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:35:23,735 - INFO - joeynmt.training - 	Hypothesis: L'arte c'è, in un senso del sistema di cambiamento climatico del sistema di cambiare il sistema.
2024-05-19 11:35:23,736 - INFO - joeynmt.training - Example #3
2024-05-19 11:35:23,736 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:35:23,736 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:35:23,736 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'vol@@', 'te', 'e', 'si', 'in@@', 'vol@@', 'te', 'e', 'si', 'in@@', 'vol@@', 'te', '.', '</s>']
2024-05-19 11:35:23,737 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:35:23,737 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:35:23,737 - INFO - joeynmt.training - 	Hypothesis: E 'involte e si involte e si involte.
2024-05-19 11:35:23,737 - INFO - joeynmt.training - Example #4
2024-05-19 11:35:23,738 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:35:23,738 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:35:23,738 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'ho', 'mostr@@', 'ato', 'un', 'pi@@', 'ac@@', 'ac@@', 'ere@@', 'o', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', ',', 'che', 'è', 'succ@@', 'esso', 'in', 'cui', 'è', 'succ@@', 'esso', 'in', 'cui', 'sono', 'succ@@', 'esso', 'in', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:35:23,739 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:35:23,739 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:35:23,739 - INFO - joeynmt.training - 	Hypothesis: L'ho mostrato un piacacereo di ciò che è successo, che è successo in cui è successo in cui sono successo in 25 anni.
2024-05-19 11:35:27,109 - INFO - joeynmt.training - Epoch   1, Step:     4100, Batch Loss:     1.912834, Batch Acc: 0.430200, Tokens per Sec:    20873, Lr: 0.000300
2024-05-19 11:35:30,446 - INFO - joeynmt.training - Epoch   1, Step:     4200, Batch Loss:     2.357344, Batch Acc: 0.427521, Tokens per Sec:    22737, Lr: 0.000300
2024-05-19 11:35:34,257 - INFO - joeynmt.training - Epoch   1, Step:     4300, Batch Loss:     1.921049, Batch Acc: 0.435838, Tokens per Sec:    19700, Lr: 0.000300
2024-05-19 11:35:38,183 - INFO - joeynmt.training - Epoch   1, Step:     4400, Batch Loss:     1.922725, Batch Acc: 0.440226, Tokens per Sec:    18916, Lr: 0.000300
2024-05-19 11:35:41,453 - INFO - joeynmt.training - Epoch   1, Step:     4500, Batch Loss:     2.134449, Batch Acc: 0.440947, Tokens per Sec:    22080, Lr: 0.000300
2024-05-19 11:35:41,454 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:35:41,454 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:35:54,027 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.00, ppl:   7.39, acc:   0.43, generation: 12.4676[sec], evaluation: 0.0000[sec]
2024-05-19 11:35:54,028 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:35:54,245 - INFO - joeynmt.helpers - delete word_level_model/2000.ckpt
2024-05-19 11:35:54,255 - INFO - joeynmt.training - Example #0
2024-05-19 11:35:54,255 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:35:54,256 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:35:54,256 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'mostr@@', 'ato', 'questi', 'due', 's@@', 'etti@@', 'vi', 'che', 'di@@', 'spon@@', 'i@@', 'bi@@', 'li', 'che', 'il', 'sol@@', 'ito', ',', 'che', 'il', 'quale', 'per', 'cui', 'il', 'più', 'grande', ',', 'per', 'il', 'più', 'di', 'tre', 'milioni', 'di', 'anni', 'ha', 'la', 'maggi@@', 'or', 'parte', 'del', '4@@', '8', '%', ',', 'ha', 'il', '4@@', '4@@', '4@@', '4@@', '4@@', '4@@', '4@@', '4@@', '4@@', '4@@', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:35:54,257 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:35:54,257 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:35:54,257 - INFO - joeynmt.training - 	Hypothesis: L'anno mostrato questi due settivi che disponibili che il solito, che il quale per cui il più grande, per il più di tre milioni di anni ha la maggior parte del 48%, ha il 444444444440%.
2024-05-19 11:35:54,257 - INFO - joeynmt.training - Example #1
2024-05-19 11:35:54,258 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:35:54,258 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:35:54,258 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', 're', 'le', 'seri@@', 'e', 'di', 'questa', 'con@@', 'c@@', 'lu@@', 'og@@', 'o', 'perché', 'non', 'si', 'mostr@@', 'a', 'perché', 'non', 'lo', 'mostr@@', 'a', 'il', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:35:54,259 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:35:54,259 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:35:54,259 - INFO - joeynmt.training - 	Hypothesis: Ma questo capire le serie di questa concluogo perché non si mostra perché non lo mostra il ghiaccio.
2024-05-19 11:35:54,259 - INFO - joeynmt.training - Example #2
2024-05-19 11:35:54,260 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:35:54,260 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:35:54,260 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'punto', 'di', 'c@@', 'ro@@', 'c@@', 'ico', 'è', ',', 'in', 'un', 'sen@@', 'so', ',', 'in', 'un', 'sen@@', 'so', 'di', 'cui', 'il', 'suo', 'sistema', '.', '</s>']
2024-05-19 11:35:54,261 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:35:54,261 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:35:54,261 - INFO - joeynmt.training - 	Hypothesis: Il punto di crocico è, in un senso, in un senso di cui il suo sistema.
2024-05-19 11:35:54,261 - INFO - joeynmt.training - Example #3
2024-05-19 11:35:54,262 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:35:54,262 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:35:54,262 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'est@@', 'e', 'e', 'si', 'si', 's@@', 'ent@@', 'ano', 'e', 'si', 'si', 's@@', 'ent@@', 'r@@', 'ano', '.', '</s>']
2024-05-19 11:35:54,262 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:35:54,263 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:35:54,263 - INFO - joeynmt.training - 	Hypothesis: E 'esteste e si si sentano e si si sentrano.
2024-05-19 11:35:54,263 - INFO - joeynmt.training - Example #4
2024-05-19 11:35:54,263 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:35:54,264 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:35:54,264 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'pro@@', 'ssi@@', 'mo', 's@@', 'an@@', 'da', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'f@@', 'ast@@', 'o', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'in', 'cui', 'si', 'è', 'succ@@', 'esso', 'il', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:35:54,264 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:35:54,264 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:35:54,265 - INFO - joeynmt.training - 	Hypothesis: Il prossimo sanda che vi mostrerà un fasto di quello che è successo in cui si è successo il 25 anni.
2024-05-19 11:35:57,638 - INFO - joeynmt.training - Epoch   1, Step:     4600, Batch Loss:     1.803668, Batch Acc: 0.450112, Tokens per Sec:    20909, Lr: 0.000300
2024-05-19 11:36:01,193 - INFO - joeynmt.training - Epoch   1, Step:     4700, Batch Loss:     1.884705, Batch Acc: 0.448137, Tokens per Sec:    21171, Lr: 0.000300
2024-05-19 11:36:05,181 - INFO - joeynmt.training - Epoch   1: total training loss 12014.34
2024-05-19 11:36:05,181 - INFO - joeynmt.training - EPOCH 2
2024-05-19 11:36:05,455 - INFO - joeynmt.training - Epoch   2, Step:     4800, Batch Loss:     1.905582, Batch Acc: 0.471562, Tokens per Sec:    17702, Lr: 0.000300
2024-05-19 11:36:08,771 - INFO - joeynmt.training - Epoch   2, Step:     4900, Batch Loss:     2.028781, Batch Acc: 0.463303, Tokens per Sec:    22524, Lr: 0.000300
2024-05-19 11:36:12,037 - INFO - joeynmt.training - Epoch   2, Step:     5000, Batch Loss:     1.937342, Batch Acc: 0.469117, Tokens per Sec:    22876, Lr: 0.000300
2024-05-19 11:36:12,038 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:36:12,038 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:36:24,736 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.95, ppl:   6.99, acc:   0.45, generation: 12.5861[sec], evaluation: 0.0000[sec]
2024-05-19 11:36:24,737 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:36:24,963 - INFO - joeynmt.helpers - delete word_level_model/2500.ckpt
2024-05-19 11:36:24,968 - INFO - joeynmt.training - Example #0
2024-05-19 11:36:24,968 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:36:24,969 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:36:24,969 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'anno', 'anno', 'mostr@@', 'ato', 'questi', 'due', 's@@', 'li@@', 'ber@@', 'i', 'che', 'di@@', 'mostr@@', 'a', 'che', 'il', 'sol@@', 'ito', ',', 'che', 'per', 'la', 'gi@@', 'u@@', 'sta', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'del', 'gen@@', 'ere', ',', 'che', 'ha', 'ri@@', 'guar@@', 'da', 'il', '4@@', '8', '%', 'di', '4@@', '8', '%', 'di', '4@@', '8', '%', '.', '</s>']
2024-05-19 11:36:24,970 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:36:24,970 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:36:24,970 - INFO - joeynmt.training - 	Hypothesis: L'anno anno anno mostrato questi due sliberi che dimostra che il solito, che per la giusta, che per la maggior parte del genere, che ha riguarda il 48% di 48% di 48%.
2024-05-19 11:36:24,970 - INFO - joeynmt.training - Example #1
2024-05-19 11:36:24,971 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:36:24,971 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:36:24,971 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'de', 'la', 'seri@@', 'amente', 'di', 'questa', 'seri@@', 'amente', 'di', 'questa', 'seri@@', 'amente', ',', 'perché', 'non', 'è', 'mostr@@', 'a', 'il', 'gh@@', 'i@@', 'acci@@', 'o', 'non', 'ri@@', 'mostr@@', 'a', 'il', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:36:24,972 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:36:24,972 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:36:24,972 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprende la seriamente di questa seriamente di questa seriamente, perché non è mostra il ghiaccio non rimostra il ghiaccio.
2024-05-19 11:36:24,972 - INFO - joeynmt.training - Example #2
2024-05-19 11:36:24,973 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:36:24,973 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:36:24,973 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ar@@', 'c@@', 'ic@@', 'ic@@', 'ic@@', 'c@@', '&@@', 'a@@', 'pos@@', ';', 'è', ',', 'in', 'un', 'sen@@', 'so', ',', 'il', 'di@@', 'ec@@', 'i', 'del', 'sistema', '.', '</s>']
2024-05-19 11:36:24,974 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:36:24,974 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:36:24,974 - INFO - joeynmt.training - 	Hypothesis: Il arcicicicc'è, in un senso, il dieci del sistema.
2024-05-19 11:36:24,974 - INFO - joeynmt.training - Example #3
2024-05-19 11:36:24,975 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:36:24,975 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:36:24,975 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', 're@@', 'te', 'in', 'gra@@', 'do', 'di', 'in@@', 'contr@@', 'o', 'e', 'in@@', 'contr@@', 'o', 'e', 'in@@', 'contr@@', 'o', '.', '</s>']
2024-05-19 11:36:24,976 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:36:24,976 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:36:24,976 - INFO - joeynmt.training - 	Hypothesis: Crete in grado di incontro e incontro e incontro.
2024-05-19 11:36:24,976 - INFO - joeynmt.training - Example #4
2024-05-19 11:36:24,977 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:36:24,977 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:36:24,977 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'pro@@', 'ssi@@', 'mo', 'pro@@', 'ssi@@', 'mo', 'mostr@@', 'o', 'che', 'vi', 'mostr@@', 'er@@', 'ete', 'un', 'pi@@', 'ac@@', 'ere', 'di', 'ciò', 'che', 'succ@@', 'ede', 'è', 'succ@@', 'esso', 'in', 'cui', 'succ@@', 'ede', 'il', 'pro@@', 'ssi@@', 'mo', 'anno', 'anno', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:36:24,978 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:36:24,978 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:36:24,978 - INFO - joeynmt.training - 	Hypothesis: Il prossimo prossimo mostro che vi mostrerete un piacere di ciò che succede è successo in cui succede il prossimo anno anno 25 anni.
2024-05-19 11:36:28,303 - INFO - joeynmt.training - Epoch   2, Step:     5100, Batch Loss:     1.848284, Batch Acc: 0.469057, Tokens per Sec:    20440, Lr: 0.000300
2024-05-19 11:36:32,504 - INFO - joeynmt.training - Epoch   2, Step:     5200, Batch Loss:     1.980354, Batch Acc: 0.471034, Tokens per Sec:    18044, Lr: 0.000300
2024-05-19 11:36:35,910 - INFO - joeynmt.training - Epoch   2, Step:     5300, Batch Loss:     1.905618, Batch Acc: 0.470445, Tokens per Sec:    22100, Lr: 0.000300
2024-05-19 11:36:39,186 - INFO - joeynmt.training - Epoch   2, Step:     5400, Batch Loss:     1.979437, Batch Acc: 0.475766, Tokens per Sec:    22910, Lr: 0.000300
2024-05-19 11:36:42,458 - INFO - joeynmt.training - Epoch   2, Step:     5500, Batch Loss:     1.601122, Batch Acc: 0.479885, Tokens per Sec:    22534, Lr: 0.000300
2024-05-19 11:36:42,458 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:36:42,458 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:36:53,861 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.89, ppl:   6.61, acc:   0.46, generation: 11.2855[sec], evaluation: 0.0000[sec]
2024-05-19 11:36:53,862 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:36:54,102 - INFO - joeynmt.helpers - delete word_level_model/3000.ckpt
2024-05-19 11:36:54,113 - INFO - joeynmt.training - Example #0
2024-05-19 11:36:54,114 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:36:54,114 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:36:54,114 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'questi', 'due', 's@@', 'li@@', 'de@@', 'm@@', 'ano', 'che', 'il', 'ar@@', 'c@@', 'ico', ',', 'che', 'il', 'gi@@', 'ro', 'di', 'cui', 'il', 'gi@@', 'u@@', 'sto', 'di', 'più', 'di', 'anni', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'del', '4@@', '8', '%', 'di', '4@@', '8', '%', 'di', '4@@', '0', '%', ',', 'si', 's@@', 'itu@@', 'd@@', 'r@@', 'ur@@', 'amente', ',', 'si', 's@@', 'itu@@', 'd@@', 'ono', 'i', '4@@', '0', '.', '</s>']
2024-05-19 11:36:54,115 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:36:54,115 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:36:54,115 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso questi due slidemano che il arcico, che il giro di cui il giusto di più di anni, che per la maggior parte del 48% di 48% di 40%, si situdruramente, si situdono i 40.
2024-05-19 11:36:54,116 - INFO - joeynmt.training - Example #1
2024-05-19 11:36:54,116 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:36:54,116 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:36:54,116 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'com@@', 'pren@@', 'de', 'il', 'seri@@', 'amente', 'di', 'questa', 'è', 'n@@', 'esso', 'di', 'questa', 'non', 'è', 'la', 'ver@@', 'a', 'parte', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:36:54,117 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:36:54,117 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:36:54,117 - INFO - joeynmt.training - 	Hypothesis: Ma questo comprende il seriamente di questa è nesso di questa non è la vera parte del ghiaccio.
2024-05-19 11:36:54,117 - INFO - joeynmt.training - Example #2
2024-05-19 11:36:54,118 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:36:54,118 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:36:54,118 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'c@@', 'li@@', 'ico', 'pubbl@@', 'ico', 'è', ',', 'in', 'un', 'sen@@', 'so', ',', 'la', 'ver@@', 'a', 's@@', 'ina', ',', 'la', 'divent@@', 'a', 'del', 'sistema', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:36:54,119 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:36:54,119 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:36:54,119 - INFO - joeynmt.training - 	Hypothesis: Il cliico pubblico è, in un senso, la vera sina, la diventa del sistema globale.
2024-05-19 11:36:54,119 - INFO - joeynmt.training - Example #3
2024-05-19 11:36:54,120 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:36:54,120 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:36:54,120 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&@@', 'a@@', 'pos@@', ';', 'es@@', 'p@@', 'and@@', 'e', 'in', 'un', 'cer@@', 'to', 'm@@', 'er', '.', '</s>']
2024-05-19 11:36:54,120 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:36:54,121 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:36:54,121 - INFO - joeynmt.training - 	Hypothesis: E 'espande in un certo mer.
2024-05-19 11:36:54,121 - INFO - joeynmt.training - Example #4
2024-05-19 11:36:54,122 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:36:54,122 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:36:54,122 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'pro@@', 'ssi@@', 'mo', 'pro@@', 'ssi@@', 'mo', 'mostr@@', 'o', 'che', 'vi', 'mostr@@', 'er@@', 'ete', 'un', 'pi@@', 'ast@@', 'o', 'di', 'cosa', 'sta', 'succ@@', 'ed@@', 'endo', 'nel', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:36:54,122 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:36:54,123 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:36:54,123 - INFO - joeynmt.training - 	Hypothesis: Il prossimo prossimo mostro che vi mostrerete un piasto di cosa sta succedendo nel 25 anni.
2024-05-19 11:36:57,442 - INFO - joeynmt.training - Epoch   2, Step:     5600, Batch Loss:     1.914563, Batch Acc: 0.480241, Tokens per Sec:    20889, Lr: 0.000300
2024-05-19 11:37:01,878 - INFO - joeynmt.training - Epoch   2, Step:     5700, Batch Loss:     1.705755, Batch Acc: 0.480294, Tokens per Sec:    16478, Lr: 0.000300
2024-05-19 11:37:05,204 - INFO - joeynmt.training - Epoch   2, Step:     5800, Batch Loss:     1.779607, Batch Acc: 0.488030, Tokens per Sec:    22571, Lr: 0.000300
2024-05-19 11:37:08,455 - INFO - joeynmt.training - Epoch   2, Step:     5900, Batch Loss:     1.739339, Batch Acc: 0.484467, Tokens per Sec:    22177, Lr: 0.000300
2024-05-19 11:37:11,786 - INFO - joeynmt.training - Epoch   2, Step:     6000, Batch Loss:     1.786797, Batch Acc: 0.488146, Tokens per Sec:    22128, Lr: 0.000300
2024-05-19 11:37:11,786 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:37:11,786 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:37:24,195 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.85, ppl:   6.38, acc:   0.47, generation: 12.3006[sec], evaluation: 0.0000[sec]
2024-05-19 11:37:24,196 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:37:24,423 - INFO - joeynmt.helpers - delete word_level_model/3500.ckpt
2024-05-19 11:37:24,433 - INFO - joeynmt.training - Example #0
2024-05-19 11:37:24,434 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:37:24,434 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:37:24,434 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'o', 'sc@@', 'or@@', 'so', 'mostr@@', 'ato', 'queste', 'due', 's@@', 'li@@', 'fic@@', 'i', 'di', 'cui', 'di@@', 'mostr@@', 'a', 'che', 'il', 'sol@@', 'ito', 'per', 'la', 'sc@@', 'en@@', 'a', ',', 'che', 'il', 'gh@@', 'i@@', 'acci@@', 'o', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'del', '4@@', '8', 'anni', ',', 'ha', 'la', '4@@', '8', '%', ',', 'ha', 'mostr@@', 'ato', 'il', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:37:24,436 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:37:24,436 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:37:24,436 - INFO - joeynmt.training - 	Hypothesis: Lo scorso mostrato queste due slifici di cui dimostra che il solito per la scena, che il ghiaccio, che per la maggior parte del 48 anni, ha la 48%, ha mostrato il 40%.
2024-05-19 11:37:24,436 - INFO - joeynmt.training - Example #1
2024-05-19 11:37:24,437 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:37:24,437 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:37:24,437 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', 'sc@@', 'e', 'la', 'seri@@', 'amente', 'di', 'questa', 'seri@@', 'amente', ',', 'perché', 'non', 'ha', 'mostr@@', 'ato', 'il', 'problema', 'perché', 'non', 'ci', 'mostr@@', 'a', 'la', 'st@@', 'essa', 'cosa', '.', '</s>']
2024-05-19 11:37:24,438 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:37:24,438 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:37:24,438 - INFO - joeynmt.training - 	Hypothesis: Ma questo capisce la seriamente di questa seriamente, perché non ha mostrato il problema perché non ci mostra la stessa cosa.
2024-05-19 11:37:24,439 - INFO - joeynmt.training - Example #2
2024-05-19 11:37:24,439 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:37:24,439 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:37:24,439 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'c@@', 'li@@', 'ico', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'divent@@', 'a', 'il', 'sistema', 'di', 'cui', 'il', 'sistema', 'di', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:37:24,440 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:37:24,440 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:37:24,440 - INFO - joeynmt.training - 	Hypothesis: Il cliico è, in un certo senso, il diventa il sistema di cui il sistema di globale.
2024-05-19 11:37:24,441 - INFO - joeynmt.training - Example #3
2024-05-19 11:37:24,441 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:37:24,441 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:37:24,441 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'contr@@', 'o', 'e', 'in@@', 'contr@@', 'o', 'e', 'in@@', 'contr@@', 'o', '.', '</s>']
2024-05-19 11:37:24,442 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:37:24,442 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:37:24,442 - INFO - joeynmt.training - 	Hypothesis: E 'incontro e incontro e incontro.
2024-05-19 11:37:24,442 - INFO - joeynmt.training - Example #4
2024-05-19 11:37:24,443 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:37:24,443 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:37:24,443 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'pro@@', 'ssi@@', 'mo', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'pi@@', 'ano', 'di', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'succ@@', 'ede', 'il', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:37:24,444 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:37:24,444 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:37:24,444 - INFO - joeynmt.training - 	Hypothesis: Il prossimo slide che vi mostrerà un piano di di ciò che è successo succede il 25 anni.
2024-05-19 11:37:28,346 - INFO - joeynmt.training - Epoch   2, Step:     6100, Batch Loss:     1.728842, Batch Acc: 0.490060, Tokens per Sec:    17583, Lr: 0.000300
2024-05-19 11:37:32,035 - INFO - joeynmt.training - Epoch   2, Step:     6200, Batch Loss:     1.807543, Batch Acc: 0.499790, Tokens per Sec:    20037, Lr: 0.000300
2024-05-19 11:37:35,259 - INFO - joeynmt.training - Epoch   2, Step:     6300, Batch Loss:     1.606933, Batch Acc: 0.495282, Tokens per Sec:    23211, Lr: 0.000300
2024-05-19 11:37:38,495 - INFO - joeynmt.training - Epoch   2, Step:     6400, Batch Loss:     1.670502, Batch Acc: 0.498095, Tokens per Sec:    23118, Lr: 0.000300
2024-05-19 11:37:42,264 - INFO - joeynmt.training - Epoch   2, Step:     6500, Batch Loss:     1.479044, Batch Acc: 0.498900, Tokens per Sec:    19906, Lr: 0.000300
2024-05-19 11:37:42,265 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:37:42,265 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:37:53,577 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.81, ppl:   6.13, acc:   0.48, generation: 11.2060[sec], evaluation: 0.0000[sec]
2024-05-19 11:37:53,578 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:37:53,808 - INFO - joeynmt.helpers - delete word_level_model/4000.ckpt
2024-05-19 11:37:53,818 - INFO - joeynmt.training - Example #0
2024-05-19 11:37:53,818 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:37:53,819 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:37:53,819 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'ho', 'mostr@@', 'ato', 'queste', 'due', 's@@', 'li@@', 'de', 'così', 'che', 'la', 'de@@', 'm@@', 'ano', 'che', 'la', 'sc@@', 'ar@@', 'c@@', 'ic@@', 'ità', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'di', 'più', 'di', '4@@', '8', 'milioni', 'di', 'anni', ',', 'ha', 's@@', 'ali@@', 're', ',', 'ha', 's@@', 'ali@@', 're', 'il', '4@@', '8', '%', ',', 'ha', 's@@', 'ali@@', 'to', 'd@@', 'ato', 'da', '4@@', '8', '%', '.', '</s>']
2024-05-19 11:37:53,820 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:37:53,820 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:37:53,821 - INFO - joeynmt.training - 	Hypothesis: L'anno ho mostrato queste due slide così che la demano che la scarcicità, che per la maggior parte di più di 48 milioni di anni, ha salire, ha salire il 48%, ha salito dato da 48%.
2024-05-19 11:37:53,821 - INFO - joeynmt.training - Example #1
2024-05-19 11:37:53,821 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:37:53,821 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:37:53,822 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', 're', 'le', 'cose', 'che', 'non', 'si', 'tratt@@', 'a', 'di', 'questa', 'cosa', 'non', 'si', 'mostr@@', 'a', 'perché', 'non', 'lo', 'mostr@@', 'a', 'il', 'problema', 'di', 's@@', 'an@@', 'gu@@', 'i@@', 'da', '.', '</s>']
2024-05-19 11:37:53,822 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:37:53,822 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:37:53,823 - INFO - joeynmt.training - 	Hypothesis: Ma questo capire le cose che non si tratta di questa cosa non si mostra perché non lo mostra il problema di sanguida.
2024-05-19 11:37:53,823 - INFO - joeynmt.training - Example #2
2024-05-19 11:37:53,824 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:37:53,824 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:37:53,824 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'c@@', 'uc@@', 'ina', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'nel', 'sen@@', 'so', 'del', 'sistema', 'glob@@', 'ale', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:37:53,825 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:37:53,825 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:37:53,825 - INFO - joeynmt.training - 	Hypothesis: La cucina è, in un certo senso, nel senso del sistema globale globale.
2024-05-19 11:37:53,826 - INFO - joeynmt.training - Example #3
2024-05-19 11:37:53,826 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:37:53,826 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:37:53,827 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'em@@', 'br@@', 'a', 'in', 'v@@', 'in@@', 'azione', 'e', 'gli', 'in@@', 'contr@@', 'at@@', 'ti', 'in', 'eff@@', 'etti', ',', 'e', 'gli', 'asp@@', 'etti', '.', '</s>']
2024-05-19 11:37:53,827 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:37:53,828 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:37:53,828 - INFO - joeynmt.training - 	Hypothesis: Sembra in vinazione e gli incontratti in effetti, e gli aspetti.
2024-05-19 11:37:53,828 - INFO - joeynmt.training - Example #4
2024-05-19 11:37:53,828 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:37:53,829 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:37:53,829 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'pro@@', 'ssi@@', 'mo', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'pi@@', 'ano', 'di', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'in', 'pi@@', 'ac@@', 'i@@', 'do', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'in', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:37:53,829 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:37:53,830 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:37:53,830 - INFO - joeynmt.training - 	Hypothesis: Il prossimo slide che vi mostrerà un piano di di ciò che è successo in piacido di quello che è successo in 25 anni.
2024-05-19 11:37:58,183 - INFO - joeynmt.training - Epoch   2, Step:     6600, Batch Loss:     1.819363, Batch Acc: 0.496994, Tokens per Sec:    15929, Lr: 0.000300
2024-05-19 11:38:01,555 - INFO - joeynmt.training - Epoch   2, Step:     6700, Batch Loss:     1.802034, Batch Acc: 0.506034, Tokens per Sec:    22563, Lr: 0.000300
2024-05-19 11:38:04,909 - INFO - joeynmt.training - Epoch   2, Step:     6800, Batch Loss:     1.859518, Batch Acc: 0.502565, Tokens per Sec:    22609, Lr: 0.000300
2024-05-19 11:38:08,218 - INFO - joeynmt.training - Epoch   2, Step:     6900, Batch Loss:     1.588984, Batch Acc: 0.504877, Tokens per Sec:    21784, Lr: 0.000300
2024-05-19 11:38:12,528 - INFO - joeynmt.training - Epoch   2, Step:     7000, Batch Loss:     1.554127, Batch Acc: 0.502736, Tokens per Sec:    17094, Lr: 0.000300
2024-05-19 11:38:12,529 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:38:12,529 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:38:24,189 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.78, ppl:   5.96, acc:   0.49, generation: 11.4702[sec], evaluation: 0.0000[sec]
2024-05-19 11:38:24,190 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:38:24,463 - INFO - joeynmt.helpers - delete word_level_model/4500.ckpt
2024-05-19 11:38:24,477 - INFO - joeynmt.training - Example #0
2024-05-19 11:38:24,478 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:38:24,478 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:38:24,478 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'anno', 'sc@@', 'or@@', 'so', 'questi', 'due', 's@@', 'li@@', 'de@@', 'm@@', 'are', 'che', 'la', 'c@@', 'en@@', 'a', 'di', 'c@@', 'en@@', 'a', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'del', '4@@', '8', 'milioni', 'di', 'anni', 'è', 'stato', 'il', '4@@', '8', 'milioni', 'di', 'anni', ',', 'ha', 'con@@', 'divi@@', 'so', 'il', '4@@', '8', '%', '.', '</s>']
2024-05-19 11:38:24,479 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:38:24,479 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:38:24,479 - INFO - joeynmt.training - 	Hypothesis: L'anno anno scorso questi due slidemare che la cena di cena, che per la maggior parte del 48 milioni di anni è stato il 48 milioni di anni, ha condiviso il 48%.
2024-05-19 11:38:24,480 - INFO - joeynmt.training - Example #1
2024-05-19 11:38:24,480 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:38:24,480 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:38:24,480 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'de', 'la', 'seri@@', 'amente', 'di', 'questa', 'con@@', 'sa@@', 'pe@@', 'v@@', 'amente', 'perché', 'non', 'la', 'mostr@@', 'a', 'la', 'ver@@', 'a', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:38:24,481 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:38:24,481 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:38:24,482 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprende la seriamente di questa consapevamente perché non la mostra la vera del ghiaccio.
2024-05-19 11:38:24,482 - INFO - joeynmt.training - Example #2
2024-05-19 11:38:24,482 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:38:24,482 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:38:24,482 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'c@@', 'ar@@', 'c@@', 'ate', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'la', 'bi@@', 'an@@', 'a', 'glob@@', 'ale', ',', 'la', 's@@', 'chi@@', 'ar@@', 'a', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:38:24,483 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:38:24,483 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:38:24,483 - INFO - joeynmt.training - 	Hypothesis: La carcate è, in un certo senso, la biana globale, la schiara globale.
2024-05-19 11:38:24,483 - INFO - joeynmt.training - Example #3
2024-05-19 11:38:24,484 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:38:24,484 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:38:24,484 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'em@@', 'br@@', 'a', 'in', 'in@@', 'contr@@', 'o', 'e', 'contr@@', 'at@@', 'ti', 'in', 'in@@', 'contr@@', 'at@@', 'ti', '.', '</s>']
2024-05-19 11:38:24,485 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:38:24,485 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:38:24,485 - INFO - joeynmt.training - 	Hypothesis: Sembra in incontro e contratti in incontratti.
2024-05-19 11:38:24,485 - INFO - joeynmt.training - Example #4
2024-05-19 11:38:24,485 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:38:24,486 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:38:24,486 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'pro@@', 'ssi@@', 'mo', 's@@', 'li@@', 'de', 'che', 'mostr@@', 'er@@', 'à', 'un', 'f@@', 'ant@@', 'ast@@', 'ico', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'in', 'modo', 'in', 'cui', 'è', 'succ@@', 'esso', 'il', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:38:24,486 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:38:24,487 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:38:24,487 - INFO - joeynmt.training - 	Hypothesis: Il prossimo slide che mostrerà un fantastico di quello che è successo in modo in cui è successo il 25 anni.
2024-05-19 11:38:28,343 - INFO - joeynmt.training - Epoch   2, Step:     7100, Batch Loss:     1.593913, Batch Acc: 0.511533, Tokens per Sec:    18237, Lr: 0.000300
2024-05-19 11:38:31,618 - INFO - joeynmt.training - Epoch   2, Step:     7200, Batch Loss:     1.600150, Batch Acc: 0.511111, Tokens per Sec:    22502, Lr: 0.000300
2024-05-19 11:38:34,890 - INFO - joeynmt.training - Epoch   2, Step:     7300, Batch Loss:     1.486927, Batch Acc: 0.511853, Tokens per Sec:    23105, Lr: 0.000300
2024-05-19 11:38:38,560 - INFO - joeynmt.training - Epoch   2, Step:     7400, Batch Loss:     1.571666, Batch Acc: 0.514270, Tokens per Sec:    19877, Lr: 0.000300
2024-05-19 11:38:42,428 - INFO - joeynmt.training - Epoch   2, Step:     7500, Batch Loss:     1.699521, Batch Acc: 0.508291, Tokens per Sec:    19476, Lr: 0.000300
2024-05-19 11:38:42,428 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:38:42,429 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:38:55,257 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.75, ppl:   5.75, acc:   0.50, generation: 12.7227[sec], evaluation: 0.0000[sec]
2024-05-19 11:38:55,258 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:38:55,480 - INFO - joeynmt.helpers - delete word_level_model/5000.ckpt
2024-05-19 11:38:55,489 - INFO - joeynmt.training - Example #0
2024-05-19 11:38:55,490 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:38:55,490 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:38:55,491 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'anno', 'mostr@@', 'ato', 'questi', 'due', 'due', 's@@', 'li@@', 'de@@', 'm@@', 'ano', 'che', 'il', 'gh@@', 'i@@', 'acci@@', 'o', 'di', 'c@@', '&@@', 'a@@', 'pos@@', ';', 'è', 'stato', 'l@@', '&@@', 'a@@', 'pos@@', ';', 'ulti@@', 'mo', '4@@', '8', 'milioni', 'di', 'anni', 'è', 'stato', 'l@@', '&@@', 'a@@', 'pos@@', ';', 'ulti@@', 'mo', '4@@', '8', '%', '.', '</s>']
2024-05-19 11:38:55,492 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:38:55,492 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:38:55,492 - INFO - joeynmt.training - 	Hypothesis: L'anno anno mostrato questi due due slidemano che il ghiaccio di c'è stato l'ultimo 48 milioni di anni è stato l'ultimo 48%.
2024-05-19 11:38:55,492 - INFO - joeynmt.training - Example #1
2024-05-19 11:38:55,493 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:38:55,493 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:38:55,493 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'com@@', 'pren@@', 'de', 'il', 'problema', 'di', 'questo', 'tipo', 'di', 'parti@@', 'col@@', 'are', 'perché', 'non', 'lo', 'mostr@@', 'a', 'il', 'problema', 'perché', 'non', 'lo', 'mostr@@', 'a', 'il', 'problema', 'della', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:38:55,494 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:38:55,494 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:38:55,494 - INFO - joeynmt.training - 	Hypothesis: Ma questo comprende il problema di questo tipo di particolare perché non lo mostra il problema perché non lo mostra il problema della ghiaccio.
2024-05-19 11:38:55,495 - INFO - joeynmt.training - Example #2
2024-05-19 11:38:55,495 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:38:55,495 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:38:55,495 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'co@@', 'sta', 'sc@@', 'en@@', 'da', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'com@@', 'ore', 'del', 'sistema', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:38:55,496 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:38:55,496 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:38:55,496 - INFO - joeynmt.training - 	Hypothesis: La costa scenda è, in un certo senso, il comore del sistema globale.
2024-05-19 11:38:55,496 - INFO - joeynmt.training - Example #3
2024-05-19 11:38:55,497 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:38:55,497 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:38:55,497 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'contr@@', 'o', 'e', 'contr@@', 'at@@', 'ti', 'in', 's@@', 'int@@', 'en@@', 'zione', '.', '</s>']
2024-05-19 11:38:55,497 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:38:55,498 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:38:55,498 - INFO - joeynmt.training - 	Hypothesis: Spande in incontro e contratti in sintenzione.
2024-05-19 11:38:55,498 - INFO - joeynmt.training - Example #4
2024-05-19 11:38:55,499 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:38:55,499 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:38:55,499 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'pro@@', 'ssi@@', 'mo', 's@@', 'li@@', 'de', 'che', 'sar@@', 'à', 'un', 'pi@@', 'ano', 'di', 'succ@@', 'esso', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'in', 'cui', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:38:55,499 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:38:55,500 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:38:55,500 - INFO - joeynmt.training - 	Hypothesis: Il prossimo slide che sarà un piano di successo di ciò che è successo in cui è successo negli ultimi 25 anni.
2024-05-19 11:38:58,855 - INFO - joeynmt.training - Epoch   2, Step:     7600, Batch Loss:     1.675920, Batch Acc: 0.517848, Tokens per Sec:    21597, Lr: 0.000300
2024-05-19 11:39:02,233 - INFO - joeynmt.training - Epoch   2, Step:     7700, Batch Loss:     1.741059, Batch Acc: 0.519393, Tokens per Sec:    22255, Lr: 0.000300
2024-05-19 11:39:05,946 - INFO - joeynmt.training - Epoch   2, Step:     7800, Batch Loss:     1.739281, Batch Acc: 0.519552, Tokens per Sec:    20556, Lr: 0.000300
2024-05-19 11:39:10,035 - INFO - joeynmt.training - Epoch   2, Step:     7900, Batch Loss:     1.643910, Batch Acc: 0.521544, Tokens per Sec:    18389, Lr: 0.000300
2024-05-19 11:39:13,252 - INFO - joeynmt.training - Epoch   2, Step:     8000, Batch Loss:     1.691040, Batch Acc: 0.517025, Tokens per Sec:    23337, Lr: 0.000300
2024-05-19 11:39:13,253 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:39:13,253 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:39:26,126 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.72, ppl:   5.60, acc:   0.51, generation: 12.7678[sec], evaluation: 0.0000[sec]
2024-05-19 11:39:26,127 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:39:26,361 - INFO - joeynmt.helpers - delete word_level_model/5500.ckpt
2024-05-19 11:39:26,366 - INFO - joeynmt.training - Example #0
2024-05-19 11:39:26,367 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:39:26,367 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:39:26,368 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'anno', 'mostr@@', 'ato', 'questi', 'due', 's@@', 'li@@', 'de@@', 'm@@', 'ano', 'che', 'il', 'de@@', 'm@@', 'ano', 'che', 'il', 'gh@@', 'i@@', 'acci@@', 'o', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'delle', 'tre', 'milioni', 'di', 'anni', 'sono', 'stata', 'l@@', '&@@', 'a@@', 'pos@@', ';', 'ulti@@', 'ma', '4@@', '8', '%', '.', '</s>']
2024-05-19 11:39:26,368 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:39:26,369 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:39:26,369 - INFO - joeynmt.training - 	Hypothesis: L'anno anno mostrato questi due slidemano che il demano che il ghiaccio, che per la maggior parte delle tre milioni di anni sono stata l'ultima 48%.
2024-05-19 11:39:26,369 - INFO - joeynmt.training - Example #1
2024-05-19 11:39:26,369 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:39:26,370 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:39:26,370 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'com@@', 'pren@@', 'de', 'il', 'ser@@', 'io', 'di', 'questo', 'problema', 'di', 'questo', 'problema', 'perché', 'non', 'è', 'il', 'problema', 'perché', 'non', 'è', 'mostr@@', 'a', 'la', 'sc@@', 'en@@', 'a', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:39:26,370 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:39:26,371 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:39:26,371 - INFO - joeynmt.training - 	Hypothesis: Ma questo comprende il serio di questo problema di questo problema perché non è il problema perché non è mostra la scena del ghiaccio.
2024-05-19 11:39:26,371 - INFO - joeynmt.training - Example #2
2024-05-19 11:39:26,372 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:39:26,372 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:39:26,372 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'c@@', 'li@@', 'ica', 'è', 'una', 'macch@@', 'ina', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'glob@@', 'ale', 'del', 'sistema', 'glob@@', 'ale', 'del', 'sistema', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:39:26,372 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:39:26,373 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:39:26,373 - INFO - joeynmt.training - 	Hypothesis: La cliica è una macchina, in un certo senso, il cuore globale del sistema globale del sistema globale.
2024-05-19 11:39:26,373 - INFO - joeynmt.training - Example #3
2024-05-19 11:39:26,373 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:39:26,374 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:39:26,374 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'contr@@', 'o', 'e', 'contr@@', 'o', 'i', 'contr@@', 'at@@', 'ti', 'in', 'eff@@', 'etti', '.', '</s>']
2024-05-19 11:39:26,374 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:39:26,375 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:39:26,375 - INFO - joeynmt.training - 	Hypothesis: Spande in incontro e contro i contratti in effetti.
2024-05-19 11:39:26,375 - INFO - joeynmt.training - Example #4
2024-05-19 11:39:26,375 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:39:26,376 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:39:26,376 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'ho', 'mostr@@', 'ato', 'un', 'pi@@', 'ano', 'di', 'pi@@', 'e@@', 'di', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'in', 'pi@@', 'e@@', 'di', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:39:26,376 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:39:26,377 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:39:26,377 - INFO - joeynmt.training - 	Hypothesis: L'ho mostrato un piano di piedi di quello che è successo in piedi di quello che è successo negli ultimi 25 anni.
2024-05-19 11:39:29,705 - INFO - joeynmt.training - Epoch   2, Step:     8100, Batch Loss:     1.761905, Batch Acc: 0.520157, Tokens per Sec:    21192, Lr: 0.000300
2024-05-19 11:39:33,065 - INFO - joeynmt.training - Epoch   2, Step:     8200, Batch Loss:     1.678290, Batch Acc: 0.516924, Tokens per Sec:    22301, Lr: 0.000300
2024-05-19 11:39:37,348 - INFO - joeynmt.training - Epoch   2, Step:     8300, Batch Loss:     1.709926, Batch Acc: 0.525147, Tokens per Sec:    17633, Lr: 0.000300
2024-05-19 11:39:40,618 - INFO - joeynmt.training - Epoch   2, Step:     8400, Batch Loss:     1.754384, Batch Acc: 0.524431, Tokens per Sec:    22837, Lr: 0.000300
2024-05-19 11:39:43,895 - INFO - joeynmt.training - Epoch   2, Step:     8500, Batch Loss:     1.762404, Batch Acc: 0.530639, Tokens per Sec:    23168, Lr: 0.000300
2024-05-19 11:39:43,896 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:39:43,896 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:39:56,072 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.69, ppl:   5.44, acc:   0.52, generation: 12.0699[sec], evaluation: 0.0000[sec]
2024-05-19 11:39:56,073 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:39:56,292 - INFO - joeynmt.helpers - delete word_level_model/6000.ckpt
2024-05-19 11:39:56,301 - INFO - joeynmt.training - Example #0
2024-05-19 11:39:56,302 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:39:56,302 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:39:56,303 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'anno', 'ho', 'mostr@@', 'ato', 'queste', 'due', 's@@', 'li@@', 'de@@', 'm@@', 'ica', ',', 'e', 'che', 'la', 'c@@', 'ic@@', 'ina', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'delle', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'del', '4@@', '0', '%', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'del', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:39:56,304 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:39:56,304 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:39:56,304 - INFO - joeynmt.training - 	Hypothesis: L'anno anno ho mostrato queste due slidemica, e che la cicina, che per la maggior parte delle tre milioni di anni è stata la dimensione del 40% di anni è stata la dimensione del 40%.
2024-05-19 11:39:56,305 - INFO - joeynmt.training - Example #1
2024-05-19 11:39:56,305 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:39:56,305 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:39:56,305 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'com@@', 'pren@@', 'de', 'la', 'seri@@', 'amente', 'di', 'questa', 'seri@@', 'amente', 'di', 'questa', 'seri@@', 'amente', ',', 'perché', 'non', 'lo', 'mostr@@', 'a', 'la', 'sc@@', 'en@@', 'a', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:39:56,306 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:39:56,306 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:39:56,306 - INFO - joeynmt.training - 	Hypothesis: Ma questo comprende la seriamente di questa seriamente di questa seriamente, perché non lo mostra la scena del ghiaccio.
2024-05-19 11:39:56,307 - INFO - joeynmt.training - Example #2
2024-05-19 11:39:56,307 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:39:56,307 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:39:56,307 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'c@@', 'acci@@', 'a', 'c@@', 'li@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'glob@@', 'ale', 'del', 'sistema', 'glob@@', 'ale', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:39:56,308 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:39:56,308 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:39:56,308 - INFO - joeynmt.training - 	Hypothesis: La caccia cliica è, in un certo senso, il cuore globale del sistema globale globale.
2024-05-19 11:39:56,309 - INFO - joeynmt.training - Example #3
2024-05-19 11:39:56,309 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:39:56,309 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:39:56,309 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'p@@', 'and@@', 'e', 'in', 'un', 'cer@@', 'to', 'v@@', 'ento', 'e', 'i', 'contr@@', 'at@@', 'ti', 'in', 'cer@@', 'to', '.', '</s>']
2024-05-19 11:39:56,310 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:39:56,310 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:39:56,310 - INFO - joeynmt.training - 	Hypothesis: Spande in un certo vento e i contratti in certo.
2024-05-19 11:39:56,310 - INFO - joeynmt.training - Example #4
2024-05-19 11:39:56,311 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:39:56,311 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:39:56,311 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'ho', 'mostr@@', 'ato', 'un', 'pi@@', 'ast@@', 'o', 'di', 'cui', 'vi', 'mostr@@', 'er@@', 'est@@', 'e', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:39:56,312 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:39:56,312 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:39:56,312 - INFO - joeynmt.training - 	Hypothesis: L'ho mostrato un piasto di cui vi mostrereste di ciò che è successo negli ultimi 25 anni.
2024-05-19 11:39:59,667 - INFO - joeynmt.training - Epoch   2, Step:     8600, Batch Loss:     1.700106, Batch Acc: 0.521933, Tokens per Sec:    21041, Lr: 0.000300
2024-05-19 11:40:03,617 - INFO - joeynmt.training - Epoch   2, Step:     8700, Batch Loss:     1.718834, Batch Acc: 0.523454, Tokens per Sec:    18957, Lr: 0.000300
2024-05-19 11:40:07,480 - INFO - joeynmt.training - Epoch   2, Step:     8800, Batch Loss:     1.543893, Batch Acc: 0.528974, Tokens per Sec:    19094, Lr: 0.000300
2024-05-19 11:40:10,714 - INFO - joeynmt.training - Epoch   2, Step:     8900, Batch Loss:     1.565043, Batch Acc: 0.528758, Tokens per Sec:    22678, Lr: 0.000300
2024-05-19 11:40:14,015 - INFO - joeynmt.training - Epoch   2, Step:     9000, Batch Loss:     1.643124, Batch Acc: 0.533371, Tokens per Sec:    22716, Lr: 0.000300
2024-05-19 11:40:14,016 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:40:14,016 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:40:24,838 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.67, ppl:   5.31, acc:   0.52, generation: 10.7129[sec], evaluation: 0.0000[sec]
2024-05-19 11:40:24,839 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:40:25,087 - INFO - joeynmt.helpers - delete word_level_model/6500.ckpt
2024-05-19 11:40:25,097 - INFO - joeynmt.training - Example #0
2024-05-19 11:40:25,098 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:40:25,098 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:40:25,098 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'o', 'anno', 'anno', 'sc@@', 'or@@', 'so', 'anno', 'questi', 'due', 's@@', 'li@@', 'de@@', 'm@@', 'ori@@', 're', 'così', 'che', 'la', 'de@@', 'sol@@', 'a', 'che', 'la', 'c@@', 'en@@', 'a', 'sc@@', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'dei', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'della', 'dimen@@', 'sione', 'della', 'dimen@@', 'sione', 'del', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:40:25,099 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:40:25,099 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:40:25,100 - INFO - joeynmt.training - 	Hypothesis: Lo anno anno scorso anno questi due slidemorire così che la desola che la cena scartica, che per la maggior parte dei tre milioni di anni è stata la dimensione della dimensione della dimensione del 40%.
2024-05-19 11:40:25,100 - INFO - joeynmt.training - Example #1
2024-05-19 11:40:25,100 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:40:25,101 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:40:25,101 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', 'sc@@', 'e', 'la', 'seri@@', 'amente', 'di', 'questa', 'com@@', 'pren@@', 'sione', ',', 'non', 'lo', 'mostr@@', 'a', 'la', 'ver@@', 'ità', 'perché', 'non', 'lo', 'mostr@@', 'a', 'il', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:40:25,102 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:40:25,102 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:40:25,102 - INFO - joeynmt.training - 	Hypothesis: Ma questo capisce la seriamente di questa comprensione, non lo mostra la verità perché non lo mostra il ghiaccio.
2024-05-19 11:40:25,102 - INFO - joeynmt.training - Example #2
2024-05-19 11:40:25,103 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:40:25,103 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:40:25,103 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'c@@', 'acci@@', 'a', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'cu@@', 'ore', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:40:25,104 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:40:25,104 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:40:25,104 - INFO - joeynmt.training - 	Hypothesis: La caccia è, in un certo senso, il cuore del cuore globale.
2024-05-19 11:40:25,104 - INFO - joeynmt.training - Example #3
2024-05-19 11:40:25,105 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:40:25,105 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:40:25,105 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['R@@', 'ic@@', 'ac@@', 'i', 'in', 'int@@', 'en@@', 'zione', 'e', 'i', 'contr@@', 'ac@@', 'ti', 'in', 'cer@@', 'ti', '.', '</s>']
2024-05-19 11:40:25,106 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:40:25,106 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:40:25,106 - INFO - joeynmt.training - 	Hypothesis: Ricaci in intenzione e i contracti in certi.
2024-05-19 11:40:25,106 - INFO - joeynmt.training - Example #4
2024-05-19 11:40:25,107 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:40:25,107 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:40:25,107 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'pro@@', 'ssi@@', 'mo', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'ete', 'un', 'pi@@', 'do', 'pi@@', 'u', '&@@', 'a@@', 'pos@@', ';', 'più', 'velo@@', 'c@@', 'emente', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:40:25,108 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:40:25,108 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:40:25,108 - INFO - joeynmt.training - 	Hypothesis: Il prossimo slide che vi mostrerete un pido piu 'più velocemente negli ultimi 25 anni.
2024-05-19 11:40:28,378 - INFO - joeynmt.training - Epoch   2, Step:     9100, Batch Loss:     1.716180, Batch Acc: 0.535033, Tokens per Sec:    20777, Lr: 0.000300
2024-05-19 11:40:32,179 - INFO - joeynmt.training - Epoch   2, Step:     9200, Batch Loss:     1.531722, Batch Acc: 0.532098, Tokens per Sec:    19038, Lr: 0.000300
2024-05-19 11:40:35,945 - INFO - joeynmt.training - Epoch   2, Step:     9300, Batch Loss:     1.550374, Batch Acc: 0.523970, Tokens per Sec:    19579, Lr: 0.000300
2024-05-19 11:40:39,315 - INFO - joeynmt.training - Epoch   2, Step:     9400, Batch Loss:     1.649272, Batch Acc: 0.529213, Tokens per Sec:    23206, Lr: 0.000300
2024-05-19 11:40:42,584 - INFO - joeynmt.training - Epoch   2, Step:     9500, Batch Loss:     1.730924, Batch Acc: 0.531304, Tokens per Sec:    22401, Lr: 0.000300
2024-05-19 11:40:42,584 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:40:42,585 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:40:54,542 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.65, ppl:   5.21, acc:   0.53, generation: 11.8519[sec], evaluation: 0.0000[sec]
2024-05-19 11:40:54,543 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:40:54,773 - INFO - joeynmt.helpers - delete word_level_model/7000.ckpt
2024-05-19 11:40:54,783 - INFO - joeynmt.training - Example #0
2024-05-19 11:40:54,784 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:40:54,784 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:40:54,784 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'o', 'anno', 'sc@@', 'or@@', 'so', 'anno', 'sc@@', 'or@@', 'so', 'questi', 'due', 's@@', 'li@@', 'de', 'così', 'che', 'la', 'de@@', 'sol@@', 'a', 'che', 'la', 'macch@@', 'ina', 'sc@@', 'att@@', 'a', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'delle', 'dimen@@', 'sioni', 'del', 'me@@', 'zzo', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'del', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:40:54,785 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:40:54,786 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:40:54,786 - INFO - joeynmt.training - 	Hypothesis: Lo anno scorso anno scorso questi due slide così che la desola che la macchina scatta, che per la maggior parte delle dimensioni del mezzo di anni è stata la dimensione del 40%.
2024-05-19 11:40:54,786 - INFO - joeynmt.training - Example #1
2024-05-19 11:40:54,787 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:40:54,787 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:40:54,787 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', 'sc@@', 'e', 'il', 'ser@@', 'io', 'di', 'questo', 'problema', 'perché', 'non', 'lo', 'mostr@@', 'a', 'il', 'problema', 'perché', 'non', 'lo', 'mostr@@', 'a', 'la', 'po@@', 'll@@', 'a', '.', '</s>']
2024-05-19 11:40:54,788 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:40:54,788 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:40:54,789 - INFO - joeynmt.training - 	Hypothesis: Ma questo capisce il serio di questo problema perché non lo mostra il problema perché non lo mostra la polla.
2024-05-19 11:40:54,789 - INFO - joeynmt.training - Example #2
2024-05-19 11:40:54,789 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:40:54,790 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:40:54,790 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'sistema', 'ar@@', 't@@', 'ico', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'cu@@', 'ore', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:40:54,790 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:40:54,791 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:40:54,791 - INFO - joeynmt.training - 	Hypothesis: Il sistema artico è, in un certo senso, il cuore del cuore globale.
2024-05-19 11:40:54,791 - INFO - joeynmt.training - Example #3
2024-05-19 11:40:54,791 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:40:54,792 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:40:54,792 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&@@', 'a@@', 'pos@@', ';', 'es@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'su@@', 'm@@', 'er', '.', '</s>']
2024-05-19 11:40:54,792 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:40:54,792 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:40:54,793 - INFO - joeynmt.training - 	Hypothesis: E 'espande in insumer.
2024-05-19 11:40:54,793 - INFO - joeynmt.training - Example #4
2024-05-19 11:40:54,793 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:40:54,793 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:40:54,794 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'pro@@', 'ssi@@', 'mo', 's@@', 'li@@', 'de', 'che', 'vi', 'sar@@', 'à', 'un', 'pi@@', 'ano', 'pi@@', 'ano', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:40:54,794 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:40:54,794 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:40:54,795 - INFO - joeynmt.training - 	Hypothesis: Il prossimo slide che vi sarà un piano piano di quello che è successo negli ultimi 25 anni.
2024-05-19 11:40:57,541 - INFO - joeynmt.training - Epoch   2: total training loss 8301.43
2024-05-19 11:40:57,542 - INFO - joeynmt.training - EPOCH 3
2024-05-19 11:40:58,333 - INFO - joeynmt.training - Epoch   3, Step:     9600, Batch Loss:     1.442954, Batch Acc: 0.546605, Tokens per Sec:    17313, Lr: 0.000300
2024-05-19 11:41:02,669 - INFO - joeynmt.training - Epoch   3, Step:     9700, Batch Loss:     1.614994, Batch Acc: 0.554101, Tokens per Sec:    16603, Lr: 0.000300
2024-05-19 11:41:06,141 - INFO - joeynmt.training - Epoch   3, Step:     9800, Batch Loss:     1.669507, Batch Acc: 0.554911, Tokens per Sec:    21583, Lr: 0.000300
2024-05-19 11:41:09,434 - INFO - joeynmt.training - Epoch   3, Step:     9900, Batch Loss:     1.435703, Batch Acc: 0.556941, Tokens per Sec:    22792, Lr: 0.000300
2024-05-19 11:41:12,856 - INFO - joeynmt.training - Epoch   3, Step:    10000, Batch Loss:     1.465369, Batch Acc: 0.552541, Tokens per Sec:    21670, Lr: 0.000300
2024-05-19 11:41:12,857 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:41:12,857 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:41:25,275 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.63, ppl:   5.13, acc:   0.53, generation: 12.3090[sec], evaluation: 0.0000[sec]
2024-05-19 11:41:25,276 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:41:25,500 - INFO - joeynmt.helpers - delete word_level_model/7500.ckpt
2024-05-19 11:41:25,510 - INFO - joeynmt.training - Example #0
2024-05-19 11:41:25,511 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:41:25,511 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:41:25,511 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 's@@', 'li@@', 'de', ',', 'quindi', 'che', 'la', 'cap@@', 'ac@@', 'ità', 'di', 'sc@@', 'or@@', 'sa', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'del', 'm@@', 'uro', ',', 'ha', 'con@@', 'divi@@', 'so', 'dal', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:41:25,512 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:41:25,513 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:41:25,513 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso, ho mostrato queste due slide, quindi che la capacità di scorsa, che per la maggior parte del muro, ha condiviso dal 40%.
2024-05-19 11:41:25,513 - INFO - joeynmt.training - Example #1
2024-05-19 11:41:25,514 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:41:25,514 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:41:25,514 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', 'sc@@', 'e', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'perché', 'non', 'lo', 'mostr@@', 'a', 'il', 'problema', 'non', 'mostr@@', 'a', 'il', 'gh@@', 'i@@', 'acci@@', 'o', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:41:25,515 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:41:25,515 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:41:25,515 - INFO - joeynmt.training - 	Hypothesis: Ma questo capisce la seriamente di questo particolare perché non lo mostra il problema non mostra il ghiaccio del ghiaccio.
2024-05-19 11:41:25,515 - INFO - joeynmt.training - Example #2
2024-05-19 11:41:25,516 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:41:25,516 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:41:25,516 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'c@@', 'att@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'glob@@', 'ale', 'del', 'sistema', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:41:25,517 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:41:25,517 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:41:25,518 - INFO - joeynmt.training - 	Hypothesis: La cattica è, in un certo senso, il cuore del sistema globale del sistema globale.
2024-05-19 11:41:25,518 - INFO - joeynmt.training - Example #3
2024-05-19 11:41:25,518 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:41:25,518 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:41:25,519 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'p@@', 'and@@', 'e', 'in', 'v@@', 'int@@', 'a', 'e', 'contr@@', 'at@@', 'ti', 'in', 'sic@@', 'uro', '.', '</s>']
2024-05-19 11:41:25,519 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:41:25,519 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:41:25,520 - INFO - joeynmt.training - 	Hypothesis: Spande in vinta e contratti in sicuro.
2024-05-19 11:41:25,520 - INFO - joeynmt.training - Example #4
2024-05-19 11:41:25,520 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:41:25,520 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:41:25,520 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'pro@@', 'ssi@@', 'mo', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'ete', 'un', 'pi@@', 'u', 'pi@@', 'u', '&@@', 'a@@', 'pos@@', ';', 'più', 'velo@@', 'c@@', 'emente', 'dell@@', '&@@', 'a@@', 'pos@@', ';', 'ulti@@', 'mo', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:41:25,521 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:41:25,521 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:41:25,521 - INFO - joeynmt.training - 	Hypothesis: Il prossimo slide che vi mostrerete un piu piu 'più velocemente dell'ultimo 25 anni.
2024-05-19 11:41:29,719 - INFO - joeynmt.training - Epoch   3, Step:    10100, Batch Loss:     1.571225, Batch Acc: 0.548744, Tokens per Sec:    16791, Lr: 0.000300
2024-05-19 11:41:33,246 - INFO - joeynmt.training - Epoch   3, Step:    10200, Batch Loss:     1.423117, Batch Acc: 0.550210, Tokens per Sec:    20693, Lr: 0.000300
2024-05-19 11:41:36,469 - INFO - joeynmt.training - Epoch   3, Step:    10300, Batch Loss:     1.525696, Batch Acc: 0.553165, Tokens per Sec:    22760, Lr: 0.000300
2024-05-19 11:41:39,714 - INFO - joeynmt.training - Epoch   3, Step:    10400, Batch Loss:     1.611750, Batch Acc: 0.552908, Tokens per Sec:    22616, Lr: 0.000300
2024-05-19 11:41:43,799 - INFO - joeynmt.training - Epoch   3, Step:    10500, Batch Loss:     1.398165, Batch Acc: 0.558968, Tokens per Sec:    18485, Lr: 0.000300
2024-05-19 11:41:43,799 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:41:43,800 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:41:55,172 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.63, ppl:   5.10, acc:   0.54, generation: 11.1820[sec], evaluation: 0.0000[sec]
2024-05-19 11:41:55,173 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:41:55,425 - INFO - joeynmt.helpers - delete word_level_model/8000.ckpt
2024-05-19 11:41:55,438 - INFO - joeynmt.training - Example #0
2024-05-19 11:41:55,438 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:41:55,439 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:41:55,439 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'sc@@', 'or@@', 'so', 'che', 'di@@', 'stri@@', 'sc@@', 'e', 'che', 'de@@', 'mon@@', 'di@@', 'ale', 'che', 'la', 'de@@', 'sol@@', 'a', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'delle', 'dimen@@', 'sioni', 'del', '4@@', '0', '%', 'di', 'anni', 'è', 'stato', 'l@@', '&@@', 'a@@', 'pos@@', ';', 'ulti@@', 'ma', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:41:55,440 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:41:55,440 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:41:55,440 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso scorso che distrisce che demondiale che la desola, che per la maggior parte delle dimensioni del 40% di anni è stato l'ultima 40%.
2024-05-19 11:41:55,440 - INFO - joeynmt.training - Example #1
2024-05-19 11:41:55,441 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:41:55,441 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:41:55,441 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', 'sc@@', 'e', 'la', 'seri@@', 'e', 'di', 'questa', 'com@@', 'pren@@', 'sione', 'del', 'problema', 'perché', 'non', 'ci', 'mostr@@', 'a', 'il', 'gh@@', 'i@@', 'acci@@', 'o', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:41:55,442 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:41:55,442 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:41:55,442 - INFO - joeynmt.training - 	Hypothesis: Ma questo capisce la serie di questa comprensione del problema perché non ci mostra il ghiaccio del ghiaccio.
2024-05-19 11:41:55,442 - INFO - joeynmt.training - Example #2
2024-05-19 11:41:55,442 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:41:55,443 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:41:55,443 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'sc@@', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'nel', 'cu@@', 'ore', 'del', 'cu@@', 'ore', 'glob@@', 'ale', 'del', 'sistema', 'del', 'sistema', 'del', 'sistema', 'del', 'sistema', 'del', 'sistema', 'del', 'sistema', '.', '</s>']
2024-05-19 11:41:55,443 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:41:55,444 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:41:55,444 - INFO - joeynmt.training - 	Hypothesis: La scartica è, in un certo senso, nel cuore del cuore globale del sistema del sistema del sistema del sistema del sistema del sistema.
2024-05-19 11:41:55,444 - INFO - joeynmt.training - Example #3
2024-05-19 11:41:55,444 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:41:55,444 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:41:55,444 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ci', 'es@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'vi@@', 'a', 'e', 'contr@@', 'at@@', 'ti', 'in', 'su@@', 'm@@', 'ere', '.', '</s>']
2024-05-19 11:41:55,445 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:41:55,445 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:41:55,445 - INFO - joeynmt.training - 	Hypothesis: Ci espande in invia e contratti in sumere.
2024-05-19 11:41:55,446 - INFO - joeynmt.training - Example #4
2024-05-19 11:41:55,446 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:41:55,446 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:41:55,446 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'pro@@', 'ssi@@', 'mo', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'ete', 'un', 'pi@@', 'ano', 'pi@@', 'ano', 'di', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:41:55,447 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:41:55,447 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:41:55,447 - INFO - joeynmt.training - 	Hypothesis: Il prossimo slide che vi mostrerete un piano piano di di quello che è successo negli ultimi 25 anni.
2024-05-19 11:41:59,747 - INFO - joeynmt.training - Epoch   3, Step:    10600, Batch Loss:     1.477199, Batch Acc: 0.552468, Tokens per Sec:    16749, Lr: 0.000300
2024-05-19 11:42:03,121 - INFO - joeynmt.training - Epoch   3, Step:    10700, Batch Loss:     1.601576, Batch Acc: 0.555627, Tokens per Sec:    22594, Lr: 0.000300
2024-05-19 11:42:06,470 - INFO - joeynmt.training - Epoch   3, Step:    10800, Batch Loss:     1.424659, Batch Acc: 0.554728, Tokens per Sec:    22691, Lr: 0.000300
2024-05-19 11:42:10,009 - INFO - joeynmt.training - Epoch   3, Step:    10900, Batch Loss:     1.530362, Batch Acc: 0.557099, Tokens per Sec:    21117, Lr: 0.000300
2024-05-19 11:42:14,158 - INFO - joeynmt.training - Epoch   3, Step:    11000, Batch Loss:     1.572500, Batch Acc: 0.557402, Tokens per Sec:    18217, Lr: 0.000300
2024-05-19 11:42:14,158 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:42:14,159 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:42:25,398 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.60, ppl:   4.97, acc:   0.54, generation: 11.0456[sec], evaluation: 0.0000[sec]
2024-05-19 11:42:25,398 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:42:25,686 - INFO - joeynmt.helpers - delete word_level_model/8500.ckpt
2024-05-19 11:42:25,700 - INFO - joeynmt.training - Example #0
2024-05-19 11:42:25,701 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:42:25,701 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:42:25,702 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'rit@@', 'to', 'che', 'ho', 'mostr@@', 'ato', 'queste', 'due', 's@@', 'li@@', 'de', 'così', 'che', 'la', 'de@@', 'sol@@', 'a', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'delle', 'dimen@@', 'sioni', 'del', '4@@', '8', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'del', '4@@', '8', '%', '.', '</s>']
2024-05-19 11:42:25,703 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:42:25,703 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:42:25,703 - INFO - joeynmt.training - 	Hypothesis: L'anno scritto che ho mostrato queste due slide così che la desola, che per la maggior parte delle dimensioni del 48 milioni di anni è stata la dimensione del 48%.
2024-05-19 11:42:25,703 - INFO - joeynmt.training - Example #1
2024-05-19 11:42:25,704 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:42:25,704 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:42:25,704 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', 'sc@@', 'e', 'la', 'seri@@', 'amente', 'di', 'questo', 'problema', 'perché', 'non', 'ci', 'ci', 'fa', 'la', 'velo@@', 'c@@', 'ità', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:42:25,705 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:42:25,705 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:42:25,705 - INFO - joeynmt.training - 	Hypothesis: Ma questo capisce la seriamente di questo problema perché non ci ci fa la velocità del ghiaccio.
2024-05-19 11:42:25,705 - INFO - joeynmt.training - Example #2
2024-05-19 11:42:25,706 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:42:25,706 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:42:25,706 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'macch@@', 'ina', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'sen@@', 'so', ',', 'nel', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'cu@@', 'ore', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:42:25,707 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:42:25,707 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:42:25,707 - INFO - joeynmt.training - 	Hypothesis: La macchina artica è, in un senso, nel senso, il cuore del cuore globale.
2024-05-19 11:42:25,707 - INFO - joeynmt.training - Example #3
2024-05-19 11:42:25,707 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:42:25,708 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:42:25,708 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'em@@', 'p@@', 'and@@', 'e', 'in', 'sost@@', 'anza', 'e', 'contr@@', 'at@@', 'ti', 'in', 'sost@@', 'anza', '.', '</s>']
2024-05-19 11:42:25,708 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:42:25,709 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:42:25,709 - INFO - joeynmt.training - 	Hypothesis: Sempande in sostanza e contratti in sostanza.
2024-05-19 11:42:25,709 - INFO - joeynmt.training - Example #4
2024-05-19 11:42:25,709 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:42:25,709 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:42:25,710 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'pro@@', 'ssi@@', 'mo', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'ete', 'un', 'pi@@', 'ano', 'di', 'velo@@', 'c@@', 'ità', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:42:25,710 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:42:25,711 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:42:25,711 - INFO - joeynmt.training - 	Hypothesis: Il prossimo slide che vi mostrerete un piano di velocità di ciò che è successo negli ultimi 25 anni.
2024-05-19 11:42:29,452 - INFO - joeynmt.training - Epoch   3, Step:    11100, Batch Loss:     1.561825, Batch Acc: 0.556536, Tokens per Sec:    18394, Lr: 0.000300
2024-05-19 11:42:32,697 - INFO - joeynmt.training - Epoch   3, Step:    11200, Batch Loss:     1.510545, Batch Acc: 0.559419, Tokens per Sec:    22837, Lr: 0.000300
2024-05-19 11:42:35,941 - INFO - joeynmt.training - Epoch   3, Step:    11300, Batch Loss:     1.473597, Batch Acc: 0.557222, Tokens per Sec:    22204, Lr: 0.000300
2024-05-19 11:42:39,830 - INFO - joeynmt.training - Epoch   3, Step:    11400, Batch Loss:     1.241860, Batch Acc: 0.555267, Tokens per Sec:    19290, Lr: 0.000300
2024-05-19 11:42:43,546 - INFO - joeynmt.training - Epoch   3, Step:    11500, Batch Loss:     1.613128, Batch Acc: 0.556057, Tokens per Sec:    19784, Lr: 0.000300
2024-05-19 11:42:43,546 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:42:43,547 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:42:56,375 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.60, ppl:   4.94, acc:   0.54, generation: 12.7213[sec], evaluation: 0.0000[sec]
2024-05-19 11:42:56,376 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:42:56,598 - INFO - joeynmt.helpers - delete word_level_model/9000.ckpt
2024-05-19 11:42:56,603 - INFO - joeynmt.training - Example #0
2024-05-19 11:42:56,604 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:42:56,604 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:42:56,604 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'ho', 'mostr@@', 'ato', 'queste', 'due', 's@@', 'a@@', 'positi@@', 've', 'così', 'che', 'la', 'c@@', 'ic@@', 'ità', 'di', 'gh@@', 'i@@', 'acci@@', 'o', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'delle', 'dimen@@', 'sioni', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '4@@', '8', 'milioni', 'di', 'anni', 'è', 'stato', 'il', '4@@', '8', '%', 'della', 'dimen@@', 'sione', 'del', '4@@', '8', '%', '.', '</s>']
2024-05-19 11:42:56,605 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:42:56,605 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:42:56,605 - INFO - joeynmt.training - 	Hypothesis: L'anno ho mostrato queste due sapositive così che la cicità di ghiaccio, che per la maggior parte delle dimensioni del ghiaccio 48 milioni di anni è stato il 48% della dimensione del 48%.
2024-05-19 11:42:56,606 - INFO - joeynmt.training - Example #1
2024-05-19 11:42:56,606 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:42:56,606 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:42:56,606 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', 'sc@@', 'e', 'la', 'seri@@', 'amente', 'di', 'questo', 'problema', 'perché', 'non', 'lo', 'mostr@@', 'a', 'la', 'po@@', 'll@@', 'a', 'perché', 'non', 'lo', 'mostr@@', 'a', 'la', 'po@@', 'll@@', 'a', 'della', 'gh@@', 'i@@', 'acci@@', 'a', '.', '</s>']
2024-05-19 11:42:56,607 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:42:56,607 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:42:56,607 - INFO - joeynmt.training - 	Hypothesis: Ma questo capisce la seriamente di questo problema perché non lo mostra la polla perché non lo mostra la polla della ghiaccia.
2024-05-19 11:42:56,608 - INFO - joeynmt.training - Example #2
2024-05-19 11:42:56,608 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:42:56,608 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:42:56,608 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'c@@', 'acci@@', 'a', 'c@@', 'li@@', 'mat@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:42:56,609 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:42:56,609 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:42:56,610 - INFO - joeynmt.training - 	Hypothesis: La caccia climatica è, in un certo senso, il cuore del climatico globale.
2024-05-19 11:42:56,610 - INFO - joeynmt.training - Example #3
2024-05-19 11:42:56,610 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:42:56,610 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:42:56,610 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'per@@', 'ò', 'es@@', 'p@@', 'and@@', 'e', 'in', 'un', 'cer@@', 'to', 'e', 'contr@@', 'atto', 'in', 'sost@@', 'anza', '.', '</s>']
2024-05-19 11:42:56,611 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:42:56,611 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:42:56,611 - INFO - joeynmt.training - 	Hypothesis: Sperò espande in un certo e contratto in sostanza.
2024-05-19 11:42:56,612 - INFO - joeynmt.training - Example #4
2024-05-19 11:42:56,612 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:42:56,612 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:42:56,612 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'pro@@', 'ssi@@', 'mo', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'ete', 'un', 'pi@@', 'u', '&@@', 'a@@', 'pos@@', ';', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:42:56,613 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:42:56,613 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:42:56,613 - INFO - joeynmt.training - 	Hypothesis: Il prossimo slide che vi mostrerete un piu 'di quello che è successo negli ultimi 25 anni.
2024-05-19 11:42:59,988 - INFO - joeynmt.training - Epoch   3, Step:    11600, Batch Loss:     1.509713, Batch Acc: 0.556521, Tokens per Sec:    20457, Lr: 0.000300
2024-05-19 11:43:03,277 - INFO - joeynmt.training - Epoch   3, Step:    11700, Batch Loss:     1.410250, Batch Acc: 0.566613, Tokens per Sec:    23043, Lr: 0.000300
2024-05-19 11:43:07,070 - INFO - joeynmt.training - Epoch   3, Step:    11800, Batch Loss:     1.512163, Batch Acc: 0.561040, Tokens per Sec:    20326, Lr: 0.000300
2024-05-19 11:43:11,030 - INFO - joeynmt.training - Epoch   3, Step:    11900, Batch Loss:     1.370736, Batch Acc: 0.562821, Tokens per Sec:    18802, Lr: 0.000300
2024-05-19 11:43:14,349 - INFO - joeynmt.training - Epoch   3, Step:    12000, Batch Loss:     1.613260, Batch Acc: 0.560432, Tokens per Sec:    23168, Lr: 0.000300
2024-05-19 11:43:14,349 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:43:14,350 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:43:26,366 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.59, ppl:   4.88, acc:   0.55, generation: 11.9120[sec], evaluation: 0.0000[sec]
2024-05-19 11:43:26,367 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:43:26,597 - INFO - joeynmt.helpers - delete word_level_model/9500.ckpt
2024-05-19 11:43:26,607 - INFO - joeynmt.training - Example #0
2024-05-19 11:43:26,608 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:43:26,609 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:43:26,609 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'sc@@', 'or@@', 'so', 'che', 'di@@', 'mostr@@', 'ò', 'che', 'la', 'car@@', 't@@', 'ica', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'delle', 'tre', 'milioni', 'di', 'anni', 'è', 'stato', 'la', 'dimen@@', 'sione', 'del', '4@@', '8', '%', 'della', 'dimen@@', 'sione', 'del', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:43:26,610 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:43:26,610 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:43:26,610 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso scorso che dimostrò che la cartica artica, che per la maggior parte delle tre milioni di anni è stato la dimensione del 48% della dimensione del 40%.
2024-05-19 11:43:26,611 - INFO - joeynmt.training - Example #1
2024-05-19 11:43:26,611 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:43:26,611 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:43:26,611 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', 'sc@@', 'e', 'la', 'seri@@', 'amente', 'di', 'questa', 'parti@@', 'col@@', 'are', 'perché', 'non', 'lo', 'mostr@@', 'a', 'la', 'sp@@', 'ese', 'della', 'gh@@', 'i@@', 'acci@@', 'a', '.', '</s>']
2024-05-19 11:43:26,612 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:43:26,612 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:43:26,613 - INFO - joeynmt.training - 	Hypothesis: Ma questo capisce la seriamente di questa particolare perché non lo mostra la spese della ghiaccia.
2024-05-19 11:43:26,613 - INFO - joeynmt.training - Example #2
2024-05-19 11:43:26,613 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:43:26,614 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:43:26,614 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'c@@', 'li@@', 'enza', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'glob@@', 'ale', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:43:26,614 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:43:26,615 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:43:26,615 - INFO - joeynmt.training - 	Hypothesis: La clienza artica è, in un certo senso, il cuore del sistema globale globale.
2024-05-19 11:43:26,615 - INFO - joeynmt.training - Example #3
2024-05-19 11:43:26,615 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:43:26,616 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:43:26,616 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'em@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'ità', 'e', 'contr@@', 'atto', 'in', 'sost@@', 'en@@', 'i@@', 'ene', '.', '</s>']
2024-05-19 11:43:26,616 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:43:26,617 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:43:26,617 - INFO - joeynmt.training - 	Hypothesis: Sempande in inverità e contratto in sosteniene.
2024-05-19 11:43:26,617 - INFO - joeynmt.training - Example #4
2024-05-19 11:43:26,617 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:43:26,618 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:43:26,618 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'pro@@', 'ssi@@', 'mo', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'ete', 'un', 'pi@@', 'd', 'pi@@', 'ano', 'di', 'di', 'cui', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:43:26,618 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:43:26,619 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:43:26,619 - INFO - joeynmt.training - 	Hypothesis: Il prossimo slide che vi mostrerete un pid piano di di cui è successo negli ultimi 25 anni.
2024-05-19 11:43:29,999 - INFO - joeynmt.training - Epoch   3, Step:    12100, Batch Loss:     1.480021, Batch Acc: 0.567250, Tokens per Sec:    20885, Lr: 0.000300
2024-05-19 11:43:33,347 - INFO - joeynmt.training - Epoch   3, Step:    12200, Batch Loss:     1.484295, Batch Acc: 0.561938, Tokens per Sec:    22830, Lr: 0.000300
2024-05-19 11:43:37,548 - INFO - joeynmt.training - Epoch   3, Step:    12300, Batch Loss:     1.480975, Batch Acc: 0.560799, Tokens per Sec:    17228, Lr: 0.000300
2024-05-19 11:43:40,891 - INFO - joeynmt.training - Epoch   3, Step:    12400, Batch Loss:     1.602420, Batch Acc: 0.560440, Tokens per Sec:    22410, Lr: 0.000300
2024-05-19 11:43:44,210 - INFO - joeynmt.training - Epoch   3, Step:    12500, Batch Loss:     1.564451, Batch Acc: 0.564923, Tokens per Sec:    22823, Lr: 0.000300
2024-05-19 11:43:44,211 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:43:44,211 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:43:56,413 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.57, ppl:   4.81, acc:   0.55, generation: 12.0955[sec], evaluation: 0.0000[sec]
2024-05-19 11:43:56,414 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:43:56,632 - INFO - joeynmt.helpers - delete word_level_model/10000.ckpt
2024-05-19 11:43:56,641 - INFO - joeynmt.training - Example #0
2024-05-19 11:43:56,642 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:43:56,642 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:43:56,642 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'ho', 'mostr@@', 'ato', 'queste', 'due', 's@@', 'a@@', 'positi@@', 'vi', 'e', 'così', 'che', 'la', 'de@@', 'sol@@', 'a', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'delle', 'dimen@@', 'sioni', 'dell@@', '&@@', 'a@@', 'pos@@', ';', 'au@@', 'to', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'delle', 'dimen@@', 'sioni', 'del', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:43:56,643 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:43:56,644 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:43:56,644 - INFO - joeynmt.training - 	Hypothesis: L'anno ho mostrato queste due sapositivi e così che la desola, che per la maggior parte delle dimensioni dell'auto, che per la maggior parte delle dimensioni del 40%.
2024-05-19 11:43:56,644 - INFO - joeynmt.training - Example #1
2024-05-19 11:43:56,644 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:43:56,645 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:43:56,645 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', 'sc@@', 'e', 'la', 'seri@@', 'amente', ',', 'perché', 'non', 'lo', 'mostr@@', 'a', 'la', 'seri@@', 'amente', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:43:56,645 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:43:56,646 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:43:56,646 - INFO - joeynmt.training - 	Hypothesis: Ma questo capisce la seriamente, perché non lo mostra la seriamente del ghiaccio.
2024-05-19 11:43:56,646 - INFO - joeynmt.training - Example #2
2024-05-19 11:43:56,646 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:43:56,647 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:43:56,647 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'c@@', 'att@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:43:56,647 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:43:56,648 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:43:56,648 - INFO - joeynmt.training - 	Hypothesis: La cattica è, in un certo senso, il cuore cuore del sistema climatico globale.
2024-05-19 11:43:56,648 - INFO - joeynmt.training - Example #3
2024-05-19 11:43:56,648 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:43:56,649 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:43:56,649 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'em@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'at@@', 'ti', 'in', 'cer@@', 'to', '.', '</s>']
2024-05-19 11:43:56,649 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:43:56,650 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:43:56,650 - INFO - joeynmt.training - 	Hypothesis: Sempande in inverno e contratti in certo.
2024-05-19 11:43:56,650 - INFO - joeynmt.training - Example #4
2024-05-19 11:43:56,650 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:43:56,651 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:43:56,651 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'pro@@', 'ssi@@', 'mo', 's@@', 'li@@', 'de', 'che', 'vi', 'sar@@', 'à', 'un', 'pi@@', 'd', 'più', 'velo@@', 'c@@', 'emente', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:43:56,651 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:43:56,652 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:43:56,652 - INFO - joeynmt.training - 	Hypothesis: Il prossimo slide che vi sarà un pid più velocemente di ciò che è successo negli ultimi 25 anni.
2024-05-19 11:44:00,035 - INFO - joeynmt.training - Epoch   3, Step:    12600, Batch Loss:     1.576512, Batch Acc: 0.561260, Tokens per Sec:    20423, Lr: 0.000300
2024-05-19 11:44:03,883 - INFO - joeynmt.training - Epoch   3, Step:    12700, Batch Loss:     1.456262, Batch Acc: 0.560982, Tokens per Sec:    19026, Lr: 0.000300
2024-05-19 11:44:07,922 - INFO - joeynmt.training - Epoch   3, Step:    12800, Batch Loss:     1.524206, Batch Acc: 0.563454, Tokens per Sec:    18471, Lr: 0.000300
2024-05-19 11:44:11,151 - INFO - joeynmt.training - Epoch   3, Step:    12900, Batch Loss:     1.410537, Batch Acc: 0.560722, Tokens per Sec:    23143, Lr: 0.000300
2024-05-19 11:44:14,461 - INFO - joeynmt.training - Epoch   3, Step:    13000, Batch Loss:     1.644896, Batch Acc: 0.565644, Tokens per Sec:    22696, Lr: 0.000300
2024-05-19 11:44:14,462 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:44:14,462 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:44:27,015 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.56, ppl:   4.76, acc:   0.55, generation: 12.4474[sec], evaluation: 0.0000[sec]
2024-05-19 11:44:27,016 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:44:27,253 - INFO - joeynmt.helpers - delete word_level_model/10500.ckpt
2024-05-19 11:44:27,264 - INFO - joeynmt.training - Example #0
2024-05-19 11:44:27,264 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:44:27,265 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:44:27,265 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'queste', 'due', 's@@', 'li@@', 'de', 'così', 'che', 'la', 'co@@', 'p@@', 'pi@@', 'a', 'che', 'la', 'b@@', 'is@@', 'si@@', 'ma', 'di', 'c@@', 'ic@@', 'ina', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'delle', 'dimen@@', 'sioni', 'del', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:44:27,266 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:44:27,266 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:44:27,267 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso queste due slide così che la coppia che la bissima di cicina, che per la maggior parte delle dimensioni del 40%.
2024-05-19 11:44:27,267 - INFO - joeynmt.training - Example #1
2024-05-19 11:44:27,268 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:44:27,268 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:44:27,268 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', 'sc@@', 'e', 'la', 'seri@@', 'amente', 'la', 'seri@@', 'amente', 'del', 'problema', 'di', 'questo', 'problema', 'perché', 'non', 'lo', 'mostr@@', 'a', 'la', 't@@', 'ent@@', 'a', 'del', 'gh@@', 'i@@', 'ere', '.', '</s>']
2024-05-19 11:44:27,269 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:44:27,269 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:44:27,269 - INFO - joeynmt.training - 	Hypothesis: Ma questo capisce la seriamente la seriamente del problema di questo problema perché non lo mostra la tenta del ghiere.
2024-05-19 11:44:27,269 - INFO - joeynmt.training - Example #2
2024-05-19 11:44:27,270 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:44:27,270 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:44:27,270 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'c@@', 'att@@', 'ina', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:44:27,271 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:44:27,271 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:44:27,272 - INFO - joeynmt.training - 	Hypothesis: La cattina è, in un certo senso, il cuore del sistema clima globale.
2024-05-19 11:44:27,272 - INFO - joeynmt.training - Example #3
2024-05-19 11:44:27,272 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:44:27,272 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:44:27,273 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'il', 'contr@@', 'atto', 'e', 'il', 'contr@@', 'atto', 'in', 'cer@@', 'to', '.', '</s>']
2024-05-19 11:44:27,273 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:44:27,274 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:44:27,274 - INFO - joeynmt.training - 	Hypothesis: E 'inverno e il contratto e il contratto in certo.
2024-05-19 11:44:27,274 - INFO - joeynmt.training - Example #4
2024-05-19 11:44:27,274 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:44:27,275 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:44:27,275 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'pro@@', 'ssi@@', 'mo', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'da', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:44:27,275 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:44:27,276 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:44:27,276 - INFO - joeynmt.training - 	Hypothesis: Il prossimo slide che vi mostrerà un rapida di ciò che è successo negli ultimi 25 anni.
2024-05-19 11:44:30,678 - INFO - joeynmt.training - Epoch   3, Step:    13100, Batch Loss:     1.304738, Batch Acc: 0.567392, Tokens per Sec:    20260, Lr: 0.000300
2024-05-19 11:44:35,001 - INFO - joeynmt.training - Epoch   3, Step:    13200, Batch Loss:     1.415121, Batch Acc: 0.569770, Tokens per Sec:    17275, Lr: 0.000300
2024-05-19 11:44:38,268 - INFO - joeynmt.training - Epoch   3, Step:    13300, Batch Loss:     1.336409, Batch Acc: 0.565695, Tokens per Sec:    23009, Lr: 0.000300
2024-05-19 11:44:41,513 - INFO - joeynmt.training - Epoch   3, Step:    13400, Batch Loss:     1.518798, Batch Acc: 0.568228, Tokens per Sec:    22177, Lr: 0.000300
2024-05-19 11:44:44,822 - INFO - joeynmt.training - Epoch   3, Step:    13500, Batch Loss:     1.453186, Batch Acc: 0.573579, Tokens per Sec:    22287, Lr: 0.000300
2024-05-19 11:44:44,822 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:44:44,823 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:44:57,468 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.55, ppl:   4.70, acc:   0.56, generation: 12.5223[sec], evaluation: 0.0000[sec]
2024-05-19 11:44:57,469 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:44:57,700 - INFO - joeynmt.helpers - delete word_level_model/11000.ckpt
2024-05-19 11:44:57,709 - INFO - joeynmt.training - Example #0
2024-05-19 11:44:57,710 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:44:57,710 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:44:57,711 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 's@@', 'li@@', 'de', 'così', 'che', 'la', 'de@@', 'mon@@', 'di@@', 'ale', 'di', 'ar@@', 'c@@', 'ic@@', 'ina', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'dei', 'tre', 'milioni', 'di', 'anni', 'è', 'stato', 'la', 'dimen@@', 'sione', 'dell@@', '&@@', 'a@@', 'pos@@', ';', 'es@@', 'per@@', 'to', 'di', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:44:57,711 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:44:57,712 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:44:57,712 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due slide così che la demondiale di arcicina, che per la maggior parte dei tre milioni di anni è stato la dimensione dell'esperto di 40%.
2024-05-19 11:44:57,712 - INFO - joeynmt.training - Example #1
2024-05-19 11:44:57,713 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:44:57,713 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:44:57,713 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'com@@', 'pren@@', 'de', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'perché', 'non', 'fa', 'mostr@@', 'a', 'la', 'seri@@', 'amente', 'del', 'gh@@', 'i@@', 'acci@@', 'o', 'perché', 'non', 'mostr@@', 'a', 'la', 'ter@@', 'ra', '.', '</s>']
2024-05-19 11:44:57,714 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:44:57,714 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:44:57,714 - INFO - joeynmt.training - 	Hypothesis: Ma questo comprende la seriamente di questo particolare perché non fa mostra la seriamente del ghiaccio perché non mostra la terra.
2024-05-19 11:44:57,714 - INFO - joeynmt.training - Example #2
2024-05-19 11:44:57,715 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:44:57,715 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:44:57,715 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'c@@', 'att@@', 'a', 'di', 'ar@@', 't@@', 'ti@@', 'che', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:44:57,716 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:44:57,716 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:44:57,716 - INFO - joeynmt.training - 	Hypothesis: La catta di arttiche è, in un certo senso, il cuore del sistema climatico globale.
2024-05-19 11:44:57,716 - INFO - joeynmt.training - Example #3
2024-05-19 11:44:57,717 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:44:57,717 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:44:57,717 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'atto', 'in', 'cer@@', 't@@', 'ell@@', 'a', '.', '</s>']
2024-05-19 11:44:57,718 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:44:57,718 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:44:57,718 - INFO - joeynmt.training - 	Hypothesis: Spande in inverno e contratto in certella.
2024-05-19 11:44:57,719 - INFO - joeynmt.training - Example #4
2024-05-19 11:44:57,719 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:44:57,719 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:44:57,719 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'pro@@', 'ssi@@', 'mo', 's@@', 'li@@', 'de', 'che', 'sar@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'da', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:44:57,720 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:44:57,720 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:44:57,721 - INFO - joeynmt.training - 	Hypothesis: Il prossimo slide che sarà un rapido rapida quello che è successo negli ultimi 25 anni.
2024-05-19 11:45:01,718 - INFO - joeynmt.training - Epoch   3, Step:    13600, Batch Loss:     1.374257, Batch Acc: 0.564380, Tokens per Sec:    17281, Lr: 0.000300
2024-05-19 11:45:05,498 - INFO - joeynmt.training - Epoch   3, Step:    13700, Batch Loss:     1.594467, Batch Acc: 0.568229, Tokens per Sec:    18759, Lr: 0.000300
2024-05-19 11:45:08,816 - INFO - joeynmt.training - Epoch   3, Step:    13800, Batch Loss:     1.686361, Batch Acc: 0.568790, Tokens per Sec:    22538, Lr: 0.000300
2024-05-19 11:45:12,097 - INFO - joeynmt.training - Epoch   3, Step:    13900, Batch Loss:     1.726925, Batch Acc: 0.568802, Tokens per Sec:    22759, Lr: 0.000300
2024-05-19 11:45:16,199 - INFO - joeynmt.training - Epoch   3, Step:    14000, Batch Loss:     1.477004, Batch Acc: 0.572612, Tokens per Sec:    18169, Lr: 0.000300
2024-05-19 11:45:16,200 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:45:16,200 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:45:28,077 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.53, ppl:   4.64, acc:   0.56, generation: 11.6505[sec], evaluation: 0.0000[sec]
2024-05-19 11:45:28,078 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:45:28,350 - INFO - joeynmt.helpers - delete word_level_model/11500.ckpt
2024-05-19 11:45:28,362 - INFO - joeynmt.training - Example #0
2024-05-19 11:45:28,363 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:45:28,363 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:45:28,364 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'o', 'anno', 'sc@@', 'or@@', 'so', 'anno', 'queste', 'due', 's@@', 'li@@', 'de', 'così', 'che', 'di@@', 'mostr@@', 'ano', 'che', 'la', 'mor@@', 'te', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'delle', 'par@@', 'ole', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'delle', 'dimen@@', 'sioni', 'del', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:45:28,365 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:45:28,365 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:45:28,365 - INFO - joeynmt.training - 	Hypothesis: Lo anno scorso anno queste due slide così che dimostrano che la morte, che per la maggior parte delle parole, che per la maggior parte delle dimensioni del 40%.
2024-05-19 11:45:28,365 - INFO - joeynmt.training - Example #1
2024-05-19 11:45:28,366 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:45:28,366 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:45:28,366 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', 'sc@@', 'e', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'perché', 'non', 'è', 'la', 'mostr@@', 'a', 'la', 'ter@@', 'ra', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:45:28,367 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:45:28,367 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:45:28,367 - INFO - joeynmt.training - 	Hypothesis: Ma questo capisce la seriamente di questo particolare perché non è la mostra la terra del ghiaccio.
2024-05-19 11:45:28,367 - INFO - joeynmt.training - Example #2
2024-05-19 11:45:28,368 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:45:28,368 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:45:28,368 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'c@@', 'acci@@', 'a', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:45:28,369 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:45:28,369 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:45:28,369 - INFO - joeynmt.training - 	Hypothesis: La caccia artica è, in un senso, il cuore del sistema clima globale.
2024-05-19 11:45:28,370 - INFO - joeynmt.training - Example #3
2024-05-19 11:45:28,370 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:45:28,370 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:45:28,370 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'em@@', 'br@@', 'a', 'in', 'int@@', 'en@@', 'zione', 'e', 'contr@@', 'at@@', 'ti', 'in', 'cer@@', 'ca', '.', '</s>']
2024-05-19 11:45:28,371 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:45:28,371 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:45:28,371 - INFO - joeynmt.training - 	Hypothesis: Sembra in intenzione e contratti in cerca.
2024-05-19 11:45:28,372 - INFO - joeynmt.training - Example #4
2024-05-19 11:45:28,372 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:45:28,372 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:45:28,372 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'do', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:45:28,373 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:45:28,373 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:45:28,374 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerà un rapido rapido di quello che è successo negli ultimi 25 anni.
2024-05-19 11:45:32,356 - INFO - joeynmt.training - Epoch   3, Step:    14100, Batch Loss:     1.439216, Batch Acc: 0.570476, Tokens per Sec:    17030, Lr: 0.000300
2024-05-19 11:45:35,670 - INFO - joeynmt.training - Epoch   3, Step:    14200, Batch Loss:     1.603099, Batch Acc: 0.571671, Tokens per Sec:    23102, Lr: 0.000300
2024-05-19 11:45:38,966 - INFO - joeynmt.training - Epoch   3, Step:    14300, Batch Loss:     1.451879, Batch Acc: 0.571032, Tokens per Sec:    22835, Lr: 0.000300
2024-05-19 11:45:41,719 - INFO - joeynmt.training - Epoch   3: total training loss 7326.66
2024-05-19 11:45:41,719 - INFO - joeynmt.training - EPOCH 4
2024-05-19 11:45:42,580 - INFO - joeynmt.training - Epoch   4, Step:    14400, Batch Loss:     1.460519, Batch Acc: 0.592296, Tokens per Sec:    17424, Lr: 0.000300
2024-05-19 11:45:46,583 - INFO - joeynmt.training - Epoch   4, Step:    14500, Batch Loss:     1.528215, Batch Acc: 0.587122, Tokens per Sec:    19056, Lr: 0.000300
2024-05-19 11:45:46,584 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:45:46,584 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:45:58,516 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.52, ppl:   4.58, acc:   0.56, generation: 11.7211[sec], evaluation: 0.0000[sec]
2024-05-19 11:45:58,517 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:45:58,804 - INFO - joeynmt.helpers - delete word_level_model/12000.ckpt
2024-05-19 11:45:58,817 - INFO - joeynmt.training - Example #0
2024-05-19 11:45:58,818 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:45:58,818 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:45:58,818 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'questi', 'due', 's@@', 'a@@', 'positi@@', 'vi', 'così', 'che', 'la', 'de@@', 'mon@@', 'di@@', 'ale', ',', 'che', 'la', 'vol@@', 'te', 'più', 'grande', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'delle', 'dimen@@', 'sioni', 'del', '4@@', '8', 'per', 'c@@', 'ento', ',', 'ha', 's@@', 'divi@@', 'so', 'da', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:45:58,819 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:45:58,819 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:45:58,820 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due sapositivi così che la demondiale, che la volte più grande, che per la maggior parte delle dimensioni del 48 per cento, ha sdiviso da 40%.
2024-05-19 11:45:58,820 - INFO - joeynmt.training - Example #1
2024-05-19 11:45:58,820 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:45:58,820 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:45:58,820 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'ser@@', 'vi@@', 'zi@@', 'o', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'la', 'più', 'gh@@', 'i@@', 'acci@@', 'a', '.', '</s>']
2024-05-19 11:45:58,821 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:45:58,821 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:45:58,822 - INFO - joeynmt.training - 	Hypothesis: Ma questo servizio di questo particolare problema di questo particolare problema perché non mostra la più ghiaccia.
2024-05-19 11:45:58,822 - INFO - joeynmt.training - Example #2
2024-05-19 11:45:58,822 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:45:58,822 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:45:58,823 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'c@@', 'li@@', 'mat@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'nel', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:45:58,823 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:45:58,823 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:45:58,824 - INFO - joeynmt.training - 	Hypothesis: La climatica è, in un certo senso, nel cuore del sistema clima globale.
2024-05-19 11:45:58,824 - INFO - joeynmt.training - Example #3
2024-05-19 11:45:58,824 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:45:58,824 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:45:58,825 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'atto', 'in', 'in@@', 'ver@@', 'no', '.', '</s>']
2024-05-19 11:45:58,825 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:45:58,826 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:45:58,826 - INFO - joeynmt.training - 	Hypothesis: Spande in inverno e contratto in inverno.
2024-05-19 11:45:58,826 - INFO - joeynmt.training - Example #4
2024-05-19 11:45:58,826 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:45:58,826 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:45:58,827 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'succ@@', 'essi@@', 'vo', 'mostr@@', 'ar@@', 'vi', 'un', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'd@@', 'amente', 'più', 'velo@@', 'c@@', 'emente', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:45:58,827 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:45:58,827 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:45:58,827 - INFO - joeynmt.training - 	Hypothesis: L'successivo mostrarvi un rapido rapidamente più velocemente negli ultimi 25 anni.
2024-05-19 11:46:02,347 - INFO - joeynmt.training - Epoch   4, Step:    14600, Batch Loss:     1.573851, Batch Acc: 0.586911, Tokens per Sec:    19558, Lr: 0.000300
2024-05-19 11:46:05,820 - INFO - joeynmt.training - Epoch   4, Step:    14700, Batch Loss:     1.431964, Batch Acc: 0.588950, Tokens per Sec:    21828, Lr: 0.000300
2024-05-19 11:46:09,136 - INFO - joeynmt.training - Epoch   4, Step:    14800, Batch Loss:     1.524557, Batch Acc: 0.585314, Tokens per Sec:    22811, Lr: 0.000300
2024-05-19 11:46:13,342 - INFO - joeynmt.training - Epoch   4, Step:    14900, Batch Loss:     1.326374, Batch Acc: 0.584004, Tokens per Sec:    17882, Lr: 0.000300
2024-05-19 11:46:16,746 - INFO - joeynmt.training - Epoch   4, Step:    15000, Batch Loss:     1.372196, Batch Acc: 0.586657, Tokens per Sec:    21188, Lr: 0.000300
2024-05-19 11:46:16,747 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:46:16,747 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:46:29,913 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.52, ppl:   4.59, acc:   0.57, generation: 13.0583[sec], evaluation: 0.0000[sec]
2024-05-19 11:46:30,142 - INFO - joeynmt.helpers - delete word_level_model/12500.ckpt
2024-05-19 11:46:30,153 - INFO - joeynmt.training - Example #0
2024-05-19 11:46:30,154 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:46:30,154 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:46:30,155 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 's@@', 'li@@', 'de', 'così', 'che', 'la', 'macch@@', 'ina', 'ar@@', 'c@@', 'ica', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'dei', 'tre', 'milioni', 'di', 'anni', 'è', 'stato', 'l@@', '&@@', 'a@@', 'pos@@', ';', 'ulti@@', 'mo', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:46:30,156 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:46:30,156 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:46:30,156 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due slide così che la macchina arcica, che per la maggior parte dei tre milioni di anni è stato l'ultimo 40%.
2024-05-19 11:46:30,157 - INFO - joeynmt.training - Example #1
2024-05-19 11:46:30,157 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:46:30,157 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:46:30,157 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', 'sc@@', 'e', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'lo', 'mostr@@', 'a', 'la', 'velo@@', 'c@@', 'ità', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:46:30,158 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:46:30,158 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:46:30,158 - INFO - joeynmt.training - 	Hypothesis: Ma questo capisce la seriamente di questo particolare problema perché non lo mostra la velocità del ghiaccio.
2024-05-19 11:46:30,159 - INFO - joeynmt.training - Example #2
2024-05-19 11:46:30,159 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:46:30,159 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:46:30,159 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'sistema', 't@@', 'ico', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'in', 'un', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:46:30,160 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:46:30,160 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:46:30,160 - INFO - joeynmt.training - 	Hypothesis: Il sistema tico è, in un certo senso, in un senso, il cuore del sistema clima globale.
2024-05-19 11:46:30,161 - INFO - joeynmt.training - Example #3
2024-05-19 11:46:30,161 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:46:30,161 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:46:30,161 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 11:46:30,162 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:46:30,162 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:46:30,162 - INFO - joeynmt.training - 	Hypothesis: E 'inverno e contratti in estate.
2024-05-19 11:46:30,163 - INFO - joeynmt.training - Example #4
2024-05-19 11:46:30,163 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:46:30,163 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:46:30,163 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'pro@@', 'ssi@@', 'mo', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'ete', 'un', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'd@@', 'amente', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:46:30,164 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:46:30,164 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:46:30,164 - INFO - joeynmt.training - 	Hypothesis: Il prossimo slide che vi mostrerete un rapido rapido rapidamente negli ultimi 25 anni.
2024-05-19 11:46:33,476 - INFO - joeynmt.training - Epoch   4, Step:    15100, Batch Loss:     1.424836, Batch Acc: 0.587656, Tokens per Sec:    21683, Lr: 0.000300
2024-05-19 11:46:36,765 - INFO - joeynmt.training - Epoch   4, Step:    15200, Batch Loss:     1.355929, Batch Acc: 0.590299, Tokens per Sec:    22046, Lr: 0.000300
2024-05-19 11:46:40,697 - INFO - joeynmt.training - Epoch   4, Step:    15300, Batch Loss:     1.467078, Batch Acc: 0.591907, Tokens per Sec:    18758, Lr: 0.000300
2024-05-19 11:46:44,385 - INFO - joeynmt.training - Epoch   4, Step:    15400, Batch Loss:     1.475507, Batch Acc: 0.587790, Tokens per Sec:    20427, Lr: 0.000300
2024-05-19 11:46:47,633 - INFO - joeynmt.training - Epoch   4, Step:    15500, Batch Loss:     1.401405, Batch Acc: 0.585482, Tokens per Sec:    22934, Lr: 0.000300
2024-05-19 11:46:47,633 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:46:47,634 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:47:00,169 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.52, ppl:   4.56, acc:   0.57, generation: 12.4140[sec], evaluation: 0.0000[sec]
2024-05-19 11:47:00,170 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:47:00,402 - INFO - joeynmt.helpers - delete word_level_model/13000.ckpt
2024-05-19 11:47:00,411 - INFO - joeynmt.training - Example #0
2024-05-19 11:47:00,412 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:47:00,412 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:47:00,413 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'questi', 'due', 's@@', 'li@@', 'de', 'così', 'che', 'la', 'de@@', 'mon@@', 'di@@', 'ale', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'delle', 'tre', 'milioni', 'di', 'anni', 'è', 'stato', 'l@@', '&@@', 'a@@', 'pos@@', ';', 'ulti@@', 'mo', '4@@', '0', 'per', 'c@@', 'ento', 'del', 'm@@', 'ale', ',', 'ha', 'sc@@', 'rit@@', 'to', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'oro', ',', 'ha', 'sc@@', 'rit@@', 'to', 'dal', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:47:00,413 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:47:00,414 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:47:00,414 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due slide così che la demondiale, che per la maggior parte delle tre milioni di anni è stato l'ultimo 40 per cento del male, ha scritto d'oro, ha scritto dal 40%.
2024-05-19 11:47:00,414 - INFO - joeynmt.training - Example #1
2024-05-19 11:47:00,414 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:47:00,415 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:47:00,415 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', 'sc@@', 'e', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'che', 'non', 'lo', 'mostr@@', 'a', 'la', 'cosa', 'non', 'mostr@@', 'a', 'la', 'velo@@', 'c@@', 'ità', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:47:00,415 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:47:00,416 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:47:00,416 - INFO - joeynmt.training - 	Hypothesis: Ma questo capisce la seriamente di questo particolare problema che non lo mostra la cosa non mostra la velocità del ghiaccio.
2024-05-19 11:47:00,416 - INFO - joeynmt.training - Example #2
2024-05-19 11:47:00,416 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:47:00,417 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:47:00,417 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'macch@@', 'ina', 't@@', 'ale', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'di', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:47:00,417 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:47:00,418 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:47:00,418 - INFO - joeynmt.training - 	Hypothesis: La macchina tale è, in un certo senso, il cuore del sistema di climatico globale.
2024-05-19 11:47:00,418 - INFO - joeynmt.training - Example #3
2024-05-19 11:47:00,418 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:47:00,419 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:47:00,419 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'em@@', 'p@@', 'and@@', 'e', 'int@@', 'ere', 'il', 'contr@@', 'atto', 'e', 'contr@@', 'atto', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 11:47:00,419 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:47:00,420 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:47:00,420 - INFO - joeynmt.training - 	Hypothesis: Sempande intere il contratto e contratto in estate.
2024-05-19 11:47:00,420 - INFO - joeynmt.training - Example #4
2024-05-19 11:47:00,421 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:47:00,421 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:47:00,421 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'ulti@@', 'mo', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'da', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:47:00,421 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:47:00,422 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:47:00,422 - INFO - joeynmt.training - 	Hypothesis: L'ultimo slide che vi mostrerà un rapido rapida di ciò che è successo negli ultimi 25 anni.
2024-05-19 11:47:03,730 - INFO - joeynmt.training - Epoch   4, Step:    15600, Batch Loss:     1.426674, Batch Acc: 0.586543, Tokens per Sec:    21153, Lr: 0.000300
2024-05-19 11:47:07,450 - INFO - joeynmt.training - Epoch   4, Step:    15700, Batch Loss:     1.396683, Batch Acc: 0.588445, Tokens per Sec:    20179, Lr: 0.000300
2024-05-19 11:47:11,508 - INFO - joeynmt.training - Epoch   4, Step:    15800, Batch Loss:     1.473701, Batch Acc: 0.581575, Tokens per Sec:    18330, Lr: 0.000300
2024-05-19 11:47:14,838 - INFO - joeynmt.training - Epoch   4, Step:    15900, Batch Loss:     1.449345, Batch Acc: 0.590869, Tokens per Sec:    22384, Lr: 0.000300
2024-05-19 11:47:18,133 - INFO - joeynmt.training - Epoch   4, Step:    16000, Batch Loss:     1.394898, Batch Acc: 0.591434, Tokens per Sec:    22829, Lr: 0.000300
2024-05-19 11:47:18,134 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:47:18,134 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:47:31,127 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.51, acc:   0.57, generation: 12.8759[sec], evaluation: 0.0000[sec]
2024-05-19 11:47:31,128 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:47:31,350 - INFO - joeynmt.helpers - delete word_level_model/13500.ckpt
2024-05-19 11:47:31,359 - INFO - joeynmt.training - Example #0
2024-05-19 11:47:31,360 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:47:31,360 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:47:31,360 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'ho', 'mostr@@', 'ato', 'queste', 'due', 's@@', 'li@@', 'de', 'così', 'che', 'il', 'gh@@', 'i@@', 'acci@@', 'o', 'ar@@', 't@@', 'ico', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'delle', 'dimen@@', 'sioni', 'dell@@', '&@@', 'a@@', 'pos@@', ';', 'ulti@@', 'ma', 'tre', 'milioni', 'di', 'anni', 'è', 'stato', 'il', '4@@', '8', '%', 'è', 'stato', 'il', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:47:31,361 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:47:31,361 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:47:31,362 - INFO - joeynmt.training - 	Hypothesis: L'anno ho mostrato queste due slide così che il ghiaccio artico, che per la maggior parte delle dimensioni dell'ultima tre milioni di anni è stato il 48% è stato il 40%.
2024-05-19 11:47:31,362 - INFO - joeynmt.training - Example #1
2024-05-19 11:47:31,362 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:47:31,362 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:47:31,363 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'com@@', 'pren@@', 'de', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'lo', 'mostr@@', 'a', 'la', 'ter@@', 'ra', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:47:31,363 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:47:31,363 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:47:31,364 - INFO - joeynmt.training - 	Hypothesis: Ma questo comprende la seriamente di questo particolare problema perché non lo mostra la terra del ghiaccio.
2024-05-19 11:47:31,364 - INFO - joeynmt.training - Example #2
2024-05-19 11:47:31,364 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:47:31,364 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:47:31,365 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'c@@', 'att@@', 'ina', ',', 'la', 'b@@', 'atti@@', 'va', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:47:31,365 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:47:31,365 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:47:31,366 - INFO - joeynmt.training - 	Hypothesis: La cattina, la battiva è, in un certo senso, il cuore del sistema climatico globale.
2024-05-19 11:47:31,366 - INFO - joeynmt.training - Example #3
2024-05-19 11:47:31,366 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:47:31,366 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:47:31,366 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'il', 'cont@@', 'en@@', 'uto', 'in', 'est@@', 'i@@', 'era', '.', '</s>']
2024-05-19 11:47:31,367 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:47:31,367 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:47:31,367 - INFO - joeynmt.training - 	Hypothesis: E 'inverno e il contenuto in estiera.
2024-05-19 11:47:31,368 - INFO - joeynmt.training - Example #4
2024-05-19 11:47:31,368 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:47:31,368 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:47:31,368 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'pro@@', 'ssi@@', 'mo', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'pi@@', 'do', 'pi@@', 'ano', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:47:31,369 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:47:31,369 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:47:31,369 - INFO - joeynmt.training - 	Hypothesis: Il prossimo slide che vi mostrerà un pido piano di ciò che è successo negli ultimi 25 anni.
2024-05-19 11:47:34,665 - INFO - joeynmt.training - Epoch   4, Step:    16100, Batch Loss:     1.374928, Batch Acc: 0.588522, Tokens per Sec:    20996, Lr: 0.000300
2024-05-19 11:47:38,986 - INFO - joeynmt.training - Epoch   4, Step:    16200, Batch Loss:     1.336425, Batch Acc: 0.592912, Tokens per Sec:    17314, Lr: 0.000300
2024-05-19 11:47:42,281 - INFO - joeynmt.training - Epoch   4, Step:    16300, Batch Loss:     1.371086, Batch Acc: 0.590656, Tokens per Sec:    22632, Lr: 0.000300
2024-05-19 11:47:45,629 - INFO - joeynmt.training - Epoch   4, Step:    16400, Batch Loss:     1.313168, Batch Acc: 0.590213, Tokens per Sec:    22249, Lr: 0.000300
2024-05-19 11:47:48,968 - INFO - joeynmt.training - Epoch   4, Step:    16500, Batch Loss:     1.329599, Batch Acc: 0.590605, Tokens per Sec:    22918, Lr: 0.000300
2024-05-19 11:47:48,969 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:47:48,969 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:48:00,863 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.50, ppl:   4.46, acc:   0.57, generation: 11.7473[sec], evaluation: 0.0000[sec]
2024-05-19 11:48:00,864 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:48:01,113 - INFO - joeynmt.helpers - delete word_level_model/14000.ckpt
2024-05-19 11:48:01,123 - INFO - joeynmt.training - Example #0
2024-05-19 11:48:01,123 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:48:01,124 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:48:01,124 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 's@@', 'li@@', 'de', 'così', 'che', 'la', 'de@@', 'mon@@', 'di@@', 'ale', 'che', 'la', 'b@@', 'oc@@', 'ca@@', 'sione', 'ar@@', 'c@@', 'ica', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'delle', 'dimen@@', 'sioni', 'dell@@', '&@@', 'a@@', 'pos@@', ';', 'ent@@', 'r@@', 'ata', 'della', 'dimen@@', 'sione', 'del', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:48:01,125 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:48:01,125 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:48:01,125 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due slide così che la demondiale che la boccasione arcica, che per la maggior parte delle dimensioni dell'entrata della dimensione del 40%.
2024-05-19 11:48:01,126 - INFO - joeynmt.training - Example #1
2024-05-19 11:48:01,126 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:48:01,126 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:48:01,126 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'capi@@', 'sc@@', 'e', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'lo', 'mostr@@', 'a', 'la', 'ter@@', 'ra', 'della', 'gh@@', 'i@@', 'acci@@', 'a', '.', '</s>']
2024-05-19 11:48:01,127 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:48:01,127 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:48:01,127 - INFO - joeynmt.training - 	Hypothesis: Ma questa capisce la seriamente di questo particolare problema perché non lo mostra la terra della ghiaccia.
2024-05-19 11:48:01,128 - INFO - joeynmt.training - Example #2
2024-05-19 11:48:01,128 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:48:01,128 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:48:01,128 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'c@@', 'att@@', 'ica', 'è', 'l@@', '&@@', 'a@@', 'pos@@', ';', 'au@@', 'mento', ',', 'in', 'un', 'cer@@', 'to', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:48:01,129 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:48:01,129 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:48:01,129 - INFO - joeynmt.training - 	Hypothesis: La cattica è l'aumento, in un certo, il cuore del sistema climatico globale.
2024-05-19 11:48:01,130 - INFO - joeynmt.training - Example #3
2024-05-19 11:48:01,130 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:48:01,130 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:48:01,130 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'at@@', 'ti', 'in', 'in@@', 'ver@@', 'no', '.', '</s>']
2024-05-19 11:48:01,131 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:48:01,131 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:48:01,131 - INFO - joeynmt.training - 	Hypothesis: E 'inverno e contratti in inverno.
2024-05-19 11:48:01,131 - INFO - joeynmt.training - Example #4
2024-05-19 11:48:01,132 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:48:01,132 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:48:01,132 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'ò', 'un', 'pi@@', 'do', 'pi@@', 'do', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:48:01,133 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:48:01,133 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:48:01,133 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò un pido pido di ciò che è successo negli ultimi 25 anni.
2024-05-19 11:48:04,899 - INFO - joeynmt.training - Epoch   4, Step:    16600, Batch Loss:     1.297510, Batch Acc: 0.591204, Tokens per Sec:    18666, Lr: 0.000300
2024-05-19 11:48:08,882 - INFO - joeynmt.training - Epoch   4, Step:    16700, Batch Loss:     1.421148, Batch Acc: 0.587053, Tokens per Sec:    18663, Lr: 0.000300
2024-05-19 11:48:12,147 - INFO - joeynmt.training - Epoch   4, Step:    16800, Batch Loss:     1.385093, Batch Acc: 0.584912, Tokens per Sec:    23006, Lr: 0.000300
2024-05-19 11:48:15,403 - INFO - joeynmt.training - Epoch   4, Step:    16900, Batch Loss:     1.498001, Batch Acc: 0.587707, Tokens per Sec:    22966, Lr: 0.000300
2024-05-19 11:48:19,117 - INFO - joeynmt.training - Epoch   4, Step:    17000, Batch Loss:     1.334547, Batch Acc: 0.590508, Tokens per Sec:    20049, Lr: 0.000300
2024-05-19 11:48:19,118 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:48:19,118 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:48:31,147 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.49, ppl:   4.43, acc:   0.57, generation: 11.9217[sec], evaluation: 0.0000[sec]
2024-05-19 11:48:31,148 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:48:31,410 - INFO - joeynmt.helpers - delete word_level_model/15000.ckpt
2024-05-19 11:48:31,416 - INFO - joeynmt.training - Example #0
2024-05-19 11:48:31,417 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:48:31,418 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:48:31,418 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'questi', 'due', 'di@@', 'a@@', 'positi@@', 'vi', 'così', 'che', 'di@@', 'mostr@@', 'ano', 'che', 'la', 'macch@@', 'ina', 'ar@@', 'c@@', 'ina', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'delle', 'ulti@@', 'me', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'delle', '4@@', '8', 'anni', 'è', 'ri@@', 'ma@@', 'sto', 'per', 'c@@', 'ento', '.', '</s>']
2024-05-19 11:48:31,419 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:48:31,420 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:48:31,420 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso questi due diapositivi così che dimostrano che la macchina arcina, che per la maggior parte delle ultime tre milioni di anni è stata la dimensione delle 48 anni è rimasto per cento.
2024-05-19 11:48:31,420 - INFO - joeynmt.training - Example #1
2024-05-19 11:48:31,420 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:48:31,421 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:48:31,421 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'com@@', 'prend@@', 'ere', 'la', 'seri@@', 'amente', 'la', 'seri@@', 'amente', 'di', 'questo', 'problema', 'perché', 'non', 'ha', 'mostr@@', 'ato', 'la', 't@@', 'end@@', 'enza', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:48:31,421 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:48:31,422 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:48:31,422 - INFO - joeynmt.training - 	Hypothesis: Ma questo comprendere la seriamente la seriamente di questo problema perché non ha mostrato la tendenza del ghiaccio.
2024-05-19 11:48:31,422 - INFO - joeynmt.training - Example #2
2024-05-19 11:48:31,423 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:48:31,423 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:48:31,423 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'c@@', 'uc@@', 'ina', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', '.', '</s>']
2024-05-19 11:48:31,424 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:48:31,424 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:48:31,424 - INFO - joeynmt.training - 	Hypothesis: La cucina artica è, in un certo senso, il cuore del sistema climatico.
2024-05-19 11:48:31,424 - INFO - joeynmt.training - Example #3
2024-05-19 11:48:31,425 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:48:31,425 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:48:31,425 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'cont@@', 'ra@@', 'i', 'in', 'est@@', 'ate', 'e', 'contr@@', 'ar@@', 'si', '.', '</s>']
2024-05-19 11:48:31,426 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:48:31,426 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:48:31,426 - INFO - joeynmt.training - 	Hypothesis: Spande in inverno e contrai in estate e contrarsi.
2024-05-19 11:48:31,426 - INFO - joeynmt.training - Example #4
2024-05-19 11:48:31,427 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:48:31,427 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:48:31,427 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'ete', 'un', 'pi@@', 'do', 'pi@@', 'ano', 'di', 'pi@@', 'e@@', 'di', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:48:31,428 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:48:31,428 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:48:31,428 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerete un pido piano di piedi di quello che è successo negli ultimi 25 anni.
2024-05-19 11:48:35,802 - INFO - joeynmt.training - Epoch   4, Step:    17100, Batch Loss:     1.513277, Batch Acc: 0.591276, Tokens per Sec:    16245, Lr: 0.000300
2024-05-19 11:48:39,054 - INFO - joeynmt.training - Epoch   4, Step:    17200, Batch Loss:     1.371792, Batch Acc: 0.588324, Tokens per Sec:    23261, Lr: 0.000300
2024-05-19 11:48:42,311 - INFO - joeynmt.training - Epoch   4, Step:    17300, Batch Loss:     1.666846, Batch Acc: 0.589734, Tokens per Sec:    23532, Lr: 0.000300
2024-05-19 11:48:45,731 - INFO - joeynmt.training - Epoch   4, Step:    17400, Batch Loss:     1.462077, Batch Acc: 0.594242, Tokens per Sec:    22107, Lr: 0.000300
2024-05-19 11:48:49,903 - INFO - joeynmt.training - Epoch   4, Step:    17500, Batch Loss:     1.426295, Batch Acc: 0.589342, Tokens per Sec:    17860, Lr: 0.000300
2024-05-19 11:48:49,903 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:48:49,904 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:49:00,814 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.48, ppl:   4.39, acc:   0.58, generation: 10.7123[sec], evaluation: 0.0000[sec]
2024-05-19 11:49:00,815 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:49:01,141 - INFO - joeynmt.helpers - delete word_level_model/14500.ckpt
2024-05-19 11:49:01,155 - INFO - joeynmt.training - Example #0
2024-05-19 11:49:01,156 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:49:01,156 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:49:01,156 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', ',', 'ho', 'mostr@@', 'ato', 'questi', 'due', 's@@', 'li@@', 'de', 'così', 'che', 'la', 'de@@', 'mon@@', 'di@@', 'ale', 'che', 'la', 'sc@@', 'en@@', 'a', 'sc@@', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'dei', '4@@', '0', 'milioni', 'di', 'anni', 'è', 'stato', 'il', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:49:01,158 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:49:01,158 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:49:01,158 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso, ho mostrato questi due slide così che la demondiale che la scena scartica, che per la maggior parte dei 40 milioni di anni è stato il 40%.
2024-05-19 11:49:01,158 - INFO - joeynmt.training - Example #1
2024-05-19 11:49:01,159 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:49:01,159 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:49:01,159 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'com@@', 'pren@@', 'de', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'il', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:49:01,160 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:49:01,160 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:49:01,160 - INFO - joeynmt.training - 	Hypothesis: Ma questo comprende la seriamente di questo particolare problema perché non mostra il ghiaccio.
2024-05-19 11:49:01,160 - INFO - joeynmt.training - Example #2
2024-05-19 11:49:01,161 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:49:01,161 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:49:01,161 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'c@@', 'att@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:49:01,162 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:49:01,162 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:49:01,162 - INFO - joeynmt.training - 	Hypothesis: La cattica è, in un certo senso, in un certo senso, il cuore del sistema climatico globale.
2024-05-19 11:49:01,162 - INFO - joeynmt.training - Example #3
2024-05-19 11:49:01,162 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:49:01,163 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:49:01,163 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'ar@@', 'g@@', 'om@@', 'enti', 'in', 'int@@', 'ere', 'e', 'contr@@', 'atto', 'in', 'in@@', 'ver@@', 'no', '.', '</s>']
2024-05-19 11:49:01,163 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:49:01,164 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:49:01,164 - INFO - joeynmt.training - 	Hypothesis: Sargomenti in intere e contratto in inverno.
2024-05-19 11:49:01,164 - INFO - joeynmt.training - Example #4
2024-05-19 11:49:01,164 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:49:01,164 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:49:01,165 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'pro@@', 'ssi@@', 'mo', 's@@', 'li@@', 'de', 'che', 'sar@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'da', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:49:01,165 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:49:01,165 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:49:01,166 - INFO - joeynmt.training - 	Hypothesis: Il prossimo slide che sarà un rapido rapida ciò che è successo negli ultimi 25 anni.
2024-05-19 11:49:05,085 - INFO - joeynmt.training - Epoch   4, Step:    17600, Batch Loss:     1.304115, Batch Acc: 0.592402, Tokens per Sec:    17129, Lr: 0.000300
2024-05-19 11:49:08,612 - INFO - joeynmt.training - Epoch   4, Step:    17700, Batch Loss:     1.397440, Batch Acc: 0.588227, Tokens per Sec:    21656, Lr: 0.000300
2024-05-19 11:49:11,963 - INFO - joeynmt.training - Epoch   4, Step:    17800, Batch Loss:     1.300098, Batch Acc: 0.594010, Tokens per Sec:    22565, Lr: 0.000300
2024-05-19 11:49:15,696 - INFO - joeynmt.training - Epoch   4, Step:    17900, Batch Loss:     1.167171, Batch Acc: 0.587019, Tokens per Sec:    19266, Lr: 0.000300
2024-05-19 11:49:19,470 - INFO - joeynmt.training - Epoch   4, Step:    18000, Batch Loss:     1.388014, Batch Acc: 0.584441, Tokens per Sec:    19473, Lr: 0.000300
2024-05-19 11:49:19,471 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:49:19,471 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:49:31,129 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.47, ppl:   4.37, acc:   0.58, generation: 11.4669[sec], evaluation: 0.0000[sec]
2024-05-19 11:49:31,130 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:49:31,442 - INFO - joeynmt.helpers - delete word_level_model/15500.ckpt
2024-05-19 11:49:31,456 - INFO - joeynmt.training - Example #0
2024-05-19 11:49:31,457 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:49:31,457 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:49:31,458 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'questi', 'due', 'di@@', 'a@@', 'positi@@', 'vi', 'così', 'che', 'la', 'di@@', 'mostr@@', 'a', 'che', 'la', 'macch@@', 'ina', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'dei', '4@@', '8', 'milioni', 'di', 'anni', 'è', 'stato', 'la', 'dimen@@', 'sione', 'del', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:49:31,459 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:49:31,459 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:49:31,459 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due diapositivi così che la dimostra che la macchina artica, che per la maggior parte dei 48 milioni di anni è stato la dimensione del 40%.
2024-05-19 11:49:31,459 - INFO - joeynmt.training - Example #1
2024-05-19 11:49:31,460 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:49:31,460 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:49:31,460 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', 'sc@@', 'e', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'lo', 'mostr@@', 'a', 'la', 'ter@@', 'ra', '.', '</s>']
2024-05-19 11:49:31,461 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:49:31,461 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:49:31,462 - INFO - joeynmt.training - 	Hypothesis: Ma questo capisce la seriamente di questo particolare problema perché non lo mostra la terra.
2024-05-19 11:49:31,462 - INFO - joeynmt.training - Example #2
2024-05-19 11:49:31,462 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:49:31,462 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:49:31,462 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'c@@', 'att@@', 'ina', 'ar@@', 't@@', 'ico', 'è', ',', 'in', 'un', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:49:31,463 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:49:31,464 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:49:31,464 - INFO - joeynmt.training - 	Hypothesis: La cattina artico è, in un senso, il cuore del sistema clima globale.
2024-05-19 11:49:31,464 - INFO - joeynmt.training - Example #3
2024-05-19 11:49:31,464 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:49:31,465 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:49:31,465 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'em@@', 'br@@', 'a', 'in', 'int@@', 'ere', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'ate', 'e', 'contr@@', 'ar@@', 'si', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 11:49:31,465 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:49:31,466 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:49:31,466 - INFO - joeynmt.training - 	Hypothesis: Sembra in intere e contratti in estate e contrarsi in estate.
2024-05-19 11:49:31,466 - INFO - joeynmt.training - Example #4
2024-05-19 11:49:31,466 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:49:31,467 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:49:31,467 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'pro@@', 'ssi@@', 'mo', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'ete', 'un', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'da', 'di', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:49:31,468 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:49:31,468 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:49:31,468 - INFO - joeynmt.training - 	Hypothesis: Il prossimo slide che vi mostrerete un rapido rapida di che è successo negli ultimi 25 anni.
2024-05-19 11:49:34,891 - INFO - joeynmt.training - Epoch   4, Step:    18100, Batch Loss:     1.305998, Batch Acc: 0.597049, Tokens per Sec:    19986, Lr: 0.000300
2024-05-19 11:49:38,134 - INFO - joeynmt.training - Epoch   4, Step:    18200, Batch Loss:     1.382176, Batch Acc: 0.593791, Tokens per Sec:    22301, Lr: 0.000300
2024-05-19 11:49:41,426 - INFO - joeynmt.training - Epoch   4, Step:    18300, Batch Loss:     1.402468, Batch Acc: 0.592961, Tokens per Sec:    22698, Lr: 0.000300
2024-05-19 11:49:45,712 - INFO - joeynmt.training - Epoch   4, Step:    18400, Batch Loss:     1.414693, Batch Acc: 0.590156, Tokens per Sec:    17150, Lr: 0.000300
2024-05-19 11:49:49,067 - INFO - joeynmt.training - Epoch   4, Step:    18500, Batch Loss:     1.286066, Batch Acc: 0.596004, Tokens per Sec:    22604, Lr: 0.000300
2024-05-19 11:49:49,068 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:49:49,068 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:50:01,046 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.47, ppl:   4.34, acc:   0.58, generation: 11.8726[sec], evaluation: 0.0000[sec]
2024-05-19 11:50:01,047 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:50:01,279 - INFO - joeynmt.helpers - delete word_level_model/16000.ckpt
2024-05-19 11:50:01,289 - INFO - joeynmt.training - Example #0
2024-05-19 11:50:01,289 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:50:01,290 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:50:01,290 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'a@@', 'positi@@', 've', 'così', 'che', 'la', 'di@@', 'mostr@@', 'a', 'che', 'la', 'macch@@', 'ina', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'delle', 'dimen@@', 'sioni', 'dei', '4@@', '8', '%', ',', 'ha', 's@@', 'chi@@', 'zz@@', 'ato', 'da', '4@@', '0', '%', ',', 'ha', 's@@', 'chi@@', 'ato', 'da', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:50:01,291 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:50:01,291 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:50:01,292 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due diapositive così che la dimostra che la macchina artica, che per la maggior parte delle dimensioni dei 48%, ha schizzato da 40%, ha schiato da 40%.
2024-05-19 11:50:01,292 - INFO - joeynmt.training - Example #1
2024-05-19 11:50:01,292 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:50:01,293 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:50:01,293 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'com@@', 'pren@@', 'de', 'la', 'seri@@', 'amente', 'del', 'tutto', 'questo', 'problema', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'lo', 'mostr@@', 'a', 'la', 'più', 'velo@@', 'c@@', 'ità', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:50:01,293 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:50:01,294 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:50:01,294 - INFO - joeynmt.training - 	Hypothesis: Ma questo comprende la seriamente del tutto questo problema particolare problema perché non lo mostra la più velocità del ghiaccio.
2024-05-19 11:50:01,294 - INFO - joeynmt.training - Example #2
2024-05-19 11:50:01,294 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:50:01,295 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:50:01,295 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'c@@', 'uc@@', 'ci@@', 'a', 'sc@@', 'al@@', 'a', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:50:01,295 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:50:01,296 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:50:01,296 - INFO - joeynmt.training - 	Hypothesis: La cuccia scala è, in un certo senso, il cuore del sistema clima globale.
2024-05-19 11:50:01,296 - INFO - joeynmt.training - Example #3
2024-05-19 11:50:01,297 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:50:01,297 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:50:01,297 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'em@@', 'br@@', 'a', 'in', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'atto', 'in', 'in@@', 'ver@@', 'no', '.', '</s>']
2024-05-19 11:50:01,297 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:50:01,298 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:50:01,298 - INFO - joeynmt.training - 	Hypothesis: Sembra in inverno e contratto in inverno.
2024-05-19 11:50:01,298 - INFO - joeynmt.training - Example #4
2024-05-19 11:50:01,298 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:50:01,299 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:50:01,299 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'ete', 'un', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'da', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:50:01,299 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:50:01,300 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:50:01,300 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerete un rapido rapida di quello che è successo negli ultimi 25 anni.
2024-05-19 11:50:04,617 - INFO - joeynmt.training - Epoch   4, Step:    18600, Batch Loss:     1.498342, Batch Acc: 0.588655, Tokens per Sec:    20278, Lr: 0.000300
2024-05-19 11:50:08,040 - INFO - joeynmt.training - Epoch   4, Step:    18700, Batch Loss:     1.544042, Batch Acc: 0.597864, Tokens per Sec:    22791, Lr: 0.000300
2024-05-19 11:50:11,764 - INFO - joeynmt.training - Epoch   4, Step:    18800, Batch Loss:     1.579116, Batch Acc: 0.590419, Tokens per Sec:    20345, Lr: 0.000300
2024-05-19 11:50:15,643 - INFO - joeynmt.training - Epoch   4, Step:    18900, Batch Loss:     1.457594, Batch Acc: 0.595835, Tokens per Sec:    18746, Lr: 0.000300
2024-05-19 11:50:18,900 - INFO - joeynmt.training - Epoch   4, Step:    19000, Batch Loss:     1.282706, Batch Acc: 0.596936, Tokens per Sec:    21884, Lr: 0.000300
2024-05-19 11:50:18,901 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:50:18,901 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:50:31,447 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.47, ppl:   4.35, acc:   0.58, generation: 12.4391[sec], evaluation: 0.0000[sec]
2024-05-19 11:50:31,678 - INFO - joeynmt.helpers - delete word_level_model/16500.ckpt
2024-05-19 11:50:31,684 - INFO - joeynmt.training - Example #0
2024-05-19 11:50:31,684 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:50:31,685 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:50:31,685 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'questi', 'due', 's@@', 'li@@', 'de', 'così', 'che', 'la', 'di@@', 'mostr@@', 'a', 'che', 'la', 'c@@', 'att@@', 'ica', ',', 'per', 'la', 'maggi@@', 'or', 'parte', 'delle', 'dimen@@', 'sioni', 'dell@@', '&@@', 'a@@', 'pos@@', ';', 'ulti@@', 'mo', 'tre', 'milioni', 'di', 'anni', 'è', 'stato', 'la', 'dimen@@', 'sione', 'del', '4@@', '8', '%', '.', '</s>']
2024-05-19 11:50:31,686 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:50:31,686 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:50:31,686 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due slide così che la dimostra che la cattica, per la maggior parte delle dimensioni dell'ultimo tre milioni di anni è stato la dimensione del 48%.
2024-05-19 11:50:31,686 - INFO - joeynmt.training - Example #1
2024-05-19 11:50:31,687 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:50:31,687 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:50:31,687 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', 'sc@@', 'e', 'la', 'seri@@', 'amente', 'del', 'problema', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'fa', 'la', 'sp@@', 'ett@@', 'a', 'della', 'gh@@', 'i@@', 'era', '.', '</s>']
2024-05-19 11:50:31,688 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:50:31,688 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:50:31,688 - INFO - joeynmt.training - 	Hypothesis: Ma questo capisce la seriamente del problema di questo particolare problema perché non fa la spetta della ghiera.
2024-05-19 11:50:31,689 - INFO - joeynmt.training - Example #2
2024-05-19 11:50:31,689 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:50:31,689 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:50:31,690 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'c@@', 'att@@', 'ica', 'c@@', 'att@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:50:31,690 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:50:31,690 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:50:31,691 - INFO - joeynmt.training - 	Hypothesis: La cattica cattica è, in un certo senso, il cuore del sistema clima del sistema clima del sistema clima globale.
2024-05-19 11:50:31,691 - INFO - joeynmt.training - Example #3
2024-05-19 11:50:31,691 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:50:31,692 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:50:31,692 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'em@@', 'br@@', 'a', 'in', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'atto', 'in', 'est@@', 'ate', 'e', 'contr@@', 'at@@', 'ori', '.', '</s>']
2024-05-19 11:50:31,692 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:50:31,692 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:50:31,693 - INFO - joeynmt.training - 	Hypothesis: Sembra in inverno e contratto in estate e contratori.
2024-05-19 11:50:31,693 - INFO - joeynmt.training - Example #4
2024-05-19 11:50:31,693 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:50:31,693 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:50:31,694 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'pro@@', 'ssi@@', 'mo', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'do', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:50:31,694 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:50:31,694 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:50:31,695 - INFO - joeynmt.training - 	Hypothesis: Il prossimo slide che vi mostrerà un rapido rapido di quello che è successo negli ultimi 25 anni.
2024-05-19 11:50:34,951 - INFO - joeynmt.training - Epoch   4, Step:    19100, Batch Loss:     1.295290, Batch Acc: 0.596459, Tokens per Sec:    20977, Lr: 0.000300
2024-05-19 11:50:37,107 - INFO - joeynmt.training - Epoch   4: total training loss 6786.29
2024-05-19 11:50:37,115 - INFO - joeynmt.training - EPOCH 5
2024-05-19 11:50:38,246 - INFO - joeynmt.training - Epoch   5, Step:    19200, Batch Loss:     1.566133, Batch Acc: 0.609956, Tokens per Sec:    21063, Lr: 0.000300
2024-05-19 11:50:42,531 - INFO - joeynmt.training - Epoch   5, Step:    19300, Batch Loss:     1.043854, Batch Acc: 0.618506, Tokens per Sec:    17172, Lr: 0.000300
2024-05-19 11:50:45,795 - INFO - joeynmt.training - Epoch   5, Step:    19400, Batch Loss:     1.284194, Batch Acc: 0.614838, Tokens per Sec:    23167, Lr: 0.000300
2024-05-19 11:50:49,068 - INFO - joeynmt.training - Epoch   5, Step:    19500, Batch Loss:     1.127846, Batch Acc: 0.616148, Tokens per Sec:    22166, Lr: 0.000300
2024-05-19 11:50:49,069 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:50:49,069 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:51:02,499 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.46, ppl:   4.32, acc:   0.58, generation: 13.3189[sec], evaluation: 0.0000[sec]
2024-05-19 11:51:02,500 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:51:02,731 - INFO - joeynmt.helpers - delete word_level_model/17000.ckpt
2024-05-19 11:51:02,741 - INFO - joeynmt.training - Example #0
2024-05-19 11:51:02,742 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:51:02,743 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:51:02,743 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 's@@', 'li@@', 'de', 'così', 'che', 'la', 'c@@', 'uc@@', 'ina', 'sc@@', 'or@@', 'sa', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'delle', 'dimen@@', 'sioni', 'dell@@', '&@@', 'a@@', 'pos@@', ';', 'ar@@', 'c@@', 'ine', 'di', '4@@', '8', '%', ',', 'ha', 'con@@', 'divi@@', 'so', 'dal', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:51:02,744 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:51:02,744 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:51:02,744 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due slide così che la cucina scorsa, che per la maggior parte delle dimensioni dell'arcine di 48%, ha condiviso dal 40%.
2024-05-19 11:51:02,745 - INFO - joeynmt.training - Example #1
2024-05-19 11:51:02,745 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:51:02,745 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:51:02,745 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'la', 'seri@@', 'amente', 'di', 'questo', 'problema', 'parti@@', 'col@@', 'are', 'perché', 'non', 'ri@@', 'es@@', 'ce', 'a', 'mostr@@', 'are', 'la', 'ter@@', 'ra', 'della', 'gh@@', 'i@@', 'acci@@', 'a', '.', '</s>']
2024-05-19 11:51:02,746 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:51:02,746 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:51:02,747 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione la seriamente di questo problema particolare perché non riesce a mostrare la terra della ghiaccia.
2024-05-19 11:51:02,747 - INFO - joeynmt.training - Example #2
2024-05-19 11:51:02,747 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:51:02,747 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:51:02,748 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'c@@', 'acci@@', 'a', 'ar@@', 'c@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:51:02,748 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:51:02,748 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:51:02,749 - INFO - joeynmt.training - 	Hypothesis: La caccia arcica è, in un certo senso, il cuore del sistema climatico globale.
2024-05-19 11:51:02,749 - INFO - joeynmt.training - Example #3
2024-05-19 11:51:02,749 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:51:02,749 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:51:02,749 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'atto', 'in', 'est@@', 'ate', 'e', 'contr@@', 'atto', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 11:51:02,750 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:51:02,750 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:51:02,751 - INFO - joeynmt.training - 	Hypothesis: E 'inverno e contratto in estate e contratto in estate.
2024-05-19 11:51:02,751 - INFO - joeynmt.training - Example #4
2024-05-19 11:51:02,751 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:51:02,751 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:51:02,751 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'pro@@', 'ssi@@', 'mo', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'ete', 'un', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'do', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:51:02,752 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:51:02,752 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:51:02,752 - INFO - joeynmt.training - 	Hypothesis: Il prossimo slide che vi mostrerete un rapido rapido di quello che è successo negli ultimi 25 anni.
2024-05-19 11:51:06,107 - INFO - joeynmt.training - Epoch   5, Step:    19600, Batch Loss:     1.376573, Batch Acc: 0.612589, Tokens per Sec:    20291, Lr: 0.000300
2024-05-19 11:51:10,483 - INFO - joeynmt.training - Epoch   5, Step:    19700, Batch Loss:     1.094402, Batch Acc: 0.614301, Tokens per Sec:    17197, Lr: 0.000300
2024-05-19 11:51:13,925 - INFO - joeynmt.training - Epoch   5, Step:    19800, Batch Loss:     1.432571, Batch Acc: 0.608839, Tokens per Sec:    21441, Lr: 0.000300
2024-05-19 11:51:17,225 - INFO - joeynmt.training - Epoch   5, Step:    19900, Batch Loss:     1.428071, Batch Acc: 0.607983, Tokens per Sec:    22481, Lr: 0.000300
2024-05-19 11:51:20,483 - INFO - joeynmt.training - Epoch   5, Step:    20000, Batch Loss:     1.415870, Batch Acc: 0.609988, Tokens per Sec:    22444, Lr: 0.000300
2024-05-19 11:51:20,484 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:51:20,484 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:51:32,248 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.46, ppl:   4.29, acc:   0.58, generation: 11.6554[sec], evaluation: 0.0000[sec]
2024-05-19 11:51:32,249 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:51:32,473 - INFO - joeynmt.helpers - delete word_level_model/17500.ckpt
2024-05-19 11:51:32,478 - INFO - joeynmt.training - Example #0
2024-05-19 11:51:32,479 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:51:32,480 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:51:32,480 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'a@@', 'positi@@', 've', 'così', 'che', 'la', 'di@@', 'mostr@@', 'a', 'che', 'la', 'ca@@', 'p', 'gh@@', 'i@@', 'acci@@', 'a', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'dei', '4@@', '8', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'dei', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:51:32,481 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:51:32,481 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:51:32,481 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due diapositive così che la dimostra che la cap ghiaccia, che per la maggior parte dei 48 milioni di anni è stata la dimensione dei 40%.
2024-05-19 11:51:32,482 - INFO - joeynmt.training - Example #1
2024-05-19 11:51:32,482 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:51:32,482 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:51:32,482 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', 'sc@@', 'e', 'la', 'ser@@', 'io', 'la', 'ser@@', 'io', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'la', 'più', 'velo@@', 'c@@', 'ità', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:51:32,483 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:51:32,483 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:51:32,484 - INFO - joeynmt.training - 	Hypothesis: Ma questo capisce la serio la serio di questo particolare problema perché non mostra la più velocità del ghiaccio.
2024-05-19 11:51:32,484 - INFO - joeynmt.training - Example #2
2024-05-19 11:51:32,484 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:51:32,484 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:51:32,485 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'sc@@', 'e@@', 'gli@@', 'ett@@', 'ica', 'è', ',', 'in', 'un', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 's@@', 'b@@', 'o', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:51:32,485 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:51:32,486 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:51:32,486 - INFO - joeynmt.training - 	Hypothesis: La scegliettica è, in un senso, il cuore sbo del sistema clima globale.
2024-05-19 11:51:32,486 - INFO - joeynmt.training - Example #3
2024-05-19 11:51:32,487 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:51:32,487 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:51:32,487 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'ate', 'e', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 11:51:32,487 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:51:32,488 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:51:32,488 - INFO - joeynmt.training - 	Hypothesis: Si espande in inverno e contratti in estate e in estate.
2024-05-19 11:51:32,488 - INFO - joeynmt.training - Example #4
2024-05-19 11:51:32,489 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:51:32,489 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:51:32,489 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'd@@', 'amente', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:51:32,490 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:51:32,490 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:51:32,490 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerà un rapido rapidamente di quello che è successo negli ultimi 25 anni.
2024-05-19 11:51:35,955 - INFO - joeynmt.training - Epoch   5, Step:    20100, Batch Loss:     1.216876, Batch Acc: 0.612227, Tokens per Sec:    20398, Lr: 0.000300
2024-05-19 11:51:39,989 - INFO - joeynmt.training - Epoch   5, Step:    20200, Batch Loss:     1.474460, Batch Acc: 0.612040, Tokens per Sec:    18252, Lr: 0.000300
2024-05-19 11:51:43,300 - INFO - joeynmt.training - Epoch   5, Step:    20300, Batch Loss:     1.379687, Batch Acc: 0.604526, Tokens per Sec:    22271, Lr: 0.000300
2024-05-19 11:51:46,557 - INFO - joeynmt.training - Epoch   5, Step:    20400, Batch Loss:     1.188154, Batch Acc: 0.606894, Tokens per Sec:    22981, Lr: 0.000300
2024-05-19 11:51:50,160 - INFO - joeynmt.training - Epoch   5, Step:    20500, Batch Loss:     1.443218, Batch Acc: 0.604590, Tokens per Sec:    20371, Lr: 0.000300
2024-05-19 11:51:50,161 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:51:50,161 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:52:02,634 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.45, ppl:   4.27, acc:   0.58, generation: 12.3654[sec], evaluation: 0.0000[sec]
2024-05-19 11:52:02,635 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:52:03,033 - INFO - joeynmt.helpers - delete word_level_model/18000.ckpt
2024-05-19 11:52:03,047 - INFO - joeynmt.training - Example #0
2024-05-19 11:52:03,048 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:52:03,048 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:52:03,048 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'ho', 'mostr@@', 'ato', 'queste', 'due', 's@@', 'li@@', 'de', 'così', 'che', 'di@@', 'mostr@@', 'ano', 'che', 'la', 'di@@', 'mostr@@', 'a', 'che', 'la', 'ca@@', 'po', 'ar@@', 't@@', 'ica', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'delle', 'dimen@@', 'sioni', 'dei', '4@@', '8', '%', ',', 'ha', 's@@', 'qual@@', 'i', '4@@', '8', '%', '.', '</s>']
2024-05-19 11:52:03,049 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:52:03,050 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:52:03,050 - INFO - joeynmt.training - 	Hypothesis: L'anno ho mostrato queste due slide così che dimostrano che la dimostra che la capo artica artica, che per la maggior parte delle dimensioni dei 48%, ha squali 48%.
2024-05-19 11:52:03,051 - INFO - joeynmt.training - Example #1
2024-05-19 11:52:03,051 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:52:03,051 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:52:03,051 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', 'sc@@', 'e', 'la', 'seri@@', 'amente', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'la', 'più', 'po@@', 'll@@', 'a', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:52:03,052 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:52:03,052 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:52:03,052 - INFO - joeynmt.training - 	Hypothesis: Ma questo capisce la seriamente la seriamente di questo particolare problema perché non mostra la più polla del ghiaccio.
2024-05-19 11:52:03,053 - INFO - joeynmt.training - Example #2
2024-05-19 11:52:03,053 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:52:03,053 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:52:03,053 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'b@@', 'ello', 'ar@@', 't@@', 'ico', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'nel', 'sen@@', 'so', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:52:03,054 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:52:03,054 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:52:03,054 - INFO - joeynmt.training - 	Hypothesis: La bello artico è, in un certo senso, nel senso del sistema climatico globale.
2024-05-19 11:52:03,055 - INFO - joeynmt.training - Example #3
2024-05-19 11:52:03,055 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:52:03,055 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:52:03,055 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'em@@', 'br@@', 'a', 'in', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'ate', 'e', 'contr@@', 'at@@', 'ti', '.', '</s>']
2024-05-19 11:52:03,056 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:52:03,056 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:52:03,056 - INFO - joeynmt.training - 	Hypothesis: Sembra in inverno e contratti in estate e contratti.
2024-05-19 11:52:03,056 - INFO - joeynmt.training - Example #4
2024-05-19 11:52:03,057 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:52:03,057 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:52:03,057 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'velo@@', 'c@@', 'emente', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:52:03,058 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:52:03,058 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:52:03,058 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerà un rapido velocemente di ciò che è successo negli ultimi 25 anni.
2024-05-19 11:52:07,401 - INFO - joeynmt.training - Epoch   5, Step:    20600, Batch Loss:     1.543440, Batch Acc: 0.611221, Tokens per Sec:    15690, Lr: 0.000300
2024-05-19 11:52:10,706 - INFO - joeynmt.training - Epoch   5, Step:    20700, Batch Loss:     1.556216, Batch Acc: 0.603908, Tokens per Sec:    22556, Lr: 0.000300
2024-05-19 11:52:13,998 - INFO - joeynmt.training - Epoch   5, Step:    20800, Batch Loss:     1.377231, Batch Acc: 0.607047, Tokens per Sec:    22407, Lr: 0.000300
2024-05-19 11:52:17,397 - INFO - joeynmt.training - Epoch   5, Step:    20900, Batch Loss:     1.512449, Batch Acc: 0.601698, Tokens per Sec:    22047, Lr: 0.000300
2024-05-19 11:52:21,459 - INFO - joeynmt.training - Epoch   5, Step:    21000, Batch Loss:     1.678214, Batch Acc: 0.612230, Tokens per Sec:    18650, Lr: 0.000300
2024-05-19 11:52:21,459 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:52:21,460 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:52:32,751 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.44, ppl:   4.23, acc:   0.59, generation: 11.0914[sec], evaluation: 0.0000[sec]
2024-05-19 11:52:32,752 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:52:33,003 - INFO - joeynmt.helpers - delete word_level_model/19000.ckpt
2024-05-19 11:52:33,019 - INFO - joeynmt.training - Example #0
2024-05-19 11:52:33,020 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:52:33,020 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:52:33,020 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'mostr@@', 'ate', 'che', 'la', 'de@@', 'mon@@', 'di@@', 'ale', 'che', 'la', 'maggi@@', 'or', 'c@@', 'att@@', 'a', 'di', 'gh@@', 'i@@', 'acci@@', 'o', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'delle', 'dimen@@', 'sioni', 'delle', 'dimen@@', 'sioni', 'del', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:52:33,021 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:52:33,021 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:52:33,021 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due dimostrate che la demondiale che la maggior catta di ghiaccio, che per la maggior parte delle dimensioni delle dimensioni del 40%.
2024-05-19 11:52:33,022 - INFO - joeynmt.training - Example #1
2024-05-19 11:52:33,022 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:52:33,022 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:52:33,022 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', 'sc@@', 'e', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'lo', 'mostr@@', 'a', 'la', 'stra@@', 'da', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:52:33,023 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:52:33,024 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:52:33,024 - INFO - joeynmt.training - 	Hypothesis: Ma questo capisce la seriamente di questo particolare problema perché non lo mostra la strada del ghiaccio.
2024-05-19 11:52:33,024 - INFO - joeynmt.training - Example #2
2024-05-19 11:52:33,024 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:52:33,025 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:52:33,025 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'c@@', 'acci@@', 'a', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:52:33,025 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:52:33,026 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:52:33,026 - INFO - joeynmt.training - 	Hypothesis: La caccia artica è, in un certo senso, il cuore del sistema clima globale.
2024-05-19 11:52:33,026 - INFO - joeynmt.training - Example #3
2024-05-19 11:52:33,026 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:52:33,026 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:52:33,026 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'il', 'contr@@', 'atto', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 11:52:33,027 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:52:33,027 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:52:33,028 - INFO - joeynmt.training - 	Hypothesis: Si espande in inverno e il contratto in estate.
2024-05-19 11:52:33,028 - INFO - joeynmt.training - Example #4
2024-05-19 11:52:33,028 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:52:33,028 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:52:33,028 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'pi@@', 'do', 'velo@@', 'c@@', 'emente', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:52:33,029 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:52:33,030 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:52:33,030 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerà un pido velocemente di ciò che è successo negli ultimi 25 anni.
2024-05-19 11:52:36,826 - INFO - joeynmt.training - Epoch   5, Step:    21100, Batch Loss:     1.341359, Batch Acc: 0.608231, Tokens per Sec:    18071, Lr: 0.000300
2024-05-19 11:52:40,074 - INFO - joeynmt.training - Epoch   5, Step:    21200, Batch Loss:     1.447955, Batch Acc: 0.608629, Tokens per Sec:    23197, Lr: 0.000300
2024-05-19 11:52:43,335 - INFO - joeynmt.training - Epoch   5, Step:    21300, Batch Loss:     1.199741, Batch Acc: 0.612382, Tokens per Sec:    22539, Lr: 0.000300
2024-05-19 11:52:47,160 - INFO - joeynmt.training - Epoch   5, Step:    21400, Batch Loss:     1.292367, Batch Acc: 0.609012, Tokens per Sec:    19411, Lr: 0.000300
2024-05-19 11:52:50,967 - INFO - joeynmt.training - Epoch   5, Step:    21500, Batch Loss:     1.393117, Batch Acc: 0.614726, Tokens per Sec:    20091, Lr: 0.000300
2024-05-19 11:52:50,968 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:52:50,968 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:53:03,886 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.44, ppl:   4.24, acc:   0.58, generation: 12.8113[sec], evaluation: 0.0000[sec]
2024-05-19 11:53:04,103 - INFO - joeynmt.helpers - delete word_level_model/18500.ckpt
2024-05-19 11:53:04,113 - INFO - joeynmt.training - Example #0
2024-05-19 11:53:04,114 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:53:04,114 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:53:04,114 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'questi', 'due', 's@@', 'li@@', 'de', 'così', 'che', 'di@@', 'mostr@@', 'a', 'che', 'il', 'gh@@', 'i@@', 'acci@@', 'o', 'ar@@', 't@@', 'ico', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'dei', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'dei', '4@@', '8', '%', 'sono', 'st@@', 'ati', 'la', 'dimen@@', 'sione', 'dei', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:53:04,115 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:53:04,115 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:53:04,116 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso questi due slide così che dimostra che il ghiaccio artico, che per la maggior parte dei tre milioni di anni è stata la dimensione dei 48% sono stati la dimensione dei 40%.
2024-05-19 11:53:04,116 - INFO - joeynmt.training - Example #1
2024-05-19 11:53:04,116 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:53:04,116 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:53:04,117 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'si', 'com@@', 'pren@@', 'de', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'lo', 'mostr@@', 'a', 'la', 'sp@@', 'ett@@', 'a', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:53:04,117 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:53:04,118 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:53:04,118 - INFO - joeynmt.training - 	Hypothesis: Ma questo si comprende la seriamente di questo particolare problema perché non lo mostra la spetta del ghiaccio.
2024-05-19 11:53:04,118 - INFO - joeynmt.training - Example #2
2024-05-19 11:53:04,119 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:53:04,119 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:53:04,119 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'c@@', 'att@@', 'ina', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'sen@@', 'so', ',', 'nel', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:53:04,120 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:53:04,120 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:53:04,120 - INFO - joeynmt.training - 	Hypothesis: La cattina artica è, in un senso, nel senso, il cuore del sistema clima globale.
2024-05-19 11:53:04,120 - INFO - joeynmt.training - Example #3
2024-05-19 11:53:04,121 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:53:04,121 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:53:04,121 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&@@', 'a@@', 'pos@@', ';', 'in', 'in@@', 'ver@@', 'no', 'e', 'il', 'contr@@', 'atto', 'e', 'il', 'contr@@', 'atto', '.', '</s>']
2024-05-19 11:53:04,121 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:53:04,122 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:53:04,122 - INFO - joeynmt.training - 	Hypothesis: E 'in inverno e il contratto e il contratto.
2024-05-19 11:53:04,122 - INFO - joeynmt.training - Example #4
2024-05-19 11:53:04,123 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:53:04,123 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:53:04,123 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'pro@@', 'ssi@@', 'mo', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'ete', 'un', 'ra@@', 'pi@@', 'do', 'più', 'velo@@', 'c@@', 'ità', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:53:04,124 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:53:04,124 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:53:04,124 - INFO - joeynmt.training - 	Hypothesis: Il prossimo slide che vi mostrerete un rapido più velocità di ciò che è successo negli ultimi 25 anni.
2024-05-19 11:53:07,517 - INFO - joeynmt.training - Epoch   5, Step:    21600, Batch Loss:     1.371006, Batch Acc: 0.613218, Tokens per Sec:    20918, Lr: 0.000300
2024-05-19 11:53:10,798 - INFO - joeynmt.training - Epoch   5, Step:    21700, Batch Loss:     1.442662, Batch Acc: 0.603604, Tokens per Sec:    22275, Lr: 0.000300
2024-05-19 11:53:14,386 - INFO - joeynmt.training - Epoch   5, Step:    21800, Batch Loss:     1.384681, Batch Acc: 0.607622, Tokens per Sec:    20601, Lr: 0.000300
2024-05-19 11:53:18,466 - INFO - joeynmt.training - Epoch   5, Step:    21900, Batch Loss:     1.162459, Batch Acc: 0.611109, Tokens per Sec:    18657, Lr: 0.000300
2024-05-19 11:53:21,776 - INFO - joeynmt.training - Epoch   5, Step:    22000, Batch Loss:     1.283419, Batch Acc: 0.609933, Tokens per Sec:    22842, Lr: 0.000300
2024-05-19 11:53:21,777 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:53:21,778 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:53:33,858 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.44, ppl:   4.22, acc:   0.59, generation: 11.9677[sec], evaluation: 0.0000[sec]
2024-05-19 11:53:33,860 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:53:34,120 - INFO - joeynmt.helpers - delete word_level_model/19500.ckpt
2024-05-19 11:53:34,130 - INFO - joeynmt.training - Example #0
2024-05-19 11:53:34,131 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:53:34,131 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:53:34,132 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'queste', 'due', 's@@', 'li@@', 'de', 'così', 'che', 'di@@', 'mostr@@', 'ano', 'che', 'la', 'sc@@', 'ar@@', 't@@', 'ica', 'sc@@', 'al@@', 'a', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'delle', 'dimen@@', 'sioni', 'del', '4@@', '8', 'st@@', 'ati', ',', 'ha', 'con@@', 'divi@@', 'so', 'il', '4@@', '0', '%', 'del', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:53:34,133 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:53:34,133 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:53:34,133 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso queste due slide così che dimostrano che la scartica scala, che per la maggior parte delle dimensioni del 48 stati, ha condiviso il 40% del 40%.
2024-05-19 11:53:34,134 - INFO - joeynmt.training - Example #1
2024-05-19 11:53:34,134 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:53:34,134 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:53:34,134 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'ser@@', 'ia', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'del', 'gh@@', 'i@@', 'acci@@', 'o', 'non', 'mostr@@', 'a', 'la', 'velo@@', 'c@@', 'ità', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:53:34,135 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:53:34,135 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:53:34,136 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione seria la seriamente di questo particolare problema del ghiaccio non mostra la velocità del ghiaccio.
2024-05-19 11:53:34,136 - INFO - joeynmt.training - Example #2
2024-05-19 11:53:34,136 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:53:34,137 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:53:34,137 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'b@@', 'ello', 'ar@@', 't@@', 'ico', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:53:34,137 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:53:34,138 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:53:34,138 - INFO - joeynmt.training - 	Hypothesis: La bello artico è, in un certo senso, il cuore del sistema climatico globale.
2024-05-19 11:53:34,138 - INFO - joeynmt.training - Example #3
2024-05-19 11:53:34,138 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:53:34,139 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:53:34,139 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'il', 'contr@@', 'atto', 'in', 'est@@', 'ate', 'e', 'i', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 11:53:34,139 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:53:34,140 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:53:34,140 - INFO - joeynmt.training - 	Hypothesis: E 'inverno e il contratto in estate e i contratti in estate.
2024-05-19 11:53:34,140 - INFO - joeynmt.training - Example #4
2024-05-19 11:53:34,140 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:53:34,141 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:53:34,141 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'pro@@', 'ssi@@', 'mo', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'ò', 'un', 'ra@@', 'pi@@', 'do', 'più', 'velo@@', 'c@@', 'emente', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:53:34,141 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:53:34,142 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:53:34,142 - INFO - joeynmt.training - 	Hypothesis: Il prossimo slide che vi mostrerò un rapido più velocemente di quello che è successo negli ultimi 25 anni.
2024-05-19 11:53:37,426 - INFO - joeynmt.training - Epoch   5, Step:    22100, Batch Loss:     1.377635, Batch Acc: 0.608615, Tokens per Sec:    21237, Lr: 0.000300
2024-05-19 11:53:40,727 - INFO - joeynmt.training - Epoch   5, Step:    22200, Batch Loss:     1.363049, Batch Acc: 0.610454, Tokens per Sec:    22508, Lr: 0.000300
2024-05-19 11:53:44,860 - INFO - joeynmt.training - Epoch   5, Step:    22300, Batch Loss:     1.399549, Batch Acc: 0.611771, Tokens per Sec:    18130, Lr: 0.000300
2024-05-19 11:53:48,398 - INFO - joeynmt.training - Epoch   5, Step:    22400, Batch Loss:     1.233349, Batch Acc: 0.605808, Tokens per Sec:    20338, Lr: 0.000300
2024-05-19 11:53:51,649 - INFO - joeynmt.training - Epoch   5, Step:    22500, Batch Loss:     1.295932, Batch Acc: 0.602296, Tokens per Sec:    22703, Lr: 0.000300
2024-05-19 11:53:51,650 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:53:51,650 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:54:03,955 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.44, ppl:   4.22, acc:   0.59, generation: 12.1936[sec], evaluation: 0.0000[sec]
2024-05-19 11:54:03,956 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:54:04,188 - INFO - joeynmt.helpers - delete word_level_model/20000.ckpt
2024-05-19 11:54:04,198 - INFO - joeynmt.training - Example #0
2024-05-19 11:54:04,199 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:54:04,200 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:54:04,200 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'sc@@', 'or@@', 'so', 'che', 'di@@', 'mostr@@', 'ano', 'che', 'la', 'di@@', 'mo@@', 'stra@@', 'zione', 'ar@@', 't@@', 'ica', 'che', 'la', 'maggi@@', 'or', 'parte', 'dei', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'dei', '4@@', '0', '%', 'è', 'stata', 'la', 'dimen@@', 'sione', 'del', '4@@', '0', '%', ',', 'ha', 's@@', 'chi@@', 'ato', 'da', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:54:04,201 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:54:04,201 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:54:04,202 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso scorso che dimostrano che la dimostrazione artica che la maggior parte dei tre milioni di anni è stata la dimensione dei 40% è stata la dimensione del 40%, ha schiato da 40%.
2024-05-19 11:54:04,202 - INFO - joeynmt.training - Example #1
2024-05-19 11:54:04,202 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:54:04,202 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:54:04,203 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'la', 'seri@@', 'amente', 'del', 'problema', 'del', 'parti@@', 'col@@', 'are', 'perché', 'non', 'mostr@@', 'a', 'la', 'sp@@', 'ett@@', 'a', 'non', 'mostr@@', 'a', 'la', 'più', 'po@@', 'll@@', 'a', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:54:04,203 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:54:04,204 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:54:04,204 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione la seriamente del problema del particolare perché non mostra la spetta non mostra la più polla del ghiaccio.
2024-05-19 11:54:04,204 - INFO - joeynmt.training - Example #2
2024-05-19 11:54:04,204 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:54:04,205 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:54:04,205 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'gh@@', 'i@@', 'acci@@', 'o', 'b@@', 'ene', 'c@@', '&@@', 'a@@', 'pos@@', ';', 'è', 'un', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:54:04,205 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:54:04,206 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:54:04,206 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio bene c'è un senso, il cuore del sistema clima globale.
2024-05-19 11:54:04,206 - INFO - joeynmt.training - Example #3
2024-05-19 11:54:04,206 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:54:04,207 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:54:04,207 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'ar@@', 'g@@', 'om@@', 'ento', 'in', 'in@@', 'ver@@', 'no', 'e', 'il', 'contr@@', 'atto', 'in', 'est@@', 'i@@', 'mi@@', 'era', '.', '</s>']
2024-05-19 11:54:04,207 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:54:04,208 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:54:04,208 - INFO - joeynmt.training - 	Hypothesis: Sargomento in inverno e il contratto in estimiera.
2024-05-19 11:54:04,208 - INFO - joeynmt.training - Example #4
2024-05-19 11:54:04,208 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:54:04,209 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:54:04,209 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'pro@@', 'ssi@@', 'mo', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'velo@@', 'c@@', 'emente', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:54:04,209 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:54:04,209 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:54:04,210 - INFO - joeynmt.training - 	Hypothesis: Il prossimo slide che vi mostrerà un rapido velocemente di quello che è successo negli ultimi 25 anni.
2024-05-19 11:54:07,586 - INFO - joeynmt.training - Epoch   5, Step:    22600, Batch Loss:     1.421128, Batch Acc: 0.607679, Tokens per Sec:    20680, Lr: 0.000300
2024-05-19 11:54:11,297 - INFO - joeynmt.training - Epoch   5, Step:    22700, Batch Loss:     1.360423, Batch Acc: 0.609833, Tokens per Sec:    20240, Lr: 0.000300
2024-05-19 11:54:15,410 - INFO - joeynmt.training - Epoch   5, Step:    22800, Batch Loss:     1.282672, Batch Acc: 0.609402, Tokens per Sec:    17927, Lr: 0.000300
2024-05-19 11:54:18,687 - INFO - joeynmt.training - Epoch   5, Step:    22900, Batch Loss:     1.193809, Batch Acc: 0.610053, Tokens per Sec:    22621, Lr: 0.000300
2024-05-19 11:54:21,936 - INFO - joeynmt.training - Epoch   5, Step:    23000, Batch Loss:     1.443598, Batch Acc: 0.607794, Tokens per Sec:    22651, Lr: 0.000300
2024-05-19 11:54:21,936 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:54:21,937 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:54:34,307 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.43, ppl:   4.18, acc:   0.59, generation: 12.2657[sec], evaluation: 0.0000[sec]
2024-05-19 11:54:34,308 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:54:34,542 - INFO - joeynmt.helpers - delete word_level_model/20500.ckpt
2024-05-19 11:54:34,552 - INFO - joeynmt.training - Example #0
2024-05-19 11:54:34,553 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:54:34,554 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:54:34,554 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', ',', 'questi', 'due', 's@@', 'li@@', 'de', 'così', 'che', 'la', 'di@@', 'mo@@', 'stra@@', 'zione', 'ar@@', 'c@@', 'ica', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'del', '4@@', '0', '%', ',', 'ha', 's@@', 'chi@@', 'zz@@', 'ato', 'da', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:54:34,555 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:54:34,555 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:54:34,556 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso, questi due slide così che la dimostrazione arcica, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione del 40%, ha schizzato da 40%.
2024-05-19 11:54:34,556 - INFO - joeynmt.training - Example #1
2024-05-19 11:54:34,557 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:54:34,557 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:54:34,557 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'capi@@', 'sc@@', 'a', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'la', 'velo@@', 'c@@', 'ità', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:54:34,558 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:54:34,558 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:54:34,558 - INFO - joeynmt.training - 	Hypothesis: Ma questa capisca la seriamente di questo particolare problema perché non mostra la velocità del ghiaccio.
2024-05-19 11:54:34,559 - INFO - joeynmt.training - Example #2
2024-05-19 11:54:34,559 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:54:34,559 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:54:34,559 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'gh@@', 'i@@', 'acci@@', 'o', 'ar@@', 't@@', 'ico', 'è', ',', 'in', 'un', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:54:34,560 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:54:34,560 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:54:34,561 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio artico è, in un senso, il cuore del sistema clima del sistema clima globale.
2024-05-19 11:54:34,561 - INFO - joeynmt.training - Example #3
2024-05-19 11:54:34,561 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:54:34,561 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:54:34,562 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'p@@', 'and@@', 'e', 'nel', 'l@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', 'e', 'il', 'contr@@', 'atto', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 11:54:34,562 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:54:34,562 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:54:34,563 - INFO - joeynmt.training - 	Hypothesis: Spande nel l'estate e il contratto in estate.
2024-05-19 11:54:34,563 - INFO - joeynmt.training - Example #4
2024-05-19 11:54:34,563 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:54:34,563 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:54:34,564 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'pro@@', 'ssi@@', 'mo', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'ete', 'un', 'velo@@', 'c@@', 'ità', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:54:34,564 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:54:34,564 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:54:34,565 - INFO - joeynmt.training - 	Hypothesis: Il prossimo slide che vi mostrerete un velocità di ciò che è successo negli ultimi 25 anni.
2024-05-19 11:54:37,934 - INFO - joeynmt.training - Epoch   5, Step:    23100, Batch Loss:     1.296953, Batch Acc: 0.606535, Tokens per Sec:    20702, Lr: 0.000300
2024-05-19 11:54:42,163 - INFO - joeynmt.training - Epoch   5, Step:    23200, Batch Loss:     1.272329, Batch Acc: 0.606323, Tokens per Sec:    17753, Lr: 0.000300
2024-05-19 11:54:45,531 - INFO - joeynmt.training - Epoch   5, Step:    23300, Batch Loss:     1.365348, Batch Acc: 0.611290, Tokens per Sec:    22467, Lr: 0.000300
2024-05-19 11:54:48,798 - INFO - joeynmt.training - Epoch   5, Step:    23400, Batch Loss:     1.400539, Batch Acc: 0.605071, Tokens per Sec:    23062, Lr: 0.000300
2024-05-19 11:54:52,086 - INFO - joeynmt.training - Epoch   5, Step:    23500, Batch Loss:     1.201383, Batch Acc: 0.606239, Tokens per Sec:    22499, Lr: 0.000300
2024-05-19 11:54:52,087 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:54:52,087 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:55:04,034 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.42, ppl:   4.14, acc:   0.59, generation: 11.8410[sec], evaluation: 0.0000[sec]
2024-05-19 11:55:04,035 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:55:04,270 - INFO - joeynmt.helpers - delete word_level_model/21500.ckpt
2024-05-19 11:55:04,276 - INFO - joeynmt.training - Example #0
2024-05-19 11:55:04,277 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:55:04,278 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:55:04,278 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', ',', 'queste', 'due', 's@@', 'li@@', 'de', 'così', 'che', 'la', 'di@@', 'mostr@@', 'a', 'che', 'la', 'maggi@@', 'or', 'parte', 'delle', 'ul@@', 'time', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'dei', '4@@', '0', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'dei', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:55:04,279 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:55:04,279 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:55:04,280 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso, queste due slide così che la dimostra che la maggior parte delle ultime tre milioni di anni è stata la dimensione dei 40 anni è stata la dimensione dei 40%.
2024-05-19 11:55:04,281 - INFO - joeynmt.training - Example #1
2024-05-19 11:55:04,281 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:55:04,282 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:55:04,282 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'capi@@', 'sc@@', 'e', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'lo', 'mostr@@', 'a', 'la', 't@@', 'ot@@', 'ale', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:55:04,283 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:55:04,283 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:55:04,283 - INFO - joeynmt.training - 	Hypothesis: Ma questa capisce la seriamente di questo particolare problema perché non lo mostra la totale del ghiaccio.
2024-05-19 11:55:04,283 - INFO - joeynmt.training - Example #2
2024-05-19 11:55:04,284 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:55:04,284 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:55:04,284 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'macch@@', 'ina', 'sc@@', 'e@@', 'gli@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'di', 'cu@@', 'ore', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:55:04,285 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:55:04,287 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:55:04,287 - INFO - joeynmt.training - 	Hypothesis: La macchina scegliica è, in un certo senso, il cuore di cuore clima globale.
2024-05-19 11:55:04,288 - INFO - joeynmt.training - Example #3
2024-05-19 11:55:04,288 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:55:04,288 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:55:04,288 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'em@@', 'br@@', 'a', 'in', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 11:55:04,289 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:55:04,289 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:55:04,290 - INFO - joeynmt.training - 	Hypothesis: Sembra in inverno e contratti in estate.
2024-05-19 11:55:04,290 - INFO - joeynmt.training - Example #4
2024-05-19 11:55:04,290 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:55:04,290 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:55:04,291 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'sar@@', 'ete', 'un', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'da', 'che', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:55:04,291 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:55:04,292 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:55:04,292 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che sarete un rapido rapida che ciò che è successo negli ultimi 25 anni.
2024-05-19 11:55:08,041 - INFO - joeynmt.training - Epoch   5, Step:    23600, Batch Loss:     1.311105, Batch Acc: 0.608336, Tokens per Sec:    18986, Lr: 0.000300
2024-05-19 11:55:12,081 - INFO - joeynmt.training - Epoch   5, Step:    23700, Batch Loss:     1.208168, Batch Acc: 0.609527, Tokens per Sec:    18778, Lr: 0.000300
2024-05-19 11:55:15,390 - INFO - joeynmt.training - Epoch   5, Step:    23800, Batch Loss:     1.272291, Batch Acc: 0.606051, Tokens per Sec:    22732, Lr: 0.000300
2024-05-19 11:55:18,679 - INFO - joeynmt.training - Epoch   5, Step:    23900, Batch Loss:     1.274294, Batch Acc: 0.610709, Tokens per Sec:    22772, Lr: 0.000300
2024-05-19 11:55:20,747 - INFO - joeynmt.training - Epoch   5: total training loss 6463.42
2024-05-19 11:55:20,748 - INFO - joeynmt.training - EPOCH 6
2024-05-19 11:55:22,228 - INFO - joeynmt.training - Epoch   6, Step:    24000, Batch Loss:     1.184461, Batch Acc: 0.624370, Tokens per Sec:    17842, Lr: 0.000300
2024-05-19 11:55:22,229 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:55:22,229 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:55:33,695 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.42, ppl:   4.12, acc:   0.59, generation: 11.3585[sec], evaluation: 0.0000[sec]
2024-05-19 11:55:33,696 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:55:33,930 - INFO - joeynmt.helpers - delete word_level_model/21000.ckpt
2024-05-19 11:55:33,940 - INFO - joeynmt.training - Example #0
2024-05-19 11:55:33,941 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:55:33,942 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:55:33,942 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'a@@', 'positi@@', 've', 'così', 'che', 'di@@', 'mostr@@', 'ano', 'la', 'macch@@', 'ina', 't@@', 'ot@@', 'ale', 'gh@@', 'i@@', 'acci@@', 'o', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'delle', 'dimen@@', 'sioni', 'dei', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:55:33,943 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:55:33,943 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:55:33,944 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due diapositive così che dimostrano la macchina totale ghiaccio, che per la maggior parte delle dimensioni dei 40%.
2024-05-19 11:55:33,944 - INFO - joeynmt.training - Example #1
2024-05-19 11:55:33,944 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:55:33,944 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:55:33,945 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'la', 'po@@', 'si@@', 'zione', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:55:33,945 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:55:33,946 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:55:33,946 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione la seriamente di questo particolare problema perché non mostra la posizione del ghiaccio.
2024-05-19 11:55:33,946 - INFO - joeynmt.training - Example #2
2024-05-19 11:55:33,946 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:55:33,947 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:55:33,947 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'c@@', 'acci@@', 'a', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', '.', '</s>']
2024-05-19 11:55:33,947 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:55:33,948 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:55:33,948 - INFO - joeynmt.training - 	Hypothesis: La caccia artica è, in un certo senso, il cuore del sistema clima.
2024-05-19 11:55:33,948 - INFO - joeynmt.training - Example #3
2024-05-19 11:55:33,949 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:55:33,949 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:55:33,949 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&@@', 'a@@', 'pos@@', ';', 'es@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'contr@@', 'at@@', 'ore', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 11:55:33,950 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:55:33,950 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:55:33,950 - INFO - joeynmt.training - 	Hypothesis: E 'espande in incontratore in estate.
2024-05-19 11:55:33,950 - INFO - joeynmt.training - Example #4
2024-05-19 11:55:33,951 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:55:33,951 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:55:33,951 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'do', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:55:33,952 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:55:33,952 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:55:33,952 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerà un rapido rapido di ciò che è successo negli ultimi 25 anni.
2024-05-19 11:55:37,894 - INFO - joeynmt.training - Epoch   6, Step:    24100, Batch Loss:     1.220553, Batch Acc: 0.629921, Tokens per Sec:    17806, Lr: 0.000300
2024-05-19 11:55:41,531 - INFO - joeynmt.training - Epoch   6, Step:    24200, Batch Loss:     1.264893, Batch Acc: 0.632254, Tokens per Sec:    20874, Lr: 0.000300
2024-05-19 11:55:44,807 - INFO - joeynmt.training - Epoch   6, Step:    24300, Batch Loss:     1.316636, Batch Acc: 0.633367, Tokens per Sec:    22990, Lr: 0.000300
2024-05-19 11:55:48,032 - INFO - joeynmt.training - Epoch   6, Step:    24400, Batch Loss:     1.304845, Batch Acc: 0.628315, Tokens per Sec:    23137, Lr: 0.000300
2024-05-19 11:55:52,029 - INFO - joeynmt.training - Epoch   6, Step:    24500, Batch Loss:     1.333153, Batch Acc: 0.628194, Tokens per Sec:    18665, Lr: 0.000300
2024-05-19 11:55:52,030 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:55:52,030 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:56:03,041 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.42, ppl:   4.14, acc:   0.59, generation: 10.9073[sec], evaluation: 0.0000[sec]
2024-05-19 11:56:03,263 - INFO - joeynmt.helpers - delete word_level_model/22000.ckpt
2024-05-19 11:56:03,269 - INFO - joeynmt.training - Example #0
2024-05-19 11:56:03,270 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:56:03,273 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:56:03,273 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'mostr@@', 'are', 'queste', 'due', 's@@', 'li@@', 'de', 'così', 'che', 'di@@', 'mostr@@', 'ano', 'che', 'l@@', '&@@', 'a@@', 'pos@@', ';', 'au@@', 'to', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'delle', 'dimen@@', 'sioni', 'dell@@', '&@@', 'a@@', 'pos@@', ';', 'ulti@@', 'mo', 'tre', 'milioni', 'di', 'anni', 'sono', 'st@@', 'ati', 'la', 'dimen@@', 'sione', 'dei', '4@@', '0', 'per', 'c@@', 'ento', '.', '</s>']
2024-05-19 11:56:03,274 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:56:03,274 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:56:03,275 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso mostrare queste due slide così che dimostrano che l'auto, che per la maggior parte delle dimensioni dell'ultimo tre milioni di anni sono stati la dimensione dei 40 per cento.
2024-05-19 11:56:03,275 - INFO - joeynmt.training - Example #1
2024-05-19 11:56:03,275 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:56:03,276 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:56:03,276 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'la', 'seri@@', 'et@@', 'à', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'la', 'velo@@', 'c@@', 'ità', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:56:03,277 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:56:03,277 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:56:03,277 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione la serietà di questo particolare problema perché non mostra la velocità del ghiaccio.
2024-05-19 11:56:03,277 - INFO - joeynmt.training - Example #2
2024-05-19 11:56:03,278 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:56:03,278 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:56:03,278 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'au@@', 'to', 'ar@@', 't@@', 'ico', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:56:03,279 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:56:03,279 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:56:03,280 - INFO - joeynmt.training - 	Hypothesis: L'auto artico è, in un certo senso, il cuore del sistema clima globale.
2024-05-19 11:56:03,280 - INFO - joeynmt.training - Example #3
2024-05-19 11:56:03,280 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:56:03,280 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:56:03,280 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', 'p@@', 'and@@', 'e', 'in', 'int@@', 'ro@@', 'dur@@', 're', 'in', 'est@@', 'i@@', 'vo', '.', '</s>']
2024-05-19 11:56:03,281 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:56:03,282 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:56:03,282 - INFO - joeynmt.training - 	Hypothesis: Si espande in introdurre in estivo.
2024-05-19 11:56:03,282 - INFO - joeynmt.training - Example #4
2024-05-19 11:56:03,282 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:56:03,282 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:56:03,283 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'ete', 'un', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'do', 'velo@@', 'ce', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:56:03,283 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:56:03,283 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:56:03,284 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerete un rapido rapido veloce di quello che è successo negli ultimi 25 anni.
2024-05-19 11:56:07,784 - INFO - joeynmt.training - Epoch   6, Step:    24600, Batch Loss:     1.159806, Batch Acc: 0.633160, Tokens per Sec:    15539, Lr: 0.000300
2024-05-19 11:56:11,137 - INFO - joeynmt.training - Epoch   6, Step:    24700, Batch Loss:     1.202439, Batch Acc: 0.622573, Tokens per Sec:    21998, Lr: 0.000300
2024-05-19 11:56:14,402 - INFO - joeynmt.training - Epoch   6, Step:    24800, Batch Loss:     1.311200, Batch Acc: 0.618588, Tokens per Sec:    22545, Lr: 0.000300
2024-05-19 11:56:17,793 - INFO - joeynmt.training - Epoch   6, Step:    24900, Batch Loss:     1.230568, Batch Acc: 0.624593, Tokens per Sec:    22192, Lr: 0.000300
2024-05-19 11:56:22,024 - INFO - joeynmt.training - Epoch   6, Step:    25000, Batch Loss:     1.231251, Batch Acc: 0.622530, Tokens per Sec:    17441, Lr: 0.000300
2024-05-19 11:56:22,025 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:56:22,025 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:56:34,662 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.42, ppl:   4.15, acc:   0.59, generation: 12.4320[sec], evaluation: 0.0000[sec]
2024-05-19 11:56:34,921 - INFO - joeynmt.helpers - delete word_level_model/22500.ckpt
2024-05-19 11:56:34,927 - INFO - joeynmt.training - Example #0
2024-05-19 11:56:34,928 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:56:34,929 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:56:34,929 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 's@@', 'li@@', 'de', 'così', 'che', 'di@@', 'mostr@@', 'ano', 'che', 'la', 'macch@@', 'ina', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'dei', '4@@', '0', '%', 'di', 'met@@', 'ri', 'di', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:56:34,930 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:56:34,931 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:56:34,931 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due slide così che dimostrano che la macchina artica, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dei 40% di metri di 40%.
2024-05-19 11:56:34,931 - INFO - joeynmt.training - Example #1
2024-05-19 11:56:34,932 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:56:34,932 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:56:34,932 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'capi@@', 'sc@@', 'e', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'la', 'velo@@', 'c@@', 'ità', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:56:34,933 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:56:34,933 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:56:34,934 - INFO - joeynmt.training - 	Hypothesis: Ma questa capisce la seriamente di questo particolare problema perché non mostra la velocità del ghiaccio.
2024-05-19 11:56:34,934 - INFO - joeynmt.training - Example #2
2024-05-19 11:56:34,934 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:56:34,935 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:56:34,935 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 't@@', 'ot@@', 't@@', 'ica', 'vol@@', 'te', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:56:34,935 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:56:34,936 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:56:34,936 - INFO - joeynmt.training - 	Hypothesis: La tottica volte è, in un certo senso, il cuore del cuore del sistema clima globale.
2024-05-19 11:56:34,936 - INFO - joeynmt.training - Example #3
2024-05-19 11:56:34,937 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:56:34,937 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:56:34,937 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&@@', 'a@@', 'pos@@', ';', 'es@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'atto', 'in', 'est@@', 'i@@', 'vo', '.', '</s>']
2024-05-19 11:56:34,938 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:56:34,938 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:56:34,938 - INFO - joeynmt.training - 	Hypothesis: E 'espande in inverno e contratto in estivo.
2024-05-19 11:56:34,939 - INFO - joeynmt.training - Example #4
2024-05-19 11:56:34,939 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:56:34,939 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:56:34,939 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'do', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:56:34,940 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:56:34,941 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:56:34,941 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerà un rapido rapido di ciò che è successo negli ultimi 25 anni.
2024-05-19 11:56:38,535 - INFO - joeynmt.training - Epoch   6, Step:    25100, Batch Loss:     1.275117, Batch Acc: 0.615827, Tokens per Sec:    19453, Lr: 0.000300
2024-05-19 11:56:41,762 - INFO - joeynmt.training - Epoch   6, Step:    25200, Batch Loss:     1.378240, Batch Acc: 0.623691, Tokens per Sec:    22992, Lr: 0.000300
2024-05-19 11:56:45,067 - INFO - joeynmt.training - Epoch   6, Step:    25300, Batch Loss:     1.258632, Batch Acc: 0.623348, Tokens per Sec:    22143, Lr: 0.000300
2024-05-19 11:56:49,094 - INFO - joeynmt.training - Epoch   6, Step:    25400, Batch Loss:     1.282924, Batch Acc: 0.625916, Tokens per Sec:    18841, Lr: 0.000300
2024-05-19 11:56:52,682 - INFO - joeynmt.training - Epoch   6, Step:    25500, Batch Loss:     1.239129, Batch Acc: 0.623644, Tokens per Sec:    20950, Lr: 0.000300
2024-05-19 11:56:52,682 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:56:52,683 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:57:03,826 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.42, ppl:   4.13, acc:   0.59, generation: 10.9274[sec], evaluation: 0.0000[sec]
2024-05-19 11:57:04,116 - INFO - joeynmt.helpers - delete word_level_model/23000.ckpt
2024-05-19 11:57:04,129 - INFO - joeynmt.training - Example #0
2024-05-19 11:57:04,130 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:57:04,130 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:57:04,130 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 's@@', 'a@@', 'positi@@', 've', 'così', 'che', 'la', 'di@@', 'mostr@@', 'a', 'che', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'sono', 'st@@', 'ati', 'le', 'dimen@@', 'sioni', 'delle', 't@@', 'ass@@', 'et@@', 'te', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'delle', 'dimen@@', 'sioni', 'del', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:57:04,131 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:57:04,132 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:57:04,132 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due sapositive così che la dimostra che la maggior parte degli ultimi tre milioni di anni sono stati le dimensioni delle tassette, che per la maggior parte delle dimensioni del 40%.
2024-05-19 11:57:04,132 - INFO - joeynmt.training - Example #1
2024-05-19 11:57:04,132 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:57:04,133 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:57:04,133 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'capi@@', 'sc@@', 'e', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'lo', 'fa', 'perché', 'non', 'lo', 'mostr@@', 'a', 'la', 'velo@@', 'c@@', 'ità', 'della', 'gh@@', 'i@@', 'acci@@', 'a', '.', '</s>']
2024-05-19 11:57:04,134 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:57:04,134 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:57:04,134 - INFO - joeynmt.training - 	Hypothesis: Ma questa capisce la seriamente di questo particolare problema perché non lo fa perché non lo mostra la velocità della ghiaccia.
2024-05-19 11:57:04,134 - INFO - joeynmt.training - Example #2
2024-05-19 11:57:04,135 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:57:04,135 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:57:04,135 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'b@@', 'an@@', 'ca', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:57:04,136 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:57:04,136 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:57:04,136 - INFO - joeynmt.training - 	Hypothesis: La banca artica è, in un certo senso, il cuore del sistema clima globale.
2024-05-19 11:57:04,137 - INFO - joeynmt.training - Example #3
2024-05-19 11:57:04,137 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:57:04,137 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:57:04,137 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&@@', 'a@@', 'pos@@', ';', 'es@@', 'p@@', 'and@@', 'e', 'in', 'cont@@', 'atto', 'in', 'est@@', 'ate', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 11:57:04,138 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:57:04,138 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:57:04,139 - INFO - joeynmt.training - 	Hypothesis: E 'espande in contatto in estate e contratti in estate.
2024-05-19 11:57:04,139 - INFO - joeynmt.training - Example #4
2024-05-19 11:57:04,139 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:57:04,140 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:57:04,140 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'velo@@', 'c@@', 'emente', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:57:04,141 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:57:04,141 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:57:04,141 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerà un rapido velocemente di ciò che è successo negli ultimi 25 anni.
2024-05-19 11:57:07,509 - INFO - joeynmt.training - Epoch   6, Step:    25600, Batch Loss:     1.348433, Batch Acc: 0.623920, Tokens per Sec:    20375, Lr: 0.000300
2024-05-19 11:57:10,858 - INFO - joeynmt.training - Epoch   6, Step:    25700, Batch Loss:     1.305585, Batch Acc: 0.619031, Tokens per Sec:    21482, Lr: 0.000300
2024-05-19 11:57:14,185 - INFO - joeynmt.training - Epoch   6, Step:    25800, Batch Loss:     1.216524, Batch Acc: 0.620970, Tokens per Sec:    22900, Lr: 0.000300
2024-05-19 11:57:18,444 - INFO - joeynmt.training - Epoch   6, Step:    25900, Batch Loss:     1.226408, Batch Acc: 0.616610, Tokens per Sec:    17533, Lr: 0.000300
2024-05-19 11:57:21,715 - INFO - joeynmt.training - Epoch   6, Step:    26000, Batch Loss:     1.178998, Batch Acc: 0.618711, Tokens per Sec:    23239, Lr: 0.000300
2024-05-19 11:57:21,716 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:57:21,716 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:57:33,913 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.41, ppl:   4.08, acc:   0.59, generation: 12.0593[sec], evaluation: 0.0000[sec]
2024-05-19 11:57:33,914 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:57:34,170 - INFO - joeynmt.helpers - delete word_level_model/25000.ckpt
2024-05-19 11:57:34,180 - INFO - joeynmt.training - Example #0
2024-05-19 11:57:34,181 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:57:34,182 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:57:34,182 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'queste', 'due', 's@@', 'li@@', 'de', 'così', 'che', 'la', 'di@@', 'mo@@', 'stra@@', 'zione', 'ar@@', 'c@@', 'ica', 'che', 'la', 'macch@@', 'ina', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'delle', 'ul@@', 'time', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'dei', '4@@', '8', '%', '.', '</s>']
2024-05-19 11:57:34,183 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:57:34,183 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:57:34,183 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso queste due slide così che la dimostrazione arcica che la macchina artica, che per la maggior parte delle ultime tre milioni di anni è stata la dimensione dei 48%.
2024-05-19 11:57:34,184 - INFO - joeynmt.training - Example #1
2024-05-19 11:57:34,184 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:57:34,184 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:57:34,184 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'la', 'seri@@', 'amente', 'di', 'questo', 'problema', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'la', 'sp@@', 'ett@@', 'a', 'della', 'gh@@', 'i@@', 'acci@@', 'a', '.', '</s>']
2024-05-19 11:57:34,185 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:57:34,185 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:57:34,186 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione la seriamente di questo problema particolare problema perché non mostra la spetta della ghiaccia.
2024-05-19 11:57:34,186 - INFO - joeynmt.training - Example #2
2024-05-19 11:57:34,186 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:57:34,187 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:57:34,187 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'g@@', 'el@@', 'o', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:57:34,187 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:57:34,188 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:57:34,188 - INFO - joeynmt.training - 	Hypothesis: La gelo artica è, in un certo senso, il cuore del sistema clima del sistema clima del sistema clima globale.
2024-05-19 11:57:34,188 - INFO - joeynmt.training - Example #3
2024-05-19 11:57:34,188 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:57:34,189 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:57:34,189 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', 'hi@@', 'ar@@', 'o', 'in', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'ate', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 11:57:34,189 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:57:34,190 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:57:34,190 - INFO - joeynmt.training - 	Hypothesis: Chiaro in inverno e contratti in estate e contratti in estate.
2024-05-19 11:57:34,190 - INFO - joeynmt.training - Example #4
2024-05-19 11:57:34,191 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:57:34,191 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:57:34,191 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'do', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:57:34,192 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:57:34,192 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:57:34,192 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerà un rapido rapido di quello che è successo negli ultimi 25 anni.
2024-05-19 11:57:37,449 - INFO - joeynmt.training - Epoch   6, Step:    26100, Batch Loss:     1.388732, Batch Acc: 0.626572, Tokens per Sec:    20601, Lr: 0.000300
2024-05-19 11:57:40,708 - INFO - joeynmt.training - Epoch   6, Step:    26200, Batch Loss:     1.291151, Batch Acc: 0.620900, Tokens per Sec:    22786, Lr: 0.000300
2024-05-19 11:57:44,559 - INFO - joeynmt.training - Epoch   6, Step:    26300, Batch Loss:     1.295100, Batch Acc: 0.621354, Tokens per Sec:    19219, Lr: 0.000300
2024-05-19 11:57:48,382 - INFO - joeynmt.training - Epoch   6, Step:    26400, Batch Loss:     1.713660, Batch Acc: 0.618044, Tokens per Sec:    19282, Lr: 0.000300
2024-05-19 11:57:51,623 - INFO - joeynmt.training - Epoch   6, Step:    26500, Batch Loss:     1.443049, Batch Acc: 0.624569, Tokens per Sec:    22488, Lr: 0.000300
2024-05-19 11:57:51,623 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:57:51,623 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:58:02,908 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.41, ppl:   4.11, acc:   0.60, generation: 11.1796[sec], evaluation: 0.0000[sec]
2024-05-19 11:58:03,121 - INFO - joeynmt.helpers - delete word_level_model/24500.ckpt
2024-05-19 11:58:03,131 - INFO - joeynmt.training - Example #0
2024-05-19 11:58:03,132 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:58:03,132 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:58:03,132 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'a@@', 'positi@@', 've', 'così', 'che', 'la', 'di@@', 'mostr@@', 'a', 'che', 'l@@', '&@@', 'a@@', 'pos@@', ';', 'ar@@', 't@@', 'ica', 'per', 'la', 'maggi@@', 'or', 'parte', 'delle', 'dimen@@', 'sioni', 'del', '4@@', '8', '%', '.', '</s>']
2024-05-19 11:58:03,133 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:58:03,133 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:58:03,134 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due diapositive così che la dimostra che l'artica per la maggior parte delle dimensioni del 48%.
2024-05-19 11:58:03,134 - INFO - joeynmt.training - Example #1
2024-05-19 11:58:03,135 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:58:03,135 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:58:03,135 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'ha', 'mostr@@', 'ato', 'la', 't@@', 'ick@@', 'n@@', 'ess', 'della', 'gh@@', 'i@@', 'acci@@', 'a', '.', '</s>']
2024-05-19 11:58:03,136 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:58:03,136 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:58:03,136 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione la seriamente di questo particolare problema perché non ha mostrato la tickness della ghiaccia.
2024-05-19 11:58:03,137 - INFO - joeynmt.training - Example #2
2024-05-19 11:58:03,137 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:58:03,137 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:58:03,137 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'ca@@', 'po', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 's@@', 'ent@@', 'im@@', 'ento', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', '.', '</s>']
2024-05-19 11:58:03,138 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:58:03,138 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:58:03,139 - INFO - joeynmt.training - 	Hypothesis: La capo artica è, in un certo senso, il cuore sentimento, il cuore del sistema clima.
2024-05-19 11:58:03,139 - INFO - joeynmt.training - Example #3
2024-05-19 11:58:03,139 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:58:03,140 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:58:03,140 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'il', 'contr@@', 'atto', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 11:58:03,140 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:58:03,141 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:58:03,141 - INFO - joeynmt.training - 	Hypothesis: E 'inverno e il contratto in estate.
2024-05-19 11:58:03,141 - INFO - joeynmt.training - Example #4
2024-05-19 11:58:03,141 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:58:03,146 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:58:03,146 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'sar@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'do', 'di', 'for@@', 'ma', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:58:03,147 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:58:03,147 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:58:03,147 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che sarà un rapido rapido di forma di quello che è successo negli ultimi 25 anni.
2024-05-19 11:58:06,489 - INFO - joeynmt.training - Epoch   6, Step:    26600, Batch Loss:     1.211720, Batch Acc: 0.620985, Tokens per Sec:    21138, Lr: 0.000300
2024-05-19 11:58:09,938 - INFO - joeynmt.training - Epoch   6, Step:    26700, Batch Loss:     1.268644, Batch Acc: 0.618020, Tokens per Sec:    21753, Lr: 0.000300
2024-05-19 11:58:14,127 - INFO - joeynmt.training - Epoch   6, Step:    26800, Batch Loss:     1.533595, Batch Acc: 0.616183, Tokens per Sec:    17783, Lr: 0.000300
2024-05-19 11:58:17,498 - INFO - joeynmt.training - Epoch   6, Step:    26900, Batch Loss:     1.235787, Batch Acc: 0.622845, Tokens per Sec:    22372, Lr: 0.000300
2024-05-19 11:58:20,944 - INFO - joeynmt.training - Epoch   6, Step:    27000, Batch Loss:     1.341484, Batch Acc: 0.622261, Tokens per Sec:    22073, Lr: 0.000300
2024-05-19 11:58:20,945 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:58:20,945 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:58:33,852 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.40, ppl:   4.07, acc:   0.60, generation: 12.8011[sec], evaluation: 0.0000[sec]
2024-05-19 11:58:33,853 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:58:34,103 - INFO - joeynmt.helpers - delete word_level_model/23500.ckpt
2024-05-19 11:58:34,118 - INFO - joeynmt.training - Example #0
2024-05-19 11:58:34,119 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:58:34,120 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:58:34,120 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'questi', 'due', 'di@@', 'a@@', 'positi@@', 'vi', 'così', 'che', 'di@@', 'mostr@@', 'ò', 'che', 'il', 'gh@@', 'i@@', 'acci@@', 'o', 'ar@@', 't@@', 'ico', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'è', 'stato', 'la', 'dimen@@', 'sione', 'degli', 'st@@', 'ati', 'più', 'bas@@', 'so', 'degli', 'st@@', 'ati', ',', 'ha', 's@@', 'qual@@', 'i', '4@@', '0', '%', '.', '</s>']
2024-05-19 11:58:34,120 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:58:34,121 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:58:34,121 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso questi due diapositivi così che dimostrò che il ghiaccio artico, che per la maggior parte degli ultimi tre milioni di anni è stato la dimensione degli stati più basso degli stati, ha squali 40%.
2024-05-19 11:58:34,121 - INFO - joeynmt.training - Example #1
2024-05-19 11:58:34,122 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:58:34,122 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:58:34,122 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'la', 'seri@@', 'et@@', 'à', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'ha', 'mostr@@', 'ato', 'la', 'po@@', 'll@@', 'enza', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:58:34,123 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:58:34,123 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:58:34,123 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione la serietà di questo particolare problema perché non ha mostrato la pollenza del ghiaccio.
2024-05-19 11:58:34,124 - INFO - joeynmt.training - Example #2
2024-05-19 11:58:34,124 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:58:34,124 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:58:34,124 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'gh@@', 'i@@', 'acci@@', 'o', 'ar@@', 't@@', 'ico', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:58:34,125 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:58:34,125 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:58:34,126 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio artico è, in un certo senso, il cuore del sistema climatico globale.
2024-05-19 11:58:34,126 - INFO - joeynmt.training - Example #3
2024-05-19 11:58:34,126 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:58:34,127 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:58:34,127 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 11:58:34,127 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:58:34,128 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:58:34,128 - INFO - joeynmt.training - 	Hypothesis: Si espande in inverno e contratti in estate.
2024-05-19 11:58:34,128 - INFO - joeynmt.training - Example #4
2024-05-19 11:58:34,128 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:58:34,128 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:58:34,129 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'sar@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'velo@@', 'ce', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:58:34,129 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:58:34,130 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:58:34,130 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che sarà un rapido veloce di quello che è successo negli ultimi 25 anni.
2024-05-19 11:58:37,434 - INFO - joeynmt.training - Epoch   6, Step:    27100, Batch Loss:     1.399410, Batch Acc: 0.624240, Tokens per Sec:    20297, Lr: 0.000300
2024-05-19 11:58:41,361 - INFO - joeynmt.training - Epoch   6, Step:    27200, Batch Loss:     1.264520, Batch Acc: 0.616909, Tokens per Sec:    18801, Lr: 0.000300
2024-05-19 11:58:45,128 - INFO - joeynmt.training - Epoch   6, Step:    27300, Batch Loss:     1.407598, Batch Acc: 0.620546, Tokens per Sec:    19976, Lr: 0.000300
2024-05-19 11:58:48,369 - INFO - joeynmt.training - Epoch   6, Step:    27400, Batch Loss:     1.201003, Batch Acc: 0.629092, Tokens per Sec:    22640, Lr: 0.000300
2024-05-19 11:58:51,663 - INFO - joeynmt.training - Epoch   6, Step:    27500, Batch Loss:     1.223479, Batch Acc: 0.623654, Tokens per Sec:    22134, Lr: 0.000300
2024-05-19 11:58:51,663 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:58:51,664 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:59:04,219 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.40, ppl:   4.06, acc:   0.60, generation: 12.4335[sec], evaluation: 0.0000[sec]
2024-05-19 11:59:04,220 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:59:04,459 - INFO - joeynmt.helpers - delete word_level_model/25500.ckpt
2024-05-19 11:59:04,464 - INFO - joeynmt.training - Example #0
2024-05-19 11:59:04,464 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:59:04,465 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:59:04,465 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'a@@', 'positi@@', 've', 'così', 'che', 'la', 'di@@', 'mostr@@', 'a', 'che', 'il', 'gh@@', 'i@@', 'acci@@', 'o', 'ar@@', 't@@', 'ico', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'dei', '4@@', '0', 'per', 'c@@', 'ento', '.', '</s>']
2024-05-19 11:59:04,466 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:59:04,466 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:59:04,466 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due diapositive così che la dimostra che il ghiaccio artico, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dei 40 per cento.
2024-05-19 11:59:04,466 - INFO - joeynmt.training - Example #1
2024-05-19 11:59:04,467 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:59:04,467 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:59:04,467 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'la', 'seri@@', 'et@@', 'à', 'di', 'questo', 'problema', 'perché', 'non', 'mostr@@', 'a', 'la', 'po@@', 'll@@', 'a', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:59:04,468 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:59:04,468 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:59:04,468 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione la serietà di questo problema perché non mostra la polla di questo particolare problema del ghiaccio.
2024-05-19 11:59:04,468 - INFO - joeynmt.training - Example #2
2024-05-19 11:59:04,469 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:59:04,469 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:59:04,469 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'gh@@', 'i@@', 'acci@@', 'o', 'ar@@', 't@@', 'ico', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:59:04,469 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:59:04,470 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:59:04,470 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio artico è, in un certo senso, il cuore del sistema clima globale.
2024-05-19 11:59:04,470 - INFO - joeynmt.training - Example #3
2024-05-19 11:59:04,471 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:59:04,471 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:59:04,471 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'per@@', 'iamo', 'in', 'in@@', 'ver@@', 'no', 'e', 'il', 'contr@@', 'atto', 'in', 'est@@', 'ate', 'e', 'il', 'contr@@', 'atto', '.', '</s>']
2024-05-19 11:59:04,472 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:59:04,472 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:59:04,472 - INFO - joeynmt.training - 	Hypothesis: Speriamo in inverno e il contratto in estate e il contratto.
2024-05-19 11:59:04,472 - INFO - joeynmt.training - Example #4
2024-05-19 11:59:04,473 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:59:04,473 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:59:04,473 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'ete', 'un', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'da', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:59:04,473 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:59:04,474 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:59:04,474 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerete un rapido rapida di quello che è successo negli ultimi 25 anni.
2024-05-19 11:59:08,219 - INFO - joeynmt.training - Epoch   6, Step:    27600, Batch Loss:     1.276115, Batch Acc: 0.626997, Tokens per Sec:    19288, Lr: 0.000300
2024-05-19 11:59:12,316 - INFO - joeynmt.training - Epoch   6, Step:    27700, Batch Loss:     1.263171, Batch Acc: 0.621946, Tokens per Sec:    18646, Lr: 0.000300
2024-05-19 11:59:15,631 - INFO - joeynmt.training - Epoch   6, Step:    27800, Batch Loss:     1.267330, Batch Acc: 0.623333, Tokens per Sec:    22147, Lr: 0.000300
2024-05-19 11:59:18,924 - INFO - joeynmt.training - Epoch   6, Step:    27900, Batch Loss:     1.279261, Batch Acc: 0.626850, Tokens per Sec:    22939, Lr: 0.000300
2024-05-19 11:59:22,645 - INFO - joeynmt.training - Epoch   6, Step:    28000, Batch Loss:     1.377276, Batch Acc: 0.626361, Tokens per Sec:    20025, Lr: 0.000300
2024-05-19 11:59:22,646 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:59:22,646 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 11:59:34,422 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.39, ppl:   4.03, acc:   0.60, generation: 11.6565[sec], evaluation: 0.0000[sec]
2024-05-19 11:59:34,423 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 11:59:34,667 - INFO - joeynmt.helpers - delete word_level_model/24000.ckpt
2024-05-19 11:59:34,678 - INFO - joeynmt.training - Example #0
2024-05-19 11:59:34,679 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 11:59:34,679 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 11:59:34,679 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'a@@', 'positi@@', 've', 'così', 'che', 'la', 'di@@', 'mo@@', 'stra@@', 'zione', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'dei', '4@@', '8', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'dei', '4@@', '8', '%', 'di', 'bas@@', 'so', ',', 'ha', 's@@', 'par@@', 'ire', 'dal', '4@@', '8', '%', '.', '</s>']
2024-05-19 11:59:34,680 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 11:59:34,681 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 11:59:34,681 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due diapositive così che la dimostrazione artica, che per la maggior parte dei 48 anni è stata la dimensione dei 48% di basso, ha sparire dal 48%.
2024-05-19 11:59:34,682 - INFO - joeynmt.training - Example #1
2024-05-19 11:59:34,682 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 11:59:34,682 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 11:59:34,682 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'la', 'seri@@', 'amente', 'del', 'tutto', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'la', 'velo@@', 'c@@', 'ità', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 11:59:34,683 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 11:59:34,683 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 11:59:34,683 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione la seriamente del tutto questo particolare problema perché non mostra la velocità del ghiaccio.
2024-05-19 11:59:34,684 - INFO - joeynmt.training - Example #2
2024-05-19 11:59:34,684 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 11:59:34,684 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 11:59:34,684 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'gh@@', 'i@@', 'acci@@', 'o', 'ar@@', 't@@', 'ico', 'è', ',', 'in', 'un', 'sen@@', 'so', ',', 'nel', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 11:59:34,685 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 11:59:34,685 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 11:59:34,685 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio artico è, in un senso, nel senso, il cuore del sistema clima globale.
2024-05-19 11:59:34,686 - INFO - joeynmt.training - Example #3
2024-05-19 11:59:34,686 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 11:59:34,686 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 11:59:34,686 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'cont@@', 'ra@@', 'sto', '.', '</s>']
2024-05-19 11:59:34,687 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 11:59:34,687 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 11:59:34,687 - INFO - joeynmt.training - 	Hypothesis: Si espande in inverno e contrasto.
2024-05-19 11:59:34,688 - INFO - joeynmt.training - Example #4
2024-05-19 11:59:34,688 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 11:59:34,688 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 11:59:34,688 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'anno', 'un', 'ra@@', 'pi@@', 'do', 'velo@@', 'c@@', 'emente', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 11:59:34,689 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 11:59:34,689 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 11:59:34,690 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostreranno un rapido velocemente di quello che è successo negli ultimi 25 anni.
2024-05-19 11:59:38,929 - INFO - joeynmt.training - Epoch   6, Step:    28100, Batch Loss:     1.261107, Batch Acc: 0.620343, Tokens per Sec:    16215, Lr: 0.000300
2024-05-19 11:59:42,314 - INFO - joeynmt.training - Epoch   6, Step:    28200, Batch Loss:     1.310093, Batch Acc: 0.621944, Tokens per Sec:    22311, Lr: 0.000300
2024-05-19 11:59:45,550 - INFO - joeynmt.training - Epoch   6, Step:    28300, Batch Loss:     1.342434, Batch Acc: 0.626478, Tokens per Sec:    22850, Lr: 0.000300
2024-05-19 11:59:48,804 - INFO - joeynmt.training - Epoch   6, Step:    28400, Batch Loss:     1.454474, Batch Acc: 0.625480, Tokens per Sec:    22708, Lr: 0.000300
2024-05-19 11:59:52,952 - INFO - joeynmt.training - Epoch   6, Step:    28500, Batch Loss:     1.175305, Batch Acc: 0.621738, Tokens per Sec:    17959, Lr: 0.000300
2024-05-19 11:59:52,952 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 11:59:52,953 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:00:05,234 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.39, ppl:   4.02, acc:   0.60, generation: 12.0849[sec], evaluation: 0.0000[sec]
2024-05-19 12:00:05,235 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 12:00:05,502 - INFO - joeynmt.helpers - delete word_level_model/26500.ckpt
2024-05-19 12:00:05,507 - INFO - joeynmt.training - Example #0
2024-05-19 12:00:05,508 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:00:05,509 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:00:05,509 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'a@@', 'positi@@', 've', 'così', 'che', 'di@@', 'mostr@@', 'ano', 'che', 'la', 'macch@@', 'ina', 'ar@@', 'c@@', 'ica', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'dei', '4@@', '0', '%', '.', '</s>']
2024-05-19 12:00:05,511 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:00:05,512 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:00:05,512 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due diapositive così che dimostrano che la macchina arcica, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dei 40%.
2024-05-19 12:00:05,512 - INFO - joeynmt.training - Example #1
2024-05-19 12:00:05,512 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:00:05,512 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:00:05,513 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'capi@@', 'sc@@', 'e', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'la', 'ter@@', 'ra', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:00:05,513 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:00:05,514 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:00:05,514 - INFO - joeynmt.training - 	Hypothesis: Ma questa capisce la seriamente di questo particolare problema perché non mostra la terra del ghiaccio.
2024-05-19 12:00:05,514 - INFO - joeynmt.training - Example #2
2024-05-19 12:00:05,514 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:00:05,515 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:00:05,515 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'c@@', 'ec@@', 'c@@', 'et@@', 'era', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:00:05,516 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:00:05,516 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:00:05,516 - INFO - joeynmt.training - 	Hypothesis: La ceccetera, in un certo senso, il cuore cuore del sistema clima del sistema clima del sistema clima globale.
2024-05-19 12:00:05,516 - INFO - joeynmt.training - Example #3
2024-05-19 12:00:05,517 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:00:05,517 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:00:05,517 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'p@@', 'and@@', 'a', 'in', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 12:00:05,517 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:00:05,518 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:00:05,518 - INFO - joeynmt.training - 	Hypothesis: Spanda in inverno e contratti in estate.
2024-05-19 12:00:05,518 - INFO - joeynmt.training - Example #4
2024-05-19 12:00:05,518 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:00:05,519 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:00:05,519 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'velo@@', 'c@@', 'emente', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:00:05,520 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:00:05,520 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:00:05,520 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerà un rapido velocemente di quello che è successo negli ultimi 25 anni.
2024-05-19 12:00:09,490 - INFO - joeynmt.training - Epoch   6, Step:    28600, Batch Loss:     1.308821, Batch Acc: 0.622348, Tokens per Sec:    17152, Lr: 0.000300
2024-05-19 12:00:12,775 - INFO - joeynmt.training - Epoch   6, Step:    28700, Batch Loss:     1.210407, Batch Acc: 0.618869, Tokens per Sec:    22344, Lr: 0.000300
2024-05-19 12:00:14,941 - INFO - joeynmt.training - Epoch   6: total training loss 6213.68
2024-05-19 12:00:14,942 - INFO - joeynmt.training - EPOCH 7
2024-05-19 12:00:16,132 - INFO - joeynmt.training - Epoch   7, Step:    28800, Batch Loss:     1.378710, Batch Acc: 0.639965, Tokens per Sec:    22132, Lr: 0.000300
2024-05-19 12:00:20,090 - INFO - joeynmt.training - Epoch   7, Step:    28900, Batch Loss:     1.158005, Batch Acc: 0.646453, Tokens per Sec:    18806, Lr: 0.000300
2024-05-19 12:00:23,938 - INFO - joeynmt.training - Epoch   7, Step:    29000, Batch Loss:     1.335185, Batch Acc: 0.639856, Tokens per Sec:    19576, Lr: 0.000300
2024-05-19 12:00:23,939 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:00:23,939 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:00:36,213 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.39, ppl:   4.03, acc:   0.60, generation: 12.1626[sec], evaluation: 0.0000[sec]
2024-05-19 12:00:36,434 - INFO - joeynmt.helpers - delete word_level_model/26000.ckpt
2024-05-19 12:00:36,439 - INFO - joeynmt.training - Example #0
2024-05-19 12:00:36,440 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:00:36,440 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:00:36,440 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'a@@', 'positi@@', 've', 'così', 'che', 'la', 'di@@', 'mo@@', 'stra@@', 'zione', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'è', 'stato', 'la', 'dimen@@', 'sione', 'dei', '4@@', '8', 'st@@', 'ati', ',', 'ha', 's@@', 'par@@', 'so', 'dal', '4@@', '8', 'st@@', 'ati', ',', 'ha', 's@@', 'par@@', 'ito', 'dal', '4@@', '0', '%', '.', '</s>']
2024-05-19 12:00:36,441 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:00:36,442 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:00:36,442 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due diapositive così che la dimostrazione artica, che per la maggior parte degli ultimi tre milioni di anni è stato la dimensione dei 48 stati, ha sparso dal 48 stati, ha sparito dal 40%.
2024-05-19 12:00:36,442 - INFO - joeynmt.training - Example #1
2024-05-19 12:00:36,442 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:00:36,443 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:00:36,443 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'capi@@', 's@@', 'cono', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'il', 'po@@', 'll@@', 'o', '.', '</s>']
2024-05-19 12:00:36,443 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:00:36,444 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:00:36,444 - INFO - joeynmt.training - 	Hypothesis: Ma questa capiscono la seriamente di questo particolare problema perché non mostra il pollo.
2024-05-19 12:00:36,444 - INFO - joeynmt.training - Example #2
2024-05-19 12:00:36,445 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:00:36,445 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:00:36,445 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'gh@@', 'i@@', 'acci@@', 'o', 'ar@@', 't@@', 'ico', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:00:36,446 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:00:36,446 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:00:36,446 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio artico è, in un certo senso, il cuore del sistema clima globale.
2024-05-19 12:00:36,447 - INFO - joeynmt.training - Example #3
2024-05-19 12:00:36,447 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:00:36,447 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:00:36,447 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'p@@', 'and@@', 'e', 'nel', 'in@@', 'ver@@', 'no', 'e', 'cont@@', 'atto', 'in', 'est@@', 'o', '.', '</s>']
2024-05-19 12:00:36,448 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:00:36,448 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:00:36,449 - INFO - joeynmt.training - 	Hypothesis: Spande nel inverno e contatto in esto.
2024-05-19 12:00:36,449 - INFO - joeynmt.training - Example #4
2024-05-19 12:00:36,449 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:00:36,450 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:00:36,450 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'pro@@', 'ssi@@', 'mo', 's@@', 'li@@', 'de', 'che', 'sar@@', 'ete', 'un', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'do', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:00:36,451 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:00:36,451 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:00:36,451 - INFO - joeynmt.training - 	Hypothesis: Il prossimo slide che sarete un rapido rapido di ciò che è successo negli ultimi 25 anni.
2024-05-19 12:00:39,687 - INFO - joeynmt.training - Epoch   7, Step:    29100, Batch Loss:     1.200470, Batch Acc: 0.640287, Tokens per Sec:    21476, Lr: 0.000300
2024-05-19 12:00:42,939 - INFO - joeynmt.training - Epoch   7, Step:    29200, Batch Loss:     0.967098, Batch Acc: 0.648692, Tokens per Sec:    22154, Lr: 0.000300
2024-05-19 12:00:46,336 - INFO - joeynmt.training - Epoch   7, Step:    29300, Batch Loss:     1.442994, Batch Acc: 0.638577, Tokens per Sec:    21710, Lr: 0.000300
2024-05-19 12:00:50,578 - INFO - joeynmt.training - Epoch   7, Step:    29400, Batch Loss:     1.389521, Batch Acc: 0.639801, Tokens per Sec:    17596, Lr: 0.000300
2024-05-19 12:00:53,878 - INFO - joeynmt.training - Epoch   7, Step:    29500, Batch Loss:     1.242810, Batch Acc: 0.637514, Tokens per Sec:    22612, Lr: 0.000300
2024-05-19 12:00:53,879 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:00:53,879 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:01:05,854 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.39, ppl:   4.03, acc:   0.60, generation: 11.8671[sec], evaluation: 0.0000[sec]
2024-05-19 12:01:06,072 - INFO - joeynmt.helpers - delete word_level_model/27000.ckpt
2024-05-19 12:01:06,077 - INFO - joeynmt.training - Example #0
2024-05-19 12:01:06,078 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:01:06,079 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:01:06,079 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'a@@', 'positi@@', 've', 'così', 'che', 'la', 'di@@', 'mostr@@', 'a', 'che', 'la', 'maggi@@', 'or', 'ca@@', 'po', 'di', 'gh@@', 'i@@', 'acci@@', 'o', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'dei', '4@@', '0', '%', '.', '</s>']
2024-05-19 12:01:06,080 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:01:06,080 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:01:06,080 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due diapositive così che la dimostra che la maggior capo di ghiaccio, che per la maggior parte dei 40%.
2024-05-19 12:01:06,080 - INFO - joeynmt.training - Example #1
2024-05-19 12:01:06,081 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:01:06,081 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:01:06,081 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'capi@@', 'sc@@', 'e', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'il', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:01:06,082 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:01:06,082 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:01:06,082 - INFO - joeynmt.training - 	Hypothesis: Ma questa capisce la seriamente di questo particolare problema perché non mostra il ghiaccio.
2024-05-19 12:01:06,083 - INFO - joeynmt.training - Example #2
2024-05-19 12:01:06,083 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:01:06,083 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:01:06,083 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'gh@@', 'i@@', 'acci@@', 'a', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:01:06,084 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:01:06,084 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:01:06,085 - INFO - joeynmt.training - 	Hypothesis: La ghiaccia artica è, in un certo senso, il cuore cuore del sistema clima globale.
2024-05-19 12:01:06,085 - INFO - joeynmt.training - Example #3
2024-05-19 12:01:06,085 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:01:06,085 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:01:06,085 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'cont@@', 'ra@@', 'st@@', 'at@@', 'ore', '.', '</s>']
2024-05-19 12:01:06,086 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:01:06,086 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:01:06,087 - INFO - joeynmt.training - 	Hypothesis: Si espande in inverno e contrastatore.
2024-05-19 12:01:06,087 - INFO - joeynmt.training - Example #4
2024-05-19 12:01:06,087 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:01:06,087 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:01:06,088 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'di', 'cui', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'pi@@', 'do', 'velo@@', 'ce', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:01:06,088 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:01:06,088 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:01:06,089 - INFO - joeynmt.training - 	Hypothesis: La prossima diapositiva di cui vi mostrerà un pido veloce di quello che è successo negli ultimi 25 anni.
2024-05-19 12:01:09,478 - INFO - joeynmt.training - Epoch   7, Step:    29600, Batch Loss:     1.249361, Batch Acc: 0.635711, Tokens per Sec:    20352, Lr: 0.000300
2024-05-19 12:01:12,753 - INFO - joeynmt.training - Epoch   7, Step:    29700, Batch Loss:     1.238438, Batch Acc: 0.632110, Tokens per Sec:    22502, Lr: 0.000300
2024-05-19 12:01:16,631 - INFO - joeynmt.training - Epoch   7, Step:    29800, Batch Loss:     1.179454, Batch Acc: 0.633177, Tokens per Sec:    19684, Lr: 0.000300
2024-05-19 12:01:20,441 - INFO - joeynmt.training - Epoch   7, Step:    29900, Batch Loss:     1.187158, Batch Acc: 0.637650, Tokens per Sec:    19438, Lr: 0.000300
2024-05-19 12:01:23,705 - INFO - joeynmt.training - Epoch   7, Step:    30000, Batch Loss:     1.216101, Batch Acc: 0.633956, Tokens per Sec:    22360, Lr: 0.000300
2024-05-19 12:01:23,706 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:01:23,706 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:01:35,430 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.39, ppl:   4.01, acc:   0.60, generation: 11.6186[sec], evaluation: 0.0000[sec]
2024-05-19 12:01:35,431 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 12:01:35,657 - INFO - joeynmt.helpers - delete word_level_model/27500.ckpt
2024-05-19 12:01:35,668 - INFO - joeynmt.training - Example #0
2024-05-19 12:01:35,668 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:01:35,669 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:01:35,669 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 's@@', 'li@@', 'de', 'così', 'che', 'la', 'di@@', 'mostr@@', 'a', 'che', 'la', 'maggi@@', 'or', 'ca@@', 'po', 'per', 'la', 'maggi@@', 'or', 'parte', 'delle', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'dei', '4@@', '8', '%', ',', 'ha', 'con@@', 'divi@@', 'so', 'dei', '4@@', '0', '%', '.', '</s>']
2024-05-19 12:01:35,670 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:01:35,671 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:01:35,671 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due slide così che la dimostra che la maggior capo per la maggior parte delle tre milioni di anni è stata la dimensione dei 48%, ha condiviso dei 40%.
2024-05-19 12:01:35,671 - INFO - joeynmt.training - Example #1
2024-05-19 12:01:35,672 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:01:35,672 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:01:35,672 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'seri@@', 'amente', 'la', 'seri@@', 'amente', 'del', 'problema', 'perché', 'non', 'è', 'mostr@@', 'ato', 'il', 'po@@', 'll@@', 'o', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:01:35,673 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:01:35,673 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:01:35,673 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione seriamente la seriamente del problema perché non è mostrato il pollo del ghiaccio.
2024-05-19 12:01:35,674 - INFO - joeynmt.training - Example #2
2024-05-19 12:01:35,674 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:01:35,674 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:01:35,674 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'ca@@', 'p', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', '.', '</s>']
2024-05-19 12:01:35,675 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:01:35,675 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:01:35,676 - INFO - joeynmt.training - 	Hypothesis: La cap artica è, in un certo senso, il cuore cuore del sistema climatico.
2024-05-19 12:01:35,676 - INFO - joeynmt.training - Example #3
2024-05-19 12:01:35,677 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:01:35,677 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:01:35,677 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'em@@', 'br@@', 'a', 'in', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 12:01:35,678 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:01:35,678 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:01:35,678 - INFO - joeynmt.training - 	Hypothesis: Sembra in inverno e contratti in estate.
2024-05-19 12:01:35,678 - INFO - joeynmt.training - Example #4
2024-05-19 12:01:35,679 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:01:35,679 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:01:35,679 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'da', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:01:35,680 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:01:35,680 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:01:35,680 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerà un rapido rapida di ciò che è successo negli ultimi 25 anni.
2024-05-19 12:01:39,028 - INFO - joeynmt.training - Epoch   7, Step:    30100, Batch Loss:     1.311004, Batch Acc: 0.632785, Tokens per Sec:    21038, Lr: 0.000300
2024-05-19 12:01:42,370 - INFO - joeynmt.training - Epoch   7, Step:    30200, Batch Loss:     1.190921, Batch Acc: 0.640863, Tokens per Sec:    22701, Lr: 0.000300
2024-05-19 12:01:46,592 - INFO - joeynmt.training - Epoch   7, Step:    30300, Batch Loss:     1.251582, Batch Acc: 0.641360, Tokens per Sec:    17086, Lr: 0.000300
2024-05-19 12:01:49,939 - INFO - joeynmt.training - Epoch   7, Step:    30400, Batch Loss:     1.196764, Batch Acc: 0.642026, Tokens per Sec:    22628, Lr: 0.000300
2024-05-19 12:01:53,223 - INFO - joeynmt.training - Epoch   7, Step:    30500, Batch Loss:     1.298551, Batch Acc: 0.637722, Tokens per Sec:    22707, Lr: 0.000300
2024-05-19 12:01:53,224 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:01:53,224 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:02:05,414 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.39, ppl:   4.03, acc:   0.60, generation: 12.0829[sec], evaluation: 0.0000[sec]
2024-05-19 12:02:05,633 - INFO - joeynmt.helpers - delete word_level_model/29000.ckpt
2024-05-19 12:02:05,643 - INFO - joeynmt.training - Example #0
2024-05-19 12:02:05,644 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:02:05,644 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:02:05,644 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'o', 'sc@@', 'or@@', 'so', 'anno', 'sc@@', 'ar@@', 'ic@@', 'ato', 'queste', 'due', 'di@@', 'a@@', 'positi@@', 've', 'così', 'che', 'la', 'ca@@', 'po', 'ar@@', 't@@', 'ica', 'gh@@', 'i@@', 'acci@@', 'o', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'dei', '4@@', '8', '%', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'dei', '4@@', '8', '%', '.', '</s>']
2024-05-19 12:02:05,645 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:02:05,646 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:02:05,646 - INFO - joeynmt.training - 	Hypothesis: Lo scorso anno scaricato queste due diapositive così che la capo artica ghiaccio che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dei 48% di anni è stata la dimensione dei 48%.
2024-05-19 12:02:05,646 - INFO - joeynmt.training - Example #1
2024-05-19 12:02:05,647 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:02:05,647 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:02:05,647 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'com@@', 'pren@@', 'de', 'la', 'seri@@', 'amente', 'del', 'tutto', 'questo', 'problema', 'problema', 'perché', 'non', 'ha', 'mostr@@', 'ato', 'il', 'gh@@', 'i@@', 'acci@@', 'o', 'della', 'gh@@', 'i@@', 'acci@@', 'a', '.', '</s>']
2024-05-19 12:02:05,648 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:02:05,648 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:02:05,648 - INFO - joeynmt.training - 	Hypothesis: Ma questo comprende la seriamente del tutto questo problema problema perché non ha mostrato il ghiaccio della ghiaccia.
2024-05-19 12:02:05,648 - INFO - joeynmt.training - Example #2
2024-05-19 12:02:05,649 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:02:05,649 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:02:05,649 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'gh@@', 'i@@', 'acci@@', 'o', 'ar@@', 't@@', 'ico', 'è', ',', 'in', 'un', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:02:05,650 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:02:05,650 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:02:05,650 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio artico è, in un senso, il cuore cuore del sistema clima globale.
2024-05-19 12:02:05,651 - INFO - joeynmt.training - Example #3
2024-05-19 12:02:05,651 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:02:05,651 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:02:05,651 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'ar@@', 'g@@', 'a', 'in', 'in@@', 'ver@@', 'no', 'e', 'cont@@', 'ra@@', 'st@@', 'at@@', 'ore', '.', '</s>']
2024-05-19 12:02:05,652 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:02:05,652 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:02:05,652 - INFO - joeynmt.training - 	Hypothesis: Sarga in inverno e contrastatore.
2024-05-19 12:02:05,653 - INFO - joeynmt.training - Example #4
2024-05-19 12:02:05,653 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:02:05,653 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:02:05,653 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'velo@@', 'ce', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:02:05,654 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:02:05,654 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:02:05,655 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerà un rapido veloce di quello che è successo negli ultimi 25 anni.
2024-05-19 12:02:09,052 - INFO - joeynmt.training - Epoch   7, Step:    30600, Batch Loss:     1.202725, Batch Acc: 0.638732, Tokens per Sec:    19954, Lr: 0.000300
2024-05-19 12:02:12,851 - INFO - joeynmt.training - Epoch   7, Step:    30700, Batch Loss:     1.230603, Batch Acc: 0.632866, Tokens per Sec:    19760, Lr: 0.000300
2024-05-19 12:02:16,755 - INFO - joeynmt.training - Epoch   7, Step:    30800, Batch Loss:     1.164800, Batch Acc: 0.633267, Tokens per Sec:    18909, Lr: 0.000300
2024-05-19 12:02:20,071 - INFO - joeynmt.training - Epoch   7, Step:    30900, Batch Loss:     1.139865, Batch Acc: 0.635581, Tokens per Sec:    22451, Lr: 0.000300
2024-05-19 12:02:23,284 - INFO - joeynmt.training - Epoch   7, Step:    31000, Batch Loss:     1.333707, Batch Acc: 0.631393, Tokens per Sec:    23605, Lr: 0.000300
2024-05-19 12:02:23,285 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:02:23,285 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:02:36,672 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.39, ppl:   4.01, acc:   0.60, generation: 13.2755[sec], evaluation: 0.0000[sec]
2024-05-19 12:02:36,673 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 12:02:36,902 - INFO - joeynmt.helpers - delete word_level_model/30500.ckpt
2024-05-19 12:02:36,912 - INFO - joeynmt.training - Example #0
2024-05-19 12:02:36,913 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:02:36,913 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:02:36,913 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'a@@', 'positi@@', 've', 'così', 'che', 'di@@', 'mostr@@', 'ano', 'che', 'il', 'gh@@', 'i@@', 'acci@@', 'o', 'ar@@', 't@@', 'ico', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'dei', '4@@', '8', 'milioni', 'di', 'anni', 'è', 'stato', 'la', 'dimen@@', 'sione', 'dei', '4@@', '0', '%', '.', '</s>']
2024-05-19 12:02:36,914 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:02:36,915 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:02:36,915 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due diapositive così che dimostrano che il ghiaccio artico, che per la maggior parte dei 48 milioni di anni è stato la dimensione dei 40%.
2024-05-19 12:02:36,916 - INFO - joeynmt.training - Example #1
2024-05-19 12:02:36,916 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:02:36,916 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:02:36,916 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'ser@@', 'ia', 'la', 'seri@@', 'amente', 'di', 'questo', 'problema', 'perché', 'non', 'mostr@@', 'a', 'la', 'v@@', 'end@@', 'enza', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:02:36,917 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:02:36,917 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:02:36,917 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione seria la seriamente di questo problema perché non mostra la vendenza del ghiaccio.
2024-05-19 12:02:36,918 - INFO - joeynmt.training - Example #2
2024-05-19 12:02:36,918 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:02:36,918 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:02:36,918 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'ca@@', 'p', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', '.', '</s>']
2024-05-19 12:02:36,919 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:02:36,919 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:02:36,920 - INFO - joeynmt.training - 	Hypothesis: La cap artica è, in un senso, il cuore cuore del sistema climatico.
2024-05-19 12:02:36,920 - INFO - joeynmt.training - Example #3
2024-05-19 12:02:36,920 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:02:36,920 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:02:36,921 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 12:02:36,921 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:02:36,921 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:02:36,922 - INFO - joeynmt.training - 	Hypothesis: Si espande in inverno e contratti in estate.
2024-05-19 12:02:36,922 - INFO - joeynmt.training - Example #4
2024-05-19 12:02:36,922 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:02:36,922 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:02:36,923 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'velo@@', 'c@@', 'ità', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:02:36,923 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:02:36,923 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:02:36,924 - INFO - joeynmt.training - 	Hypothesis: La prossima diapositiva che vi mostrerà un rapido velocità di quello che è successo negli ultimi 25 anni.
2024-05-19 12:02:40,310 - INFO - joeynmt.training - Epoch   7, Step:    31100, Batch Loss:     1.265014, Batch Acc: 0.628242, Tokens per Sec:    20360, Lr: 0.000300
2024-05-19 12:02:44,538 - INFO - joeynmt.training - Epoch   7, Step:    31200, Batch Loss:     1.240724, Batch Acc: 0.633382, Tokens per Sec:    17175, Lr: 0.000300
2024-05-19 12:02:47,964 - INFO - joeynmt.training - Epoch   7, Step:    31300, Batch Loss:     1.317277, Batch Acc: 0.631122, Tokens per Sec:    21975, Lr: 0.000300
2024-05-19 12:02:51,455 - INFO - joeynmt.training - Epoch   7, Step:    31400, Batch Loss:     1.239056, Batch Acc: 0.633568, Tokens per Sec:    21062, Lr: 0.000300
2024-05-19 12:02:55,061 - INFO - joeynmt.training - Epoch   7, Step:    31500, Batch Loss:     1.140834, Batch Acc: 0.634651, Tokens per Sec:    20716, Lr: 0.000300
2024-05-19 12:02:55,062 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:02:55,062 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:03:07,808 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.39, ppl:   4.00, acc:   0.60, generation: 12.6335[sec], evaluation: 0.0000[sec]
2024-05-19 12:03:07,809 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 12:03:08,061 - INFO - joeynmt.helpers - delete word_level_model/29500.ckpt
2024-05-19 12:03:08,066 - INFO - joeynmt.training - Example #0
2024-05-19 12:03:08,067 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:03:08,068 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:03:08,068 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'a@@', 'positi@@', 've', 'così', 'da', 'di@@', 'mostr@@', 'are', 'che', 'il', 'gh@@', 'i@@', 'acci@@', 'o', 'sc@@', 'rit@@', 'ico', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'dei', '4@@', '8', '%', '.', '</s>']
2024-05-19 12:03:08,069 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:03:08,069 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:03:08,070 - INFO - joeynmt.training - 	Hypothesis: L'anno ho mostrato queste due diapositive così da dimostrare che il ghiaccio scritico, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dei 48%.
2024-05-19 12:03:08,070 - INFO - joeynmt.training - Example #1
2024-05-19 12:03:08,071 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:03:08,071 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:03:08,071 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', 'sc@@', 'e', 'la', 'seri@@', 'amente', 'la', 'seri@@', 'amente', 'del', 'gh@@', 'i@@', 'acci@@', 'o', 'perché', 'non', 'ha', 'mostr@@', 'ato', 'il', 'più', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:03:08,072 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:03:08,072 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:03:08,072 - INFO - joeynmt.training - 	Hypothesis: Ma questo capisce la seriamente la seriamente del ghiaccio perché non ha mostrato il più ghiaccio.
2024-05-19 12:03:08,073 - INFO - joeynmt.training - Example #2
2024-05-19 12:03:08,073 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:03:08,073 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:03:08,073 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'gh@@', 'i@@', 'acci@@', 'o', 'ar@@', 't@@', 'ico', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'mio', 'cu@@', 'ore', 'ar@@', 't@@', 'ico', 'è', 'il', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:03:08,074 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:03:08,074 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:03:08,075 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio artico è, in un certo senso, il mio cuore artico è il sistema clima globale.
2024-05-19 12:03:08,075 - INFO - joeynmt.training - Example #3
2024-05-19 12:03:08,075 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:03:08,076 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:03:08,076 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'o@@', 'st@@', 'e', 'in', 'cont@@', 'atto', 'in', 'est@@', 'at@@', 'ore', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 12:03:08,076 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:03:08,077 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:03:08,077 - INFO - joeynmt.training - 	Hypothesis: Soste in contatto in estatore e contratti in estate.
2024-05-19 12:03:08,077 - INFO - joeynmt.training - Example #4
2024-05-19 12:03:08,078 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:03:08,078 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:03:08,078 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'velo@@', 'c@@', 'emente', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:03:08,079 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:03:08,079 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:03:08,079 - INFO - joeynmt.training - 	Hypothesis: La prossima diapositiva che vi mostrerà un rapido velocemente di quello che è successo negli ultimi 25 anni.
2024-05-19 12:03:12,786 - INFO - joeynmt.training - Epoch   7, Step:    31600, Batch Loss:     1.381861, Batch Acc: 0.626693, Tokens per Sec:    15192, Lr: 0.000300
2024-05-19 12:03:16,426 - INFO - joeynmt.training - Epoch   7, Step:    31700, Batch Loss:     1.356871, Batch Acc: 0.635625, Tokens per Sec:    20868, Lr: 0.000300
2024-05-19 12:03:19,877 - INFO - joeynmt.training - Epoch   7, Step:    31800, Batch Loss:     1.294281, Batch Acc: 0.632412, Tokens per Sec:    21657, Lr: 0.000300
2024-05-19 12:03:23,231 - INFO - joeynmt.training - Epoch   7, Step:    31900, Batch Loss:     1.240453, Batch Acc: 0.636066, Tokens per Sec:    21620, Lr: 0.000300
2024-05-19 12:03:27,613 - INFO - joeynmt.training - Epoch   7, Step:    32000, Batch Loss:     1.195342, Batch Acc: 0.630492, Tokens per Sec:    17021, Lr: 0.000300
2024-05-19 12:03:27,614 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:03:27,614 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:03:39,289 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.38, ppl:   3.96, acc:   0.60, generation: 11.4641[sec], evaluation: 0.0000[sec]
2024-05-19 12:03:39,291 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 12:03:39,561 - INFO - joeynmt.helpers - delete word_level_model/28000.ckpt
2024-05-19 12:03:39,575 - INFO - joeynmt.training - Example #0
2024-05-19 12:03:39,577 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:03:39,577 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:03:39,577 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 's@@', 'li@@', 'de', 'così', 'che', 'di@@', 'mostr@@', 'ano', 'che', 'la', 'cosa', 'ar@@', 't@@', 'ica', 'il', 'gh@@', 'i@@', 'acci@@', 'o', 't@@', 'ot@@', 'ale', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'sono', 'state', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', '%', '.', '</s>']
2024-05-19 12:03:39,578 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:03:39,579 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:03:39,580 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due slide così che dimostrano che la cosa artica il ghiaccio totale, che per la maggior parte degli ultimi tre milioni di anni sono state le dimensioni dei 48%.
2024-05-19 12:03:39,580 - INFO - joeynmt.training - Example #1
2024-05-19 12:03:39,580 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:03:39,580 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:03:39,581 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'la', 'seri@@', 'amente', 'ser@@', 'io', 'del', 'gh@@', 'i@@', 'acci@@', 'o', 'perché', 'non', 'mostr@@', 'a', 'la', 't@@', 'ot@@', 'ale', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:03:39,581 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:03:39,582 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:03:39,582 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione la seriamente serio del ghiaccio perché non mostra la totale del ghiaccio.
2024-05-19 12:03:39,582 - INFO - joeynmt.training - Example #2
2024-05-19 12:03:39,583 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:03:39,583 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:03:39,583 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'gh@@', 'i@@', 'acci@@', 'o', 'ar@@', 't@@', 'ico', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'del', 'sistema', 'c@@', 'li@@', 'ma', '.', '</s>']
2024-05-19 12:03:39,584 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:03:39,584 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:03:39,584 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio artico è, in un certo senso, il cuore del sistema clima del sistema clima.
2024-05-19 12:03:39,584 - INFO - joeynmt.training - Example #3
2024-05-19 12:03:39,585 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:03:39,585 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:03:39,585 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'il', 'contr@@', 'atto', 'in', 'est@@', 'i@@', 'vo', '.', '</s>']
2024-05-19 12:03:39,586 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:03:39,586 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:03:39,586 - INFO - joeynmt.training - 	Hypothesis: Si espande in inverno e il contratto in estivo.
2024-05-19 12:03:39,586 - INFO - joeynmt.training - Example #4
2024-05-19 12:03:39,587 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:03:39,587 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:03:39,587 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'di', 'quello', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'da', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:03:39,588 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:03:39,588 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:03:39,588 - INFO - joeynmt.training - 	Hypothesis: La prossima diapositiva di quello che vi mostrerà un rapido rapida di quello che è successo negli ultimi 25 anni.
2024-05-19 12:03:43,689 - INFO - joeynmt.training - Epoch   7, Step:    32100, Batch Loss:     1.109784, Batch Acc: 0.630788, Tokens per Sec:    16881, Lr: 0.000300
2024-05-19 12:03:47,102 - INFO - joeynmt.training - Epoch   7, Step:    32200, Batch Loss:     1.399995, Batch Acc: 0.630598, Tokens per Sec:    21634, Lr: 0.000300
2024-05-19 12:03:50,492 - INFO - joeynmt.training - Epoch   7, Step:    32300, Batch Loss:     1.052085, Batch Acc: 0.631500, Tokens per Sec:    21959, Lr: 0.000300
2024-05-19 12:03:54,424 - INFO - joeynmt.training - Epoch   7, Step:    32400, Batch Loss:     1.202075, Batch Acc: 0.635690, Tokens per Sec:    18805, Lr: 0.000300
2024-05-19 12:03:58,330 - INFO - joeynmt.training - Epoch   7, Step:    32500, Batch Loss:     1.351365, Batch Acc: 0.629880, Tokens per Sec:    19413, Lr: 0.000300
2024-05-19 12:03:58,331 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:03:58,331 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:04:10,680 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.38, ppl:   3.96, acc:   0.60, generation: 12.1451[sec], evaluation: 0.0000[sec]
2024-05-19 12:04:10,681 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 12:04:10,939 - INFO - joeynmt.helpers - delete word_level_model/28500.ckpt
2024-05-19 12:04:10,949 - INFO - joeynmt.training - Example #0
2024-05-19 12:04:10,950 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:04:10,951 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:04:10,951 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'mostr@@', 'are', 'queste', 'due', 'di@@', 'a@@', 'positi@@', 've', 'così', 'che', 'di@@', 'mostr@@', 'ano', 'che', 'la', 'maggi@@', 'or', 'ca@@', 'po', 'di', 'sc@@', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'dei', '4@@', '8', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'dei', '4@@', '0', '%', 'di', 'circa', 'il', '4@@', '0', '%', '.', '</s>']
2024-05-19 12:04:10,951 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:04:10,952 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:04:10,952 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso mostrare queste due diapositive così che dimostrano che la maggior capo di scartica, che per la maggior parte dei 48 anni è stata la dimensione dei 40% di circa il 40%.
2024-05-19 12:04:10,952 - INFO - joeynmt.training - Example #1
2024-05-19 12:04:10,953 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:04:10,953 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:04:10,953 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'seri@@', 'amente', 'la', 'seri@@', 'amente', 'del', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'la', 'po@@', 'll@@', 'a', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:04:10,954 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:04:10,954 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:04:10,954 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione seriamente la seriamente del questo particolare problema perché non mostra la polla del ghiaccio.
2024-05-19 12:04:10,955 - INFO - joeynmt.training - Example #2
2024-05-19 12:04:10,955 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:04:10,955 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:04:10,955 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'gh@@', 'i@@', 'acci@@', 'o', 'ar@@', 't@@', 'ico', 'è', ',', 'in', 'un', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:04:10,956 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:04:10,956 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:04:10,957 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio artico è, in un senso, il cuore del sistema clima globale.
2024-05-19 12:04:10,957 - INFO - joeynmt.training - Example #3
2024-05-19 12:04:10,957 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:04:10,957 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:04:10,958 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'st@@', 'im@@', 'a', 'nel', 'in@@', 'ver@@', 'no', 'e', 'il', 'contr@@', 'atto', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 12:04:10,958 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:04:10,958 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:04:10,959 - INFO - joeynmt.training - 	Hypothesis: Si stima nel inverno e il contratto in estate.
2024-05-19 12:04:10,959 - INFO - joeynmt.training - Example #4
2024-05-19 12:04:10,959 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:04:10,960 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:04:10,960 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'do', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:04:10,960 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:04:10,961 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:04:10,961 - INFO - joeynmt.training - 	Hypothesis: La prossima diapositiva che vi mostrerà un rapido rapido di ciò che è successo negli ultimi 25 anni.
2024-05-19 12:04:14,430 - INFO - joeynmt.training - Epoch   7, Step:    32600, Batch Loss:     1.163375, Batch Acc: 0.634270, Tokens per Sec:    20445, Lr: 0.000300
2024-05-19 12:04:17,823 - INFO - joeynmt.training - Epoch   7, Step:    32700, Batch Loss:     1.296650, Batch Acc: 0.630550, Tokens per Sec:    21589, Lr: 0.000300
2024-05-19 12:04:21,407 - INFO - joeynmt.training - Epoch   7, Step:    32800, Batch Loss:     1.536171, Batch Acc: 0.629822, Tokens per Sec:    20717, Lr: 0.000300
2024-05-19 12:04:25,796 - INFO - joeynmt.training - Epoch   7, Step:    32900, Batch Loss:     1.415369, Batch Acc: 0.630106, Tokens per Sec:    17568, Lr: 0.000300
2024-05-19 12:04:29,261 - INFO - joeynmt.training - Epoch   7, Step:    33000, Batch Loss:     1.040369, Batch Acc: 0.635225, Tokens per Sec:    21201, Lr: 0.000300
2024-05-19 12:04:29,262 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:04:29,262 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:04:41,970 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.37, ppl:   3.95, acc:   0.61, generation: 12.6016[sec], evaluation: 0.0000[sec]
2024-05-19 12:04:41,971 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 12:04:42,214 - INFO - joeynmt.helpers - delete word_level_model/30000.ckpt
2024-05-19 12:04:42,225 - INFO - joeynmt.training - Example #0
2024-05-19 12:04:42,226 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:04:42,226 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:04:42,227 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'questi', 'due', 'di@@', 'mostr@@', 'i', 'così', 'che', 'la', 'di@@', 'mostr@@', 'a', 'che', 'la', 'maggi@@', 'or', 'parte', 'delle', 't@@', 'ico', 'gh@@', 'i@@', 'acci@@', 'o', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'dei', '4@@', '8', 'anni', 'è', 'stato', 'la', 'dimen@@', 'sione', 'dei', '4@@', '8', 'st@@', 'ati', ',', 'ha', 's@@', 'chi@@', 'f@@', 'o', 'da', '4@@', '8', '%', '.', '</s>']
2024-05-19 12:04:42,228 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:04:42,228 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:04:42,229 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato questi due dimostri così che la dimostra che la maggior parte delle tico ghiaccio, che per la maggior parte dei 48 anni è stato la dimensione dei 48 stati, ha schifo da 48%.
2024-05-19 12:04:42,229 - INFO - joeynmt.training - Example #1
2024-05-19 12:04:42,230 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:04:42,230 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:04:42,230 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'ser@@', 'ia', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'la', 'po@@', 'll@@', 'a', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:04:42,231 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:04:42,231 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:04:42,231 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione seria la seriamente di questo particolare problema perché non mostra la polla del ghiaccio.
2024-05-19 12:04:42,231 - INFO - joeynmt.training - Example #2
2024-05-19 12:04:42,232 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:04:42,232 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:04:42,232 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'gh@@', 'i@@', 'acci@@', 'o', 'ar@@', 't@@', 'ico', 'è', ',', 'in', 'un', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:04:42,233 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:04:42,233 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:04:42,233 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio artico è, in un senso, il cuore del sistema clima, il cuore del sistema clima globale.
2024-05-19 12:04:42,234 - INFO - joeynmt.training - Example #3
2024-05-19 12:04:42,234 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:04:42,234 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:04:42,234 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'st@@', 'im@@', 'a', 'in', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 12:04:42,235 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:04:42,235 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:04:42,235 - INFO - joeynmt.training - 	Hypothesis: Si stima in inverno e contratti in estate.
2024-05-19 12:04:42,236 - INFO - joeynmt.training - Example #4
2024-05-19 12:04:42,236 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:04:42,236 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:04:42,236 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'da', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:04:42,237 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:04:42,237 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:04:42,238 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerà un rapida di ciò che è successo negli ultimi 25 anni.
2024-05-19 12:04:45,538 - INFO - joeynmt.training - Epoch   7, Step:    33100, Batch Loss:     1.283040, Batch Acc: 0.627925, Tokens per Sec:    20681, Lr: 0.000300
2024-05-19 12:04:48,930 - INFO - joeynmt.training - Epoch   7, Step:    33200, Batch Loss:     1.223434, Batch Acc: 0.631011, Tokens per Sec:    21936, Lr: 0.000300
2024-05-19 12:04:53,176 - INFO - joeynmt.training - Epoch   7, Step:    33300, Batch Loss:     1.197299, Batch Acc: 0.626263, Tokens per Sec:    17406, Lr: 0.000300
2024-05-19 12:04:56,632 - INFO - joeynmt.training - Epoch   7, Step:    33400, Batch Loss:     1.171237, Batch Acc: 0.633396, Tokens per Sec:    21661, Lr: 0.000300
2024-05-19 12:04:59,916 - INFO - joeynmt.training - Epoch   7, Step:    33500, Batch Loss:     1.372079, Batch Acc: 0.639473, Tokens per Sec:    23077, Lr: 0.000300
2024-05-19 12:04:59,917 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:04:59,917 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:05:12,515 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.37, ppl:   3.92, acc:   0.61, generation: 12.4864[sec], evaluation: 0.0000[sec]
2024-05-19 12:05:12,515 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 12:05:12,758 - INFO - joeynmt.helpers - delete word_level_model/31000.ckpt
2024-05-19 12:05:12,768 - INFO - joeynmt.training - Example #0
2024-05-19 12:05:12,769 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:05:12,769 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:05:12,769 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'a@@', 'positi@@', 've', 'così', 'che', 'la', 'macch@@', 'ina', 'gh@@', 'i@@', 'acci@@', 'a', 't@@', 'ica', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'dei', '4@@', '8', 'st@@', 'ati', ',', 'ha', 's@@', 'chi@@', 'f@@', 'o', 'di', '4@@', '8', 'st@@', 'ati', ',', 'ha', 's@@', 'chi@@', 'f@@', 'o', 'del', '4@@', '0', '%', '.', '</s>']
2024-05-19 12:05:12,771 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:05:12,771 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:05:12,771 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due diapositive così che la macchina ghiaccia tica che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dei 48 stati, ha schifo di 48 stati, ha schifo del 40%.
2024-05-19 12:05:12,771 - INFO - joeynmt.training - Example #1
2024-05-19 12:05:12,772 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:05:12,772 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:05:12,772 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'seri@@', 'amente', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'la', 'più', 'po@@', 'll@@', 'a', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:05:12,773 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:05:12,773 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:05:12,773 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione seriamente la seriamente di questo particolare problema perché non mostra la più polla del ghiaccio.
2024-05-19 12:05:12,773 - INFO - joeynmt.training - Example #2
2024-05-19 12:05:12,774 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:05:12,774 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:05:12,774 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'g@@', 'am@@', 'ma', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:05:12,775 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:05:12,775 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:05:12,775 - INFO - joeynmt.training - 	Hypothesis: La gamma artica è, in un certo senso, il cuore cuore del sistema clima globale.
2024-05-19 12:05:12,775 - INFO - joeynmt.training - Example #3
2024-05-19 12:05:12,776 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:05:12,776 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:05:12,776 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&@@', 'a@@', 'pos@@', ';', 'in', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'ate', 'e', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 12:05:12,776 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:05:12,777 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:05:12,777 - INFO - joeynmt.training - 	Hypothesis: E 'in inverno e contratti in estate e in estate.
2024-05-19 12:05:12,777 - INFO - joeynmt.training - Example #4
2024-05-19 12:05:12,777 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:05:12,777 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:05:12,778 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'di', 'quello', 'che', 'è', 'ac@@', 'ca@@', 'd@@', 'ut@@', 'a', 'di', 'ciò', 'che', 'è', 'ac@@', 'ca@@', 'd@@', 'ut@@', 'a', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:05:12,778 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:05:12,778 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:05:12,779 - INFO - joeynmt.training - 	Hypothesis: La prossima prossima diapositiva di quello che è accaduta di ciò che è accaduta negli ultimi 25 anni.
2024-05-19 12:05:14,774 - INFO - joeynmt.training - Epoch   7: total training loss 6015.05
2024-05-19 12:05:14,774 - INFO - joeynmt.training - EPOCH 8
2024-05-19 12:05:16,016 - INFO - joeynmt.training - Epoch   8, Step:    33600, Batch Loss:     1.012890, Batch Acc: 0.656889, Tokens per Sec:    23622, Lr: 0.000300
2024-05-19 12:05:19,831 - INFO - joeynmt.training - Epoch   8, Step:    33700, Batch Loss:     1.207516, Batch Acc: 0.649376, Tokens per Sec:    19785, Lr: 0.000300
2024-05-19 12:05:23,728 - INFO - joeynmt.training - Epoch   8, Step:    33800, Batch Loss:     1.249525, Batch Acc: 0.644861, Tokens per Sec:    19176, Lr: 0.000300
2024-05-19 12:05:26,964 - INFO - joeynmt.training - Epoch   8, Step:    33900, Batch Loss:     1.240849, Batch Acc: 0.649671, Tokens per Sec:    23112, Lr: 0.000300
2024-05-19 12:05:30,191 - INFO - joeynmt.training - Epoch   8, Step:    34000, Batch Loss:     1.189682, Batch Acc: 0.653846, Tokens per Sec:    23480, Lr: 0.000300
2024-05-19 12:05:30,192 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:05:30,192 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:05:42,306 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.38, ppl:   3.96, acc:   0.61, generation: 11.9959[sec], evaluation: 0.0000[sec]
2024-05-19 12:05:42,525 - INFO - joeynmt.helpers - delete word_level_model/31500.ckpt
2024-05-19 12:05:42,530 - INFO - joeynmt.training - Example #0
2024-05-19 12:05:42,531 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:05:42,531 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:05:42,532 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'a@@', 'positi@@', 've', ',', 'così', 'da', 'di@@', 'mostr@@', 'are', 'che', 'la', 'macch@@', 'ina', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'dei', '4@@', '0', '%', '.', '</s>']
2024-05-19 12:05:42,532 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:05:42,533 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:05:42,533 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due diapositive, così da dimostrare che la macchina artica, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dei 40%.
2024-05-19 12:05:42,533 - INFO - joeynmt.training - Example #1
2024-05-19 12:05:42,534 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:05:42,534 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:05:42,534 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'ser@@', 'ia', 'la', 'seri@@', 'et@@', 'à', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'la', 'po@@', 'll@@', 'a', 'della', 'gh@@', 'i@@', 'acci@@', 'a', '.', '</s>']
2024-05-19 12:05:42,535 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:05:42,535 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:05:42,535 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione seria la serietà di questo particolare problema perché non mostra la polla della ghiaccia.
2024-05-19 12:05:42,535 - INFO - joeynmt.training - Example #2
2024-05-19 12:05:42,536 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:05:42,536 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:05:42,536 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'g@@', 'am@@', 'ma', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:05:42,537 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:05:42,537 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:05:42,537 - INFO - joeynmt.training - 	Hypothesis: La gamma artica è, in un certo senso, il cuore cuore del sistema clima globale.
2024-05-19 12:05:42,537 - INFO - joeynmt.training - Example #3
2024-05-19 12:05:42,538 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:05:42,538 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:05:42,538 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'cont@@', 'atto', 'in', 'est@@', 'ate', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 12:05:42,539 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:05:42,539 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:05:42,539 - INFO - joeynmt.training - 	Hypothesis: Si espande in inverno e contatto in estate e contratti in estate.
2024-05-19 12:05:42,539 - INFO - joeynmt.training - Example #4
2024-05-19 12:05:42,540 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:05:42,540 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:05:42,540 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'da', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:05:42,541 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:05:42,541 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:05:42,541 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerà un rapido rapida di ciò che è successo negli ultimi 25 anni.
2024-05-19 12:05:45,817 - INFO - joeynmt.training - Epoch   8, Step:    34100, Batch Loss:     1.127425, Batch Acc: 0.649730, Tokens per Sec:    21210, Lr: 0.000300
2024-05-19 12:05:50,131 - INFO - joeynmt.training - Epoch   8, Step:    34200, Batch Loss:     1.149781, Batch Acc: 0.651461, Tokens per Sec:    17055, Lr: 0.000300
2024-05-19 12:05:53,511 - INFO - joeynmt.training - Epoch   8, Step:    34300, Batch Loss:     1.315470, Batch Acc: 0.648940, Tokens per Sec:    22376, Lr: 0.000300
2024-05-19 12:05:56,848 - INFO - joeynmt.training - Epoch   8, Step:    34400, Batch Loss:     1.153287, Batch Acc: 0.652465, Tokens per Sec:    23017, Lr: 0.000300
2024-05-19 12:06:00,133 - INFO - joeynmt.training - Epoch   8, Step:    34500, Batch Loss:     1.087567, Batch Acc: 0.642217, Tokens per Sec:    22417, Lr: 0.000300
2024-05-19 12:06:00,133 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:06:00,134 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:06:12,432 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.37, ppl:   3.95, acc:   0.61, generation: 12.1834[sec], evaluation: 0.0000[sec]
2024-05-19 12:06:12,646 - INFO - joeynmt.helpers - delete word_level_model/32000.ckpt
2024-05-19 12:06:12,656 - INFO - joeynmt.training - Example #0
2024-05-19 12:06:12,657 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:06:12,657 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:06:12,658 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'a@@', 'positi@@', 've', 'così', 'che', 'la', 'di@@', 'mostr@@', 'a', 'che', 'il', 'gh@@', 'i@@', 'acci@@', 'o', 'ar@@', 't@@', 'ico', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'sono', 'state', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', '%', '.', '</s>']
2024-05-19 12:06:12,658 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:06:12,659 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:06:12,659 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due diapositive così che la dimostra che il ghiaccio artico che per la maggior parte degli ultimi tre milioni di anni sono state le dimensioni dei 48%.
2024-05-19 12:06:12,659 - INFO - joeynmt.training - Example #1
2024-05-19 12:06:12,660 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:06:12,660 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:06:12,660 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'seri@@', 'amente', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'la', 'po@@', 'll@@', 'enza', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:06:12,661 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:06:12,661 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:06:12,661 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione seriamente la seriamente di questo particolare problema perché non mostra la pollenza del ghiaccio.
2024-05-19 12:06:12,662 - INFO - joeynmt.training - Example #2
2024-05-19 12:06:12,662 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:06:12,662 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:06:12,662 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'gh@@', 'i@@', 'acci@@', 'o', 'ar@@', 't@@', 'ico', 'è', ',', 'in', 'un', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', '.', '</s>']
2024-05-19 12:06:12,663 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:06:12,663 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:06:12,663 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio artico è, in un senso, il cuore cuore del sistema clima.
2024-05-19 12:06:12,663 - INFO - joeynmt.training - Example #3
2024-05-19 12:06:12,664 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:06:12,664 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:06:12,664 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', 'p@@', 'and@@', 'e', 'nel', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 12:06:12,665 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:06:12,665 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:06:12,665 - INFO - joeynmt.training - 	Hypothesis: Si espande nel inverno e contratti in estate.
2024-05-19 12:06:12,665 - INFO - joeynmt.training - Example #4
2024-05-19 12:06:12,666 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:06:12,666 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:06:12,666 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'av@@', 'anti', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:06:12,666 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:06:12,667 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:06:12,667 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerà un rapido avanti di ciò che è successo negli ultimi 25 anni.
2024-05-19 12:06:16,405 - INFO - joeynmt.training - Epoch   8, Step:    34600, Batch Loss:     1.263117, Batch Acc: 0.652266, Tokens per Sec:    18686, Lr: 0.000300
2024-05-19 12:06:20,310 - INFO - joeynmt.training - Epoch   8, Step:    34700, Batch Loss:     1.183323, Batch Acc: 0.648688, Tokens per Sec:    19465, Lr: 0.000300
2024-05-19 12:06:23,600 - INFO - joeynmt.training - Epoch   8, Step:    34800, Batch Loss:     1.197437, Batch Acc: 0.645230, Tokens per Sec:    22806, Lr: 0.000300
2024-05-19 12:06:26,848 - INFO - joeynmt.training - Epoch   8, Step:    34900, Batch Loss:     1.207455, Batch Acc: 0.636257, Tokens per Sec:    23040, Lr: 0.000300
2024-05-19 12:06:30,565 - INFO - joeynmt.training - Epoch   8, Step:    35000, Batch Loss:     1.403097, Batch Acc: 0.644070, Tokens per Sec:    20220, Lr: 0.000300
2024-05-19 12:06:30,565 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:06:30,565 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:06:42,083 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.37, ppl:   3.94, acc:   0.61, generation: 11.4116[sec], evaluation: 0.0000[sec]
2024-05-19 12:06:42,307 - INFO - joeynmt.helpers - delete word_level_model/34000.ckpt
2024-05-19 12:06:42,317 - INFO - joeynmt.training - Example #0
2024-05-19 12:06:42,318 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:06:42,318 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:06:42,319 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'il', 'gh@@', 'i@@', 'acci@@', 'o', 'ar@@', 't@@', 'ico', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'dell@@', '&@@', 'a@@', 'pos@@', ';', 'ulti@@', 'mo', '4@@', '0', '%', '.', '</s>']
2024-05-19 12:06:42,319 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:06:42,320 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:06:42,320 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due diapositive per dimostrare che il ghiaccio artico, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dell'ultimo 40%.
2024-05-19 12:06:42,320 - INFO - joeynmt.training - Example #1
2024-05-19 12:06:42,321 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:06:42,321 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:06:42,321 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'ser@@', 'ia', 'la', 'seri@@', 'amente', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'la', 'cosa', 'che', 'non', 'mostr@@', 'a', 'la', 'po@@', 'll@@', 'a', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:06:42,322 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:06:42,322 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:06:42,322 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione seria la seriamente del problema perché non mostra la cosa che non mostra la polla del ghiaccio.
2024-05-19 12:06:42,322 - INFO - joeynmt.training - Example #2
2024-05-19 12:06:42,323 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:06:42,323 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:06:42,323 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'c@@', 'uc@@', 'ci@@', 'a', 'gh@@', 'i@@', 'acci@@', 'a', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:06:42,324 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:06:42,324 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:06:42,324 - INFO - joeynmt.training - 	Hypothesis: La cuccia ghiaccia è, in un certo senso, il cuore del sistema clima globale.
2024-05-19 12:06:42,324 - INFO - joeynmt.training - Example #3
2024-05-19 12:06:42,325 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:06:42,325 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:06:42,325 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 12:06:42,325 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:06:42,326 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:06:42,326 - INFO - joeynmt.training - 	Hypothesis: Si espande in inverno e contratti in estate.
2024-05-19 12:06:42,326 - INFO - joeynmt.training - Example #4
2024-05-19 12:06:42,326 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:06:42,327 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:06:42,327 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'anno', 'un', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'do', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:06:42,327 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:06:42,328 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:06:42,328 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostreranno un rapido rapido di ciò che è successo negli ultimi 25 anni.
2024-05-19 12:06:46,674 - INFO - joeynmt.training - Epoch   8, Step:    35100, Batch Loss:     1.305449, Batch Acc: 0.651048, Tokens per Sec:    15956, Lr: 0.000300
2024-05-19 12:06:50,148 - INFO - joeynmt.training - Epoch   8, Step:    35200, Batch Loss:     1.204527, Batch Acc: 0.642595, Tokens per Sec:    21054, Lr: 0.000300
2024-05-19 12:06:53,451 - INFO - joeynmt.training - Epoch   8, Step:    35300, Batch Loss:     1.304031, Batch Acc: 0.641661, Tokens per Sec:    22965, Lr: 0.000300
2024-05-19 12:06:56,695 - INFO - joeynmt.training - Epoch   8, Step:    35400, Batch Loss:     1.006168, Batch Acc: 0.644574, Tokens per Sec:    22381, Lr: 0.000300
2024-05-19 12:07:01,194 - INFO - joeynmt.training - Epoch   8, Step:    35500, Batch Loss:     1.262571, Batch Acc: 0.642751, Tokens per Sec:    16917, Lr: 0.000300
2024-05-19 12:07:01,195 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:07:01,195 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:07:12,442 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.37, ppl:   3.95, acc:   0.60, generation: 11.0424[sec], evaluation: 0.0000[sec]
2024-05-19 12:07:12,724 - INFO - joeynmt.helpers - delete word_level_model/32500.ckpt
2024-05-19 12:07:12,738 - INFO - joeynmt.training - Example #0
2024-05-19 12:07:12,738 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:07:12,739 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:07:12,739 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'a@@', 'positi@@', 've', 'per', 'la', 'di@@', 'a@@', 'positi@@', 'va', 'che', 'il', 'gh@@', 'i@@', 'acci@@', 'o', 'ar@@', 't@@', 'ico', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'dei', '4@@', '0', 'st@@', 'ati', ',', 'ha', 's@@', 'par@@', 'ito', 'da', '4@@', '0', '%', '.', '</s>']
2024-05-19 12:07:12,740 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:07:12,740 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:07:12,740 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due diapositive per la diapositiva che il ghiaccio artico, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dei 40 stati, ha sparito da 40%.
2024-05-19 12:07:12,741 - INFO - joeynmt.training - Example #1
2024-05-19 12:07:12,741 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:07:12,741 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:07:12,741 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'seri@@', 'amente', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'il', 'gh@@', 'i@@', 'acci@@', 'o', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:07:12,742 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:07:12,743 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:07:12,743 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione seriamente la seriamente di questo particolare problema il ghiaccio del ghiaccio.
2024-05-19 12:07:12,743 - INFO - joeynmt.training - Example #2
2024-05-19 12:07:12,743 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:07:12,744 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:07:12,744 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'gh@@', 'i@@', 'acci@@', 'o', 'ar@@', 't@@', 'ico', 'è', 'un', 'ca@@', 'po', 'di', 'ar@@', 't@@', 'ico', 'è', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:07:12,744 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:07:12,745 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:07:12,745 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio artico è un capo di artico è il cuore del sistema climatico globale.
2024-05-19 12:07:12,745 - INFO - joeynmt.training - Example #3
2024-05-19 12:07:12,746 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:07:12,746 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:07:12,746 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', 'p@@', 'and@@', 'e', 'in', 'est@@', 'ate', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 12:07:12,747 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:07:12,747 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:07:12,747 - INFO - joeynmt.training - 	Hypothesis: Si espande in estate e contratti in estate.
2024-05-19 12:07:12,748 - INFO - joeynmt.training - Example #4
2024-05-19 12:07:12,748 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:07:12,748 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:07:12,748 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 'fo@@', 'to', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'da', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:07:12,749 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:07:12,749 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:07:12,750 - INFO - joeynmt.training - 	Hypothesis: La prossima foto che vi mostrerà un rapido rapida di quello che è successo negli ultimi 25 anni.
2024-05-19 12:07:16,930 - INFO - joeynmt.training - Epoch   8, Step:    35600, Batch Loss:     1.208619, Batch Acc: 0.644984, Tokens per Sec:    16376, Lr: 0.000300
2024-05-19 12:07:20,228 - INFO - joeynmt.training - Epoch   8, Step:    35700, Batch Loss:     1.233073, Batch Acc: 0.642350, Tokens per Sec:    22980, Lr: 0.000300
2024-05-19 12:07:23,567 - INFO - joeynmt.training - Epoch   8, Step:    35800, Batch Loss:     1.247637, Batch Acc: 0.635495, Tokens per Sec:    22051, Lr: 0.000300
2024-05-19 12:07:27,221 - INFO - joeynmt.training - Epoch   8, Step:    35900, Batch Loss:     1.270864, Batch Acc: 0.644374, Tokens per Sec:    20746, Lr: 0.000300
2024-05-19 12:07:31,331 - INFO - joeynmt.training - Epoch   8, Step:    36000, Batch Loss:     1.218341, Batch Acc: 0.646161, Tokens per Sec:    18776, Lr: 0.000300
2024-05-19 12:07:31,332 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:07:31,332 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:07:42,150 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.37, ppl:   3.93, acc:   0.61, generation: 10.6200[sec], evaluation: 0.0000[sec]
2024-05-19 12:07:42,454 - INFO - joeynmt.helpers - delete word_level_model/35500.ckpt
2024-05-19 12:07:42,464 - INFO - joeynmt.helpers - delete /content/drive/MyDrive/mt5/mt-exercise-5/word_level_model/35500.ckpt
2024-05-19 12:07:42,464 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /content/drive/MyDrive/mt5/mt-exercise-5/word_level_model/35500.ckpt but file does not exist. ([Errno 2] No such file or directory: '/content/drive/MyDrive/mt5/mt-exercise-5/word_level_model/35500.ckpt')
2024-05-19 12:07:42,468 - INFO - joeynmt.training - Example #0
2024-05-19 12:07:42,469 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:07:42,469 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:07:42,469 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 's@@', 'li@@', 'de', 'così', 'che', 'di@@', 'mostr@@', 'ano', 'che', 'il', 'gh@@', 'i@@', 'acci@@', 'o', 'ar@@', 'c@@', 'ico', ',', 'che', 'per', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'sono', 'st@@', 'ati', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '0', '%', '.', '</s>']
2024-05-19 12:07:42,471 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:07:42,471 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:07:42,471 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due slide così che dimostrano che il ghiaccio arcico, che per maggior parte degli ultimi tre milioni di anni sono stati le dimensioni dei 40%.
2024-05-19 12:07:42,471 - INFO - joeynmt.training - Example #1
2024-05-19 12:07:42,472 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:07:42,472 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:07:42,472 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'la', 'seri@@', 'et@@', 'à', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'il', 'po@@', 'll@@', 'o', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:07:42,473 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:07:42,473 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:07:42,473 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione la serietà di questo particolare problema perché non mostra il pollo del ghiaccio.
2024-05-19 12:07:42,473 - INFO - joeynmt.training - Example #2
2024-05-19 12:07:42,473 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:07:42,474 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:07:42,474 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'gh@@', 'i@@', 'acci@@', 'o', 'ar@@', 't@@', 'ico', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:07:42,474 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:07:42,475 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:07:42,475 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio artico è, in un certo senso, il cuore del sistema climatico globale.
2024-05-19 12:07:42,475 - INFO - joeynmt.training - Example #3
2024-05-19 12:07:42,475 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:07:42,475 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:07:42,475 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&@@', 'a@@', 'pos@@', ';', 'in', 'in@@', 'ver@@', 'no', 'e', 'in@@', 'ver@@', 'no', '.', '</s>']
2024-05-19 12:07:42,476 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:07:42,476 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:07:42,476 - INFO - joeynmt.training - 	Hypothesis: E 'in inverno e inverno.
2024-05-19 12:07:42,477 - INFO - joeynmt.training - Example #4
2024-05-19 12:07:42,477 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:07:42,477 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:07:42,477 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'pro@@', 'ssi@@', 'mo', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'av@@', 'anti', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:07:42,478 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:07:42,478 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:07:42,478 - INFO - joeynmt.training - 	Hypothesis: Il prossimo slide che vi mostrerà un rapido avanti di quello che è successo negli ultimi 25 anni.
2024-05-19 12:07:46,395 - INFO - joeynmt.training - Epoch   8, Step:    36100, Batch Loss:     1.194833, Batch Acc: 0.643782, Tokens per Sec:    17286, Lr: 0.000300
2024-05-19 12:07:49,701 - INFO - joeynmt.training - Epoch   8, Step:    36200, Batch Loss:     1.178057, Batch Acc: 0.641441, Tokens per Sec:    22841, Lr: 0.000300
2024-05-19 12:07:53,069 - INFO - joeynmt.training - Epoch   8, Step:    36300, Batch Loss:     1.152518, Batch Acc: 0.641631, Tokens per Sec:    22257, Lr: 0.000300
2024-05-19 12:07:56,983 - INFO - joeynmt.training - Epoch   8, Step:    36400, Batch Loss:     1.189886, Batch Acc: 0.645168, Tokens per Sec:    19296, Lr: 0.000300
2024-05-19 12:08:00,687 - INFO - joeynmt.training - Epoch   8, Step:    36500, Batch Loss:     1.208702, Batch Acc: 0.644226, Tokens per Sec:    20063, Lr: 0.000300
2024-05-19 12:08:00,688 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:08:00,688 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:08:13,816 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.37, ppl:   3.92, acc:   0.61, generation: 13.0206[sec], evaluation: 0.0000[sec]
2024-05-19 12:08:13,817 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 12:08:14,062 - INFO - joeynmt.helpers - delete word_level_model/34500.ckpt
2024-05-19 12:08:14,071 - INFO - joeynmt.training - Example #0
2024-05-19 12:08:14,072 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:08:14,072 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:08:14,072 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'queste', 'due', 'di@@', 'a@@', 'positi@@', 've', 'così', 'che', 'di@@', 'di@@', 'di@@', 'di@@', 'di@@', 'mon@@', 'str@@', 'ano', 'che', 'il', 'gh@@', 'i@@', 'acci@@', 'o', 'ar@@', 't@@', 'ico', ',', 'che', 'per', 'molti', 'tre', 'milioni', 'di', 'anni', 'sono', 'state', 'le', 'dimen@@', 'sioni', 'del', '4@@', '0', '%', '.', '</s>']
2024-05-19 12:08:14,073 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:08:14,073 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:08:14,074 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso queste due diapositive così che dididididimonstrano che il ghiaccio artico, che per molti tre milioni di anni sono state le dimensioni del 40%.
2024-05-19 12:08:14,074 - INFO - joeynmt.training - Example #1
2024-05-19 12:08:14,074 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:08:14,074 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:08:14,074 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'seri@@', 'amente', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'ha', 'mostr@@', 'ato', 'il', 'più', 'velo@@', 'ce', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:08:14,075 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:08:14,075 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:08:14,076 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione seriamente la seriamente di questo particolare problema perché non ha mostrato il più veloce del ghiaccio.
2024-05-19 12:08:14,076 - INFO - joeynmt.training - Example #2
2024-05-19 12:08:14,076 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:08:14,076 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:08:14,076 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'c@@', 'ec@@', 'c@@', 'et@@', 'era', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:08:14,077 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:08:14,077 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:08:14,077 - INFO - joeynmt.training - 	Hypothesis: La ceccetera è, in un certo senso, il cuore del sistema climatico globale.
2024-05-19 12:08:14,078 - INFO - joeynmt.training - Example #3
2024-05-19 12:08:14,078 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:08:14,078 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:08:14,078 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'in@@', 'ver@@', 'no', '.', '</s>']
2024-05-19 12:08:14,079 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:08:14,079 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:08:14,079 - INFO - joeynmt.training - 	Hypothesis: Spande in inverno e inverno.
2024-05-19 12:08:14,079 - INFO - joeynmt.training - Example #4
2024-05-19 12:08:14,080 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:08:14,080 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:08:14,080 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'av@@', 'anti', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:08:14,081 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:08:14,081 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:08:14,081 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerà un rapido avanti di quello che è successo negli ultimi 25 anni.
2024-05-19 12:08:17,326 - INFO - joeynmt.training - Epoch   8, Step:    36600, Batch Loss:     1.103095, Batch Acc: 0.637582, Tokens per Sec:    20773, Lr: 0.000300
2024-05-19 12:08:20,653 - INFO - joeynmt.training - Epoch   8, Step:    36700, Batch Loss:     1.313430, Batch Acc: 0.645915, Tokens per Sec:    22196, Lr: 0.000300
2024-05-19 12:08:24,115 - INFO - joeynmt.training - Epoch   8, Step:    36800, Batch Loss:     1.246028, Batch Acc: 0.640316, Tokens per Sec:    20889, Lr: 0.000300
2024-05-19 12:08:28,205 - INFO - joeynmt.training - Epoch   8, Step:    36900, Batch Loss:     1.249440, Batch Acc: 0.640685, Tokens per Sec:    17626, Lr: 0.000300
2024-05-19 12:08:31,485 - INFO - joeynmt.training - Epoch   8, Step:    37000, Batch Loss:     1.152488, Batch Acc: 0.638702, Tokens per Sec:    22982, Lr: 0.000300
2024-05-19 12:08:31,485 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:08:31,486 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:08:43,774 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.36, ppl:   3.90, acc:   0.61, generation: 12.1748[sec], evaluation: 0.0000[sec]
2024-05-19 12:08:43,774 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 12:08:44,018 - INFO - joeynmt.helpers - delete word_level_model/33000.ckpt
2024-05-19 12:08:44,023 - INFO - joeynmt.training - Example #0
2024-05-19 12:08:44,024 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:08:44,024 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:08:44,025 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'ho', 'mostr@@', 'ato', 'queste', 'due', 's@@', 'li@@', 'di', 'così', 'che', 'di@@', 'mostr@@', 'ano', 'che', 'la', 'ca@@', 'de', 'ar@@', 't@@', 'ica', 'gh@@', 'i@@', 'acci@@', 'o', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'più', 'bas@@', 'ata', 'della', 'dimen@@', 'sione', 'più', 'bas@@', 'e', 'dell@@', '&@@', 'a@@', 'pos@@', ';', '4@@', '0', '%', '.', '</s>']
2024-05-19 12:08:44,026 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:08:44,026 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:08:44,026 - INFO - joeynmt.training - 	Hypothesis: L'anno ho mostrato queste due slidi così che dimostrano che la cade artica ghiaccio che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione più basata della dimensione più base dell' 40%.
2024-05-19 12:08:44,026 - INFO - joeynmt.training - Example #1
2024-05-19 12:08:44,027 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:08:44,027 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:08:44,027 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'seri@@', 'et@@', 'à', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'ha', 'mostr@@', 'ato', 'la', 'velo@@', 'ce', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:08:44,028 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:08:44,028 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:08:44,028 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione serietà di questo particolare problema perché non ha mostrato la veloce del ghiaccio.
2024-05-19 12:08:44,028 - INFO - joeynmt.training - Example #2
2024-05-19 12:08:44,029 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:08:44,029 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:08:44,029 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'ca@@', 'po', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'sen@@', 'so', ',', 'in', 'un', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:08:44,030 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:08:44,030 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:08:44,030 - INFO - joeynmt.training - 	Hypothesis: La capo artica è, in un senso, in un senso, il cuore cuore del sistema clima globale.
2024-05-19 12:08:44,030 - INFO - joeynmt.training - Example #3
2024-05-19 12:08:44,031 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:08:44,031 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:08:44,031 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 's@@', 'f@@', 'r@@', 'ut@@', 't@@', 'ore', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'ate', 'e', 'contr@@', 'at@@', 'ti', '.', '</s>']
2024-05-19 12:08:44,032 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:08:44,032 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:08:44,032 - INFO - joeynmt.training - 	Hypothesis: Si sfruttore e contratti in estate e contratti.
2024-05-19 12:08:44,032 - INFO - joeynmt.training - Example #4
2024-05-19 12:08:44,033 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:08:44,033 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:08:44,033 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'di', 'ra@@', 'pi@@', 'da', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:08:44,033 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:08:44,034 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:08:44,034 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerà una rapida di rapida di ciò che è successo negli ultimi 25 anni.
2024-05-19 12:08:47,371 - INFO - joeynmt.training - Epoch   8, Step:    37100, Batch Loss:     1.220382, Batch Acc: 0.642261, Tokens per Sec:    20601, Lr: 0.000300
2024-05-19 12:08:50,702 - INFO - joeynmt.training - Epoch   8, Step:    37200, Batch Loss:     1.187352, Batch Acc: 0.643109, Tokens per Sec:    22644, Lr: 0.000300
2024-05-19 12:08:54,794 - INFO - joeynmt.training - Epoch   8, Step:    37300, Batch Loss:     1.324561, Batch Acc: 0.636438, Tokens per Sec:    17676, Lr: 0.000300
2024-05-19 12:08:58,265 - INFO - joeynmt.training - Epoch   8, Step:    37400, Batch Loss:     1.087134, Batch Acc: 0.641178, Tokens per Sec:    21429, Lr: 0.000300
2024-05-19 12:09:01,548 - INFO - joeynmt.training - Epoch   8, Step:    37500, Batch Loss:     1.327694, Batch Acc: 0.643107, Tokens per Sec:    22852, Lr: 0.000300
2024-05-19 12:09:01,548 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:09:01,549 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:09:14,217 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.35, ppl:   3.87, acc:   0.61, generation: 12.5580[sec], evaluation: 0.0000[sec]
2024-05-19 12:09:14,218 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 12:09:14,470 - INFO - joeynmt.helpers - delete word_level_model/35000.ckpt
2024-05-19 12:09:14,479 - INFO - joeynmt.training - Example #0
2024-05-19 12:09:14,481 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:09:14,481 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:09:14,481 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 's@@', 'li@@', 'de', 'così', 'che', 'di@@', 'mostr@@', 'ano', 'che', 'il', 'gh@@', 'i@@', 'acci@@', 'o', 't@@', 'ico', 'per', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'dei', '4@@', '0', '%', '.', '</s>']
2024-05-19 12:09:14,482 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:09:14,482 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:09:14,483 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso scorso ho mostrato queste due slide così che dimostrano che il ghiaccio tico per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dei 40%.
2024-05-19 12:09:14,483 - INFO - joeynmt.training - Example #1
2024-05-19 12:09:14,483 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:09:14,484 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:09:14,484 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'com@@', 'pren@@', 'de', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'la', 'sp@@', 'azz@@', 'atur@@', 'a', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:09:14,485 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:09:14,485 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:09:14,485 - INFO - joeynmt.training - 	Hypothesis: Ma questo comprende la seriamente di questo particolare problema perché non mostra la spazzatura del ghiaccio.
2024-05-19 12:09:14,485 - INFO - joeynmt.training - Example #2
2024-05-19 12:09:14,486 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:09:14,486 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:09:14,486 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'c@@', 'uc@@', 'ina', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:09:14,487 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:09:14,487 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:09:14,487 - INFO - joeynmt.training - 	Hypothesis: La cucina d'artica è, in un senso, il cuore cuore del sistema clima globale.
2024-05-19 12:09:14,487 - INFO - joeynmt.training - Example #3
2024-05-19 12:09:14,488 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:09:14,488 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:09:14,488 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'p@@', 'on@@', 'gono', 'in', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 12:09:14,489 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:09:14,489 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:09:14,489 - INFO - joeynmt.training - 	Hypothesis: Spongono in inverno e contratti in estate.
2024-05-19 12:09:14,490 - INFO - joeynmt.training - Example #4
2024-05-19 12:09:14,490 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:09:14,490 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:09:14,490 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'di', 'ra@@', 'pi@@', 'do', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:09:14,491 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:09:14,491 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:09:14,491 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerà un rapido di rapido di ciò che è successo negli ultimi 25 anni.
2024-05-19 12:09:17,859 - INFO - joeynmt.training - Epoch   8, Step:    37600, Batch Loss:     1.221760, Batch Acc: 0.644505, Tokens per Sec:    20963, Lr: 0.000300
2024-05-19 12:09:21,620 - INFO - joeynmt.training - Epoch   8, Step:    37700, Batch Loss:     1.092373, Batch Acc: 0.640567, Tokens per Sec:    20369, Lr: 0.000300
2024-05-19 12:09:25,603 - INFO - joeynmt.training - Epoch   8, Step:    37800, Batch Loss:     1.229306, Batch Acc: 0.641459, Tokens per Sec:    18406, Lr: 0.000300
2024-05-19 12:09:28,854 - INFO - joeynmt.training - Epoch   8, Step:    37900, Batch Loss:     1.008406, Batch Acc: 0.640039, Tokens per Sec:    22531, Lr: 0.000300
2024-05-19 12:09:32,134 - INFO - joeynmt.training - Epoch   8, Step:    38000, Batch Loss:     1.222845, Batch Acc: 0.643018, Tokens per Sec:    22264, Lr: 0.000300
2024-05-19 12:09:32,134 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:09:32,135 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:09:44,241 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.36, ppl:   3.88, acc:   0.61, generation: 11.9978[sec], evaluation: 0.0000[sec]
2024-05-19 12:09:44,464 - INFO - joeynmt.helpers - delete word_level_model/36000.ckpt
2024-05-19 12:09:44,473 - INFO - joeynmt.training - Example #0
2024-05-19 12:09:44,474 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:09:44,474 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:09:44,474 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'che', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'a@@', 'positi@@', 've', 'così', 'che', 'di@@', 'mostr@@', 'ano', 'che', 'l@@', '&@@', 'a@@', 'pos@@', ';', 'au@@', 'to', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'dei', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'dei', '4@@', '0', '%', '.', '</s>']
2024-05-19 12:09:44,475 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:09:44,476 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:09:44,476 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso che ho mostrato queste due diapositive così che dimostrano che l'auto, che per la maggior parte dei tre milioni di anni è stata la dimensione dei 40%.
2024-05-19 12:09:44,476 - INFO - joeynmt.training - Example #1
2024-05-19 12:09:44,477 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:09:44,477 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:09:44,477 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'com@@', 'pren@@', 'de', 'la', 'seri@@', 'amente', 'di', 'questo', 'problema', 'parti@@', 'col@@', 'are', 'perché', 'non', 'mostr@@', 'a', 'la', 'tr@@', 'acci@@', 'a', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:09:44,478 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:09:44,478 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:09:44,478 - INFO - joeynmt.training - 	Hypothesis: Ma questo comprende la seriamente di questo problema particolare perché non mostra la traccia del ghiaccio.
2024-05-19 12:09:44,479 - INFO - joeynmt.training - Example #2
2024-05-19 12:09:44,479 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:09:44,479 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:09:44,479 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'ca@@', 'p', 'è', ',', 'in', 'un', 'sen@@', 'so', ',', 'in', 'un', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'di', 's@@', 'b@@', 'agli@@', 'o', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:09:44,480 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:09:44,480 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:09:44,480 - INFO - joeynmt.training - 	Hypothesis: La cap è, in un senso, in un senso, il cuore di sbaglio climatico globale.
2024-05-19 12:09:44,480 - INFO - joeynmt.training - Example #3
2024-05-19 12:09:44,481 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:09:44,481 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:09:44,481 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 12:09:44,482 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:09:44,482 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:09:44,482 - INFO - joeynmt.training - 	Hypothesis: Spande in inverno e contratti in estate.
2024-05-19 12:09:44,483 - INFO - joeynmt.training - Example #4
2024-05-19 12:09:44,483 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:09:44,483 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:09:44,483 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'che', 'vi', 'mostr@@', 'o', 'sar@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'do', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:09:44,484 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:09:44,484 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:09:44,484 - INFO - joeynmt.training - 	Hypothesis: La prossima diapositiva che vi mostro sarà un rapido rapido di ciò che è successo negli ultimi 25 anni.
2024-05-19 12:09:47,789 - INFO - joeynmt.training - Epoch   8, Step:    38100, Batch Loss:     1.372710, Batch Acc: 0.634493, Tokens per Sec:    20358, Lr: 0.000300
2024-05-19 12:09:52,011 - INFO - joeynmt.training - Epoch   8, Step:    38200, Batch Loss:     1.242264, Batch Acc: 0.635190, Tokens per Sec:    17682, Lr: 0.000300
2024-05-19 12:09:55,617 - INFO - joeynmt.training - Epoch   8, Step:    38300, Batch Loss:     1.237552, Batch Acc: 0.644703, Tokens per Sec:    20989, Lr: 0.000300
2024-05-19 12:09:57,455 - INFO - joeynmt.training - Epoch   8: total training loss 5845.39
2024-05-19 12:09:57,456 - INFO - joeynmt.training - EPOCH 9
2024-05-19 12:09:58,944 - INFO - joeynmt.training - Epoch   9, Step:    38400, Batch Loss:     1.132672, Batch Acc: 0.661960, Tokens per Sec:    23231, Lr: 0.000300
2024-05-19 12:10:02,341 - INFO - joeynmt.training - Epoch   9, Step:    38500, Batch Loss:     1.189187, Batch Acc: 0.663494, Tokens per Sec:    22153, Lr: 0.000300
2024-05-19 12:10:02,342 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:10:02,342 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:10:15,211 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.36, ppl:   3.89, acc:   0.61, generation: 12.7560[sec], evaluation: 0.0000[sec]
2024-05-19 12:10:15,436 - INFO - joeynmt.helpers - delete word_level_model/33500.ckpt
2024-05-19 12:10:15,446 - INFO - joeynmt.training - Example #0
2024-05-19 12:10:15,447 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:10:15,447 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:10:15,447 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'mostr@@', 'ò', 'queste', 'due', 's@@', 'li@@', 'de', 'così', 'che', 'di@@', 'mostr@@', 'ano', 'che', 'l@@', '&@@', 'a@@', 'pos@@', ';', 'au@@', 'to', 'ar@@', 'c@@', 'ico', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'dei', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'dei', '4@@', '8', 'per', 'c@@', 'ento', '.', '</s>']
2024-05-19 12:10:15,448 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:10:15,448 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:10:15,449 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso mostrò queste due slide così che dimostrano che l'auto arcico, che per la maggior parte dei tre milioni di anni è stata la dimensione dei 48 per cento.
2024-05-19 12:10:15,449 - INFO - joeynmt.training - Example #1
2024-05-19 12:10:15,449 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:10:15,449 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:10:15,450 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'la', 'più', 'pi@@', 'u', '&@@', 'a@@', 'pos@@', ';', 'di', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:10:15,450 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:10:15,450 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:10:15,451 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione la seriamente di questo particolare problema perché non mostra la più piu 'di ghiaccio.
2024-05-19 12:10:15,451 - INFO - joeynmt.training - Example #2
2024-05-19 12:10:15,451 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:10:15,451 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:10:15,452 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'ca@@', 'p', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:10:15,452 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:10:15,452 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:10:15,453 - INFO - joeynmt.training - 	Hypothesis: La cap artica è, in un certo senso, il cuore del sistema clima globale.
2024-05-19 12:10:15,453 - INFO - joeynmt.training - Example #3
2024-05-19 12:10:15,453 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:10:15,453 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:10:15,454 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', 'o@@', 'st@@', 'e', 'nel', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'i@@', 'va', '.', '</s>']
2024-05-19 12:10:15,454 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:10:15,454 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:10:15,455 - INFO - joeynmt.training - 	Hypothesis: Coste nel inverno e contratti in estiva.
2024-05-19 12:10:15,455 - INFO - joeynmt.training - Example #4
2024-05-19 12:10:15,455 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:10:15,455 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:10:15,456 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:10:15,456 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:10:15,456 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:10:15,457 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerà una rapida di ciò che è successo negli ultimi 25 anni.
2024-05-19 12:10:19,238 - INFO - joeynmt.training - Epoch   9, Step:    38600, Batch Loss:     1.151315, Batch Acc: 0.660118, Tokens per Sec:    17992, Lr: 0.000300
2024-05-19 12:10:23,209 - INFO - joeynmt.training - Epoch   9, Step:    38700, Batch Loss:     1.141426, Batch Acc: 0.661264, Tokens per Sec:    18880, Lr: 0.000300
2024-05-19 12:10:26,507 - INFO - joeynmt.training - Epoch   9, Step:    38800, Batch Loss:     1.143348, Batch Acc: 0.657860, Tokens per Sec:    22318, Lr: 0.000300
2024-05-19 12:10:29,792 - INFO - joeynmt.training - Epoch   9, Step:    38900, Batch Loss:     1.279650, Batch Acc: 0.657054, Tokens per Sec:    22665, Lr: 0.000300
2024-05-19 12:10:33,644 - INFO - joeynmt.training - Epoch   9, Step:    39000, Batch Loss:     1.229668, Batch Acc: 0.658258, Tokens per Sec:    19538, Lr: 0.000300
2024-05-19 12:10:33,644 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:10:33,645 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:10:45,114 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.36, ppl:   3.88, acc:   0.61, generation: 11.3623[sec], evaluation: 0.0000[sec]
2024-05-19 12:10:45,336 - INFO - joeynmt.helpers - delete word_level_model/36500.ckpt
2024-05-19 12:10:45,340 - INFO - joeynmt.training - Example #0
2024-05-19 12:10:45,341 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:10:45,342 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:10:45,342 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'a@@', 'positi@@', 've', 'così', 'che', 'l@@', '&@@', 'a@@', 'pos@@', ';', 'au@@', 'to', 'ar@@', 'c@@', 'ico', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'dei', '4@@', '0', '%', '.', '</s>']
2024-05-19 12:10:45,343 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:10:45,343 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:10:45,344 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due diapositive così che l'auto arcico, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dei 40%.
2024-05-19 12:10:45,344 - INFO - joeynmt.training - Example #1
2024-05-19 12:10:45,344 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:10:45,344 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:10:45,345 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'la', 'seri@@', 'et@@', 'à', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'il', 'gh@@', 'i@@', 'acci@@', 'o', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:10:45,345 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:10:45,346 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:10:45,346 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione la serietà di questo particolare problema perché non mostra il ghiaccio del ghiaccio.
2024-05-19 12:10:45,346 - INFO - joeynmt.training - Example #2
2024-05-19 12:10:45,346 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:10:45,347 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:10:45,347 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'ca@@', 'p', 'è', 'sc@@', 'e@@', 'gli@@', 'a', 't@@', 'ot@@', 'ale', 'è', ',', 'in', 'un', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:10:45,347 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:10:45,347 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:10:45,348 - INFO - joeynmt.training - 	Hypothesis: La cap è sceglia totale è, in un senso, il cuore del sistema clima globale.
2024-05-19 12:10:45,348 - INFO - joeynmt.training - Example #3
2024-05-19 12:10:45,348 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:10:45,348 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:10:45,349 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'si', 'cont@@', 'a', 'in', 'est@@', 'i@@', 'va', '.', '</s>']
2024-05-19 12:10:45,349 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:10:45,349 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:10:45,350 - INFO - joeynmt.training - 	Hypothesis: Si espande in inverno e si conta in estiva.
2024-05-19 12:10:45,350 - INFO - joeynmt.training - Example #4
2024-05-19 12:10:45,350 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:10:45,350 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:10:45,351 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'che', 'vi', 'mostr@@', 'er@@', 'ete', 'una', 'ra@@', 'pi@@', 'da', 'di', 'ra@@', 'pi@@', 'da', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:10:45,351 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:10:45,351 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:10:45,352 - INFO - joeynmt.training - 	Hypothesis: La prossima diapositiva che vi mostrerete una rapida di rapida di ciò che è successo negli ultimi 25 anni.
2024-05-19 12:10:49,895 - INFO - joeynmt.training - Epoch   9, Step:    39100, Batch Loss:     1.186472, Batch Acc: 0.650889, Tokens per Sec:    15910, Lr: 0.000300
2024-05-19 12:10:53,209 - INFO - joeynmt.training - Epoch   9, Step:    39200, Batch Loss:     1.140021, Batch Acc: 0.655638, Tokens per Sec:    22615, Lr: 0.000300
2024-05-19 12:10:56,490 - INFO - joeynmt.training - Epoch   9, Step:    39300, Batch Loss:     1.068389, Batch Acc: 0.657605, Tokens per Sec:    22673, Lr: 0.000300
2024-05-19 12:10:59,891 - INFO - joeynmt.training - Epoch   9, Step:    39400, Batch Loss:     1.054526, Batch Acc: 0.649877, Tokens per Sec:    20995, Lr: 0.000300
2024-05-19 12:11:04,261 - INFO - joeynmt.training - Epoch   9, Step:    39500, Batch Loss:     1.248547, Batch Acc: 0.645646, Tokens per Sec:    16734, Lr: 0.000300
2024-05-19 12:11:04,262 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:11:04,262 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:11:16,068 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.37, ppl:   3.92, acc:   0.61, generation: 11.6119[sec], evaluation: 0.0000[sec]
2024-05-19 12:11:16,074 - INFO - joeynmt.training - Example #0
2024-05-19 12:11:16,074 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:11:16,075 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:11:16,075 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'che', 'questi', 'due', 'di@@', 'a@@', 'positi@@', 've', 'così', 'che', 'di@@', 'mostr@@', 'ano', 'che', 'l@@', '&@@', 'a@@', 'pos@@', ';', 'au@@', 'to', 'ar@@', 'c@@', 'ica', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'dei', '4@@', '8', 'st@@', 'ati', ',', 'ha', 's@@', 'chi@@', 'f@@', 'o', 'il', '4@@', '0', '%', '.', '</s>']
2024-05-19 12:11:16,076 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:11:16,076 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:11:16,076 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso che questi due diapositive così che dimostrano che l'auto arcica, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dei 48 stati, ha schifo il 40%.
2024-05-19 12:11:16,076 - INFO - joeynmt.training - Example #1
2024-05-19 12:11:16,077 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:11:16,077 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:11:16,077 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'seri@@', 'amente', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'il', 'po@@', 'll@@', 'o', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:11:16,078 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:11:16,078 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:11:16,078 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione seriamente la seriamente di questo particolare problema perché non mostra il pollo del ghiaccio.
2024-05-19 12:11:16,078 - INFO - joeynmt.training - Example #2
2024-05-19 12:11:16,078 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:11:16,079 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:11:16,079 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'ca@@', 'p', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'dell@@', '&@@', 'a@@', 'pos@@', ';', 'ar@@', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:11:16,079 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:11:16,080 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:11:16,080 - INFO - joeynmt.training - 	Hypothesis: La cap artica è, in un certo senso, il cuore dell'arclima globale.
2024-05-19 12:11:16,080 - INFO - joeynmt.training - Example #3
2024-05-19 12:11:16,080 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:11:16,080 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:11:16,080 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'ate', 'e', 'contr@@', 'at@@', 'ti', '.', '</s>']
2024-05-19 12:11:16,081 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:11:16,081 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:11:16,082 - INFO - joeynmt.training - 	Hypothesis: Si espande in inverno e contratti in estate e contratti.
2024-05-19 12:11:16,082 - INFO - joeynmt.training - Example #4
2024-05-19 12:11:16,082 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:11:16,082 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:11:16,082 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'ò', 'un', 'ra@@', 'pi@@', 'do', 'velo@@', 'c@@', 'emente', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:11:16,083 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:11:16,083 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:11:16,084 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò un rapido velocemente di quello che è successo negli ultimi 25 anni.
2024-05-19 12:11:19,919 - INFO - joeynmt.training - Epoch   9, Step:    39600, Batch Loss:     1.233340, Batch Acc: 0.650322, Tokens per Sec:    19346, Lr: 0.000300
2024-05-19 12:11:23,230 - INFO - joeynmt.training - Epoch   9, Step:    39700, Batch Loss:     1.178319, Batch Acc: 0.655278, Tokens per Sec:    22880, Lr: 0.000300
2024-05-19 12:11:26,493 - INFO - joeynmt.training - Epoch   9, Step:    39800, Batch Loss:     1.073668, Batch Acc: 0.654682, Tokens per Sec:    22922, Lr: 0.000300
2024-05-19 12:11:30,395 - INFO - joeynmt.training - Epoch   9, Step:    39900, Batch Loss:     1.008653, Batch Acc: 0.647565, Tokens per Sec:    18869, Lr: 0.000300
2024-05-19 12:11:34,221 - INFO - joeynmt.training - Epoch   9, Step:    40000, Batch Loss:     1.207773, Batch Acc: 0.651376, Tokens per Sec:    19528, Lr: 0.000300
2024-05-19 12:11:34,221 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:11:34,222 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:11:46,302 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.36, ppl:   3.90, acc:   0.61, generation: 11.8960[sec], evaluation: 0.0000[sec]
2024-05-19 12:11:46,522 - INFO - joeynmt.helpers - delete word_level_model/37000.ckpt
2024-05-19 12:11:46,532 - INFO - joeynmt.training - Example #0
2024-05-19 12:11:46,533 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:11:46,533 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:11:46,533 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'a@@', 'positi@@', 've', 'così', 'che', 'la', 'di@@', 'mo@@', 'stra@@', 'zione', 'ar@@', 't@@', 'ica', 'che', 'l@@', '&@@', 'a@@', 'pos@@', ';', 'au@@', 'to', 'gh@@', 'i@@', 'acci@@', 'o', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'dei', '4@@', '8', 'st@@', 'ati', ',', 'hanno', 'la', 'dimen@@', 'sione', 'dei', '4@@', '0', '%', '.', '</s>']
2024-05-19 12:11:46,534 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:11:46,535 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:11:46,535 - INFO - joeynmt.training - 	Hypothesis: L'anno ho mostrato queste due diapositive così che la dimostrazione artica che l'auto ghiaccio, che per la maggior parte dei 48 stati, hanno la dimensione dei 40%.
2024-05-19 12:11:46,535 - INFO - joeynmt.training - Example #1
2024-05-19 12:11:46,536 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:11:46,536 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:11:46,536 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'con@@', 'os@@', 'ci@@', 'ut@@', 'a', 'la', 'seri@@', 'et@@', 'à', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'ha', 'mostr@@', 'ato', 'la', 'velo@@', 'c@@', 'ità', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:11:46,537 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:11:46,537 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:11:46,537 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione conosciuta la serietà di questo particolare problema perché non ha mostrato la velocità del ghiaccio.
2024-05-19 12:11:46,537 - INFO - joeynmt.training - Example #2
2024-05-19 12:11:46,538 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:11:46,538 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:11:46,538 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'c@@', 'uc@@', 'ina', 'd@@', 'or@@', 's@@', 'ale', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:11:46,539 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:11:46,539 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:11:46,539 - INFO - joeynmt.training - 	Hypothesis: La cucina dorsale è, in un certo senso, il cuore del sistema climatico globale.
2024-05-19 12:11:46,539 - INFO - joeynmt.training - Example #3
2024-05-19 12:11:46,540 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:11:46,540 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:11:46,540 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'at@@', 'te', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 12:11:46,541 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:11:46,541 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:11:46,541 - INFO - joeynmt.training - 	Hypothesis: Si espande in inverno e contratte in estate.
2024-05-19 12:11:46,541 - INFO - joeynmt.training - Example #4
2024-05-19 12:11:46,542 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:11:46,542 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:11:46,542 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'pi@@', 'da', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:11:46,542 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:11:46,543 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:11:46,543 - INFO - joeynmt.training - 	Hypothesis: La prossima diapositiva che vi mostrerà una rapida pida di ciò che è successo negli ultimi 25 anni.
2024-05-19 12:11:49,804 - INFO - joeynmt.training - Epoch   9, Step:    40100, Batch Loss:     1.235951, Batch Acc: 0.653162, Tokens per Sec:    20873, Lr: 0.000300
2024-05-19 12:11:53,139 - INFO - joeynmt.training - Epoch   9, Step:    40200, Batch Loss:     1.177908, Batch Acc: 0.649924, Tokens per Sec:    22341, Lr: 0.000300
2024-05-19 12:11:56,469 - INFO - joeynmt.training - Epoch   9, Step:    40300, Batch Loss:     1.207712, Batch Acc: 0.643916, Tokens per Sec:    21532, Lr: 0.000300
2024-05-19 12:12:00,756 - INFO - joeynmt.training - Epoch   9, Step:    40400, Batch Loss:     1.174193, Batch Acc: 0.653438, Tokens per Sec:    16792, Lr: 0.000300
2024-05-19 12:12:04,099 - INFO - joeynmt.training - Epoch   9, Step:    40500, Batch Loss:     1.272754, Batch Acc: 0.652448, Tokens per Sec:    22351, Lr: 0.000300
2024-05-19 12:12:04,100 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:12:04,100 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:12:16,489 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.35, ppl:   3.88, acc:   0.61, generation: 12.2828[sec], evaluation: 0.0000[sec]
2024-05-19 12:12:16,702 - INFO - joeynmt.helpers - delete word_level_model/40000.ckpt
2024-05-19 12:12:16,709 - INFO - joeynmt.helpers - delete /content/drive/MyDrive/mt5/mt-exercise-5/word_level_model/40000.ckpt
2024-05-19 12:12:16,710 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /content/drive/MyDrive/mt5/mt-exercise-5/word_level_model/40000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/content/drive/MyDrive/mt5/mt-exercise-5/word_level_model/40000.ckpt')
2024-05-19 12:12:16,713 - INFO - joeynmt.training - Example #0
2024-05-19 12:12:16,714 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:12:16,714 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:12:16,715 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'f@@', 'og@@', 'ge', 'così', 'che', 'di@@', 'mostr@@', 'ano', 'l@@', '&@@', 'a@@', 'pos@@', ';', 'au@@', 'to', 'ar@@', 't@@', 'ico', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'dei', 'tre', 'milioni', 'di', 'anni', 'sono', 'state', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', '%', '.', '</s>']
2024-05-19 12:12:16,716 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:12:16,716 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:12:16,716 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due fogge così che dimostrano l'auto artico, che per la maggior parte dei tre milioni di anni sono state le dimensioni dei 48%.
2024-05-19 12:12:16,717 - INFO - joeynmt.training - Example #1
2024-05-19 12:12:16,717 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:12:16,717 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:12:16,717 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'ser@@', 'va', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'la', 't@@', 'av@@', 'ola', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:12:16,718 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:12:16,718 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:12:16,718 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione serva la seriamente di questo particolare problema perché non mostra la tavola del ghiaccio.
2024-05-19 12:12:16,719 - INFO - joeynmt.training - Example #2
2024-05-19 12:12:16,719 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:12:16,719 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:12:16,719 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'ca@@', 'p', 'ca@@', 'p', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:12:16,720 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:12:16,720 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:12:16,721 - INFO - joeynmt.training - 	Hypothesis: La cap cap è, in un certo senso, in un certo senso, il cuore del sistema clima globale.
2024-05-19 12:12:16,721 - INFO - joeynmt.training - Example #3
2024-05-19 12:12:16,721 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:12:16,721 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:12:16,722 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'cont@@', 'atto', 'in', 'est@@', 'i@@', 'zione', '.', '</s>']
2024-05-19 12:12:16,722 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:12:16,722 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:12:16,723 - INFO - joeynmt.training - 	Hypothesis: Si espande in inverno e contatto in estizione.
2024-05-19 12:12:16,723 - INFO - joeynmt.training - Example #4
2024-05-19 12:12:16,723 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:12:16,723 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:12:16,724 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'una', 'pi@@', 'da', 'di', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:12:16,724 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:12:16,724 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:12:16,725 - INFO - joeynmt.training - 	Hypothesis: La prossima diapositiva che vi mostrerà una pida di che è successo negli ultimi 25 anni.
2024-05-19 12:12:20,038 - INFO - joeynmt.training - Epoch   9, Step:    40600, Batch Loss:     1.102031, Batch Acc: 0.652131, Tokens per Sec:    20611, Lr: 0.000300
2024-05-19 12:12:23,425 - INFO - joeynmt.training - Epoch   9, Step:    40700, Batch Loss:     1.458592, Batch Acc: 0.653428, Tokens per Sec:    22553, Lr: 0.000300
2024-05-19 12:12:27,329 - INFO - joeynmt.training - Epoch   9, Step:    40800, Batch Loss:     1.152642, Batch Acc: 0.654538, Tokens per Sec:    19544, Lr: 0.000300
2024-05-19 12:12:31,083 - INFO - joeynmt.training - Epoch   9, Step:    40900, Batch Loss:     1.101878, Batch Acc: 0.654926, Tokens per Sec:    20200, Lr: 0.000300
2024-05-19 12:12:34,350 - INFO - joeynmt.training - Epoch   9, Step:    41000, Batch Loss:     1.098256, Batch Acc: 0.654099, Tokens per Sec:    22628, Lr: 0.000300
2024-05-19 12:12:34,351 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:12:34,351 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:12:46,751 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.36, ppl:   3.89, acc:   0.61, generation: 12.2871[sec], evaluation: 0.0000[sec]
2024-05-19 12:12:46,969 - INFO - joeynmt.helpers - delete word_level_model/38500.ckpt
2024-05-19 12:12:46,979 - INFO - joeynmt.training - Example #0
2024-05-19 12:12:46,981 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:12:46,981 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:12:46,981 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 's@@', 'li@@', 'de', 'così', 'che', 'di@@', 'mostr@@', 'ano', 'che', 'l@@', '&@@', 'a@@', 'pos@@', ';', 'acqu@@', 'a', 'gh@@', 'i@@', 'acci@@', 'o', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'dei', '4@@', '0', '%', '.', '</s>']
2024-05-19 12:12:46,982 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:12:46,983 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:12:46,983 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due slide così che dimostrano che l'acqua ghiaccio, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dei 40%.
2024-05-19 12:12:46,983 - INFO - joeynmt.training - Example #1
2024-05-19 12:12:46,984 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:12:46,984 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:12:46,984 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'seri@@', 'amente', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'ha', 'mostr@@', 'ato', 'il', 'po@@', 'll@@', 'o', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:12:46,985 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:12:46,985 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:12:46,986 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione seriamente la seriamente di questo particolare problema perché non ha mostrato il pollo del ghiaccio.
2024-05-19 12:12:46,986 - INFO - joeynmt.training - Example #2
2024-05-19 12:12:46,986 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:12:46,986 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:12:46,986 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'macch@@', 'ina', 'ar@@', 't@@', 'ica', 'è', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:12:46,987 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:12:46,987 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:12:46,987 - INFO - joeynmt.training - 	Hypothesis: La macchina artica è in un certo senso, in un certo senso, il cuore del sistema clima globale.
2024-05-19 12:12:46,988 - INFO - joeynmt.training - Example #3
2024-05-19 12:12:46,988 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:12:46,988 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:12:46,988 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'si', 'contr@@', 'a', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 12:12:46,989 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:12:46,989 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:12:46,990 - INFO - joeynmt.training - 	Hypothesis: Si espande in inverno e si contra in estate.
2024-05-19 12:12:46,990 - INFO - joeynmt.training - Example #4
2024-05-19 12:12:46,990 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:12:46,991 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:12:46,991 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'velo@@', 'ce', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:12:46,992 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:12:46,992 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:12:46,992 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerà un rapido veloce di ciò che è successo negli ultimi 25 anni.
2024-05-19 12:12:50,287 - INFO - joeynmt.training - Epoch   9, Step:    41100, Batch Loss:     1.187342, Batch Acc: 0.654119, Tokens per Sec:    21634, Lr: 0.000300
2024-05-19 12:12:53,723 - INFO - joeynmt.training - Epoch   9, Step:    41200, Batch Loss:     1.246127, Batch Acc: 0.646364, Tokens per Sec:    21156, Lr: 0.000300
2024-05-19 12:12:58,011 - INFO - joeynmt.training - Epoch   9, Step:    41300, Batch Loss:     1.138283, Batch Acc: 0.641989, Tokens per Sec:    16816, Lr: 0.000300
2024-05-19 12:13:01,295 - INFO - joeynmt.training - Epoch   9, Step:    41400, Batch Loss:     1.447494, Batch Acc: 0.649538, Tokens per Sec:    23511, Lr: 0.000300
2024-05-19 12:13:04,658 - INFO - joeynmt.training - Epoch   9, Step:    41500, Batch Loss:     1.263499, Batch Acc: 0.648355, Tokens per Sec:    22045, Lr: 0.000300
2024-05-19 12:13:04,659 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:13:04,659 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:13:17,054 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.36, ppl:   3.89, acc:   0.61, generation: 12.2888[sec], evaluation: 0.0000[sec]
2024-05-19 12:13:17,269 - INFO - joeynmt.helpers - delete word_level_model/41000.ckpt
2024-05-19 12:13:17,275 - INFO - joeynmt.helpers - delete /content/drive/MyDrive/mt5/mt-exercise-5/word_level_model/41000.ckpt
2024-05-19 12:13:17,276 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /content/drive/MyDrive/mt5/mt-exercise-5/word_level_model/41000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/content/drive/MyDrive/mt5/mt-exercise-5/word_level_model/41000.ckpt')
2024-05-19 12:13:17,279 - INFO - joeynmt.training - Example #0
2024-05-19 12:13:17,280 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:13:17,280 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:13:17,281 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'che', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'mo@@', 'stra@@', 'zioni', 'così', 'che', 'la', 'di@@', 'mo@@', 'stra@@', 'zione', 'ar@@', 't@@', 'ica', 'che', 'l@@', '&@@', 'a@@', 'pos@@', ';', 'au@@', 'to', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'delle', 'dimen@@', 'sioni', 'dell@@', '&@@', 'a@@', 'pos@@', ';', 'ulti@@', 'mo', '4@@', '8', '%', '.', '</s>']
2024-05-19 12:13:17,281 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:13:17,282 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:13:17,282 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso che ho mostrato queste due dimostrazioni così che la dimostrazione artica che l'auto, che per la maggior parte delle dimensioni dell'ultimo 48%.
2024-05-19 12:13:17,282 - INFO - joeynmt.training - Example #1
2024-05-19 12:13:17,283 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:13:17,283 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:13:17,283 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'seri@@', 'amente', 'la', 'seri@@', 'amente', 'di', 'questo', 'problema', 'perché', 'non', 'ha', 'mostr@@', 'ato', 'il', 'po@@', 'll@@', 'o', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:13:17,284 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:13:17,284 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:13:17,284 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione seriamente la seriamente di questo problema perché non ha mostrato il pollo del ghiaccio.
2024-05-19 12:13:17,284 - INFO - joeynmt.training - Example #2
2024-05-19 12:13:17,285 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:13:17,285 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:13:17,285 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'cap@@', 'ita', 'ar@@', 't@@', 'ica', 'di', 'gh@@', 'i@@', 'acci@@', 'o', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:13:17,286 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:13:17,286 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:13:17,286 - INFO - joeynmt.training - 	Hypothesis: La capita artica di ghiaccio è, in un certo senso, il cuore del sistema clima globale.
2024-05-19 12:13:17,287 - INFO - joeynmt.training - Example #3
2024-05-19 12:13:17,287 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:13:17,287 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:13:17,287 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', 'p@@', 'and@@', 'e', 'nel', 'me@@', 'zzo', 'di', 'in@@', 'ver@@', 'no', 'e', 'si', 'si', 'in@@', 'for@@', 'z@@', 'a', '.', '</s>']
2024-05-19 12:13:17,288 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:13:17,288 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:13:17,288 - INFO - joeynmt.training - 	Hypothesis: Si espande nel mezzo di inverno e si si inforza.
2024-05-19 12:13:17,289 - INFO - joeynmt.training - Example #4
2024-05-19 12:13:17,289 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:13:17,289 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:13:17,289 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'av@@', 'anti', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:13:17,290 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:13:17,290 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:13:17,290 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerà un rapido avanti di quello che è successo negli ultimi 25 anni.
2024-05-19 12:13:20,595 - INFO - joeynmt.training - Epoch   9, Step:    41600, Batch Loss:     1.088412, Batch Acc: 0.644910, Tokens per Sec:    21026, Lr: 0.000300
2024-05-19 12:13:24,445 - INFO - joeynmt.training - Epoch   9, Step:    41700, Batch Loss:     1.234727, Batch Acc: 0.649614, Tokens per Sec:    19261, Lr: 0.000300
2024-05-19 12:13:28,242 - INFO - joeynmt.training - Epoch   9, Step:    41800, Batch Loss:     1.049187, Batch Acc: 0.647573, Tokens per Sec:    19495, Lr: 0.000300
2024-05-19 12:13:31,566 - INFO - joeynmt.training - Epoch   9, Step:    41900, Batch Loss:     1.179594, Batch Acc: 0.648845, Tokens per Sec:    22103, Lr: 0.000300
2024-05-19 12:13:34,909 - INFO - joeynmt.training - Epoch   9, Step:    42000, Batch Loss:     1.183804, Batch Acc: 0.647868, Tokens per Sec:    22647, Lr: 0.000300
2024-05-19 12:13:34,910 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:13:34,910 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:13:46,807 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.35, ppl:   3.86, acc:   0.61, generation: 11.7812[sec], evaluation: 0.0000[sec]
2024-05-19 12:13:46,808 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 12:13:47,040 - INFO - joeynmt.helpers - delete word_level_model/41500.ckpt
2024-05-19 12:13:47,053 - INFO - joeynmt.training - Example #0
2024-05-19 12:13:47,054 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:13:47,055 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:13:47,055 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'a@@', 'positi@@', 've', 'così', 'che', 'di@@', 'mostr@@', 'ano', 'che', 'l@@', '&@@', 'a@@', 'pos@@', ';', 'au@@', 'to', 'ar@@', 't@@', 'ico', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'del', '4@@', '0', '%', '.', '</s>']
2024-05-19 12:13:47,055 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:13:47,056 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:13:47,056 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso mostrato queste due diapositive così che dimostrano che l'auto artico, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione del 40%.
2024-05-19 12:13:47,056 - INFO - joeynmt.training - Example #1
2024-05-19 12:13:47,057 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:13:47,057 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:13:47,057 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'seri@@', 'amente', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'il', 'po@@', 'll@@', 'o', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:13:47,058 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:13:47,058 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:13:47,058 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione seriamente la seriamente di questo particolare problema perché non mostra il pollo del ghiaccio.
2024-05-19 12:13:47,058 - INFO - joeynmt.training - Example #2
2024-05-19 12:13:47,059 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:13:47,059 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:13:47,059 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'cap@@', 'ac@@', 'ità', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:13:47,060 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:13:47,060 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:13:47,060 - INFO - joeynmt.training - 	Hypothesis: La capacità artica è, in un certo senso, il cuore del sistema climatico globale.
2024-05-19 12:13:47,060 - INFO - joeynmt.training - Example #3
2024-05-19 12:13:47,061 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:13:47,061 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:13:47,061 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ci', 'es@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 12:13:47,062 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:13:47,062 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:13:47,062 - INFO - joeynmt.training - 	Hypothesis: Ci espande in inverno e contratti in estate.
2024-05-19 12:13:47,062 - INFO - joeynmt.training - Example #4
2024-05-19 12:13:47,063 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:13:47,063 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:13:47,063 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:13:47,063 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:13:47,064 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:13:47,064 - INFO - joeynmt.training - 	Hypothesis: La prossima diapositiva che vi mostrerà un rapido di ciò che è successo negli ultimi 25 anni.
2024-05-19 12:13:50,493 - INFO - joeynmt.training - Epoch   9, Step:    42100, Batch Loss:     1.234129, Batch Acc: 0.650134, Tokens per Sec:    20387, Lr: 0.000300
2024-05-19 12:13:54,924 - INFO - joeynmt.training - Epoch   9, Step:    42200, Batch Loss:     1.259396, Batch Acc: 0.647481, Tokens per Sec:    16528, Lr: 0.000300
2024-05-19 12:13:58,223 - INFO - joeynmt.training - Epoch   9, Step:    42300, Batch Loss:     1.328125, Batch Acc: 0.645535, Tokens per Sec:    22479, Lr: 0.000300
2024-05-19 12:14:01,537 - INFO - joeynmt.training - Epoch   9, Step:    42400, Batch Loss:     1.196537, Batch Acc: 0.648870, Tokens per Sec:    22971, Lr: 0.000300
2024-05-19 12:14:04,934 - INFO - joeynmt.training - Epoch   9, Step:    42500, Batch Loss:     1.295898, Batch Acc: 0.650378, Tokens per Sec:    21234, Lr: 0.000300
2024-05-19 12:14:04,935 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:14:04,935 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:14:17,051 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.34, ppl:   3.84, acc:   0.61, generation: 12.0056[sec], evaluation: 0.0000[sec]
2024-05-19 12:14:17,053 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 12:14:17,288 - INFO - joeynmt.helpers - delete word_level_model/39000.ckpt
2024-05-19 12:14:17,298 - INFO - joeynmt.training - Example #0
2024-05-19 12:14:17,298 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:14:17,299 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:14:17,299 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'che', 'mostr@@', 'ai', 'due', 'di@@', 'a@@', 'positi@@', 've', 'così', 'da', 'di@@', 'mostr@@', 'are', 'che', 'l@@', '&@@', 'a@@', 'pos@@', ';', 'au@@', 'to', 'ar@@', 't@@', 'ico', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'dei', '4@@', '8', 'per', 'c@@', 'ento', '.', '</s>']
2024-05-19 12:14:17,300 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:14:17,300 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:14:17,300 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso che mostrai due diapositive così da dimostrare che l'auto artico, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dei 48 per cento.
2024-05-19 12:14:17,301 - INFO - joeynmt.training - Example #1
2024-05-19 12:14:17,301 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:14:17,301 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:14:17,301 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'ser@@', 'va', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'la', 'po@@', 'll@@', 'a', '.', '</s>']
2024-05-19 12:14:17,302 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:14:17,302 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:14:17,302 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione serva la seriamente di questo particolare problema perché non mostra la polla.
2024-05-19 12:14:17,303 - INFO - joeynmt.training - Example #2
2024-05-19 12:14:17,303 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:14:17,303 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:14:17,303 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'ca@@', 'p', 'è', 'ar@@', 't@@', 'ica', ',', 'in', 'un', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 's@@', 'ett@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:14:17,304 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:14:17,304 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:14:17,304 - INFO - joeynmt.training - 	Hypothesis: La cap è artica, in un senso, il cuore settore del sistema climatico globale.
2024-05-19 12:14:17,305 - INFO - joeynmt.training - Example #3
2024-05-19 12:14:17,305 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:14:17,305 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:14:17,305 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'i@@', 'zione', '.', '</s>']
2024-05-19 12:14:17,306 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:14:17,306 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:14:17,306 - INFO - joeynmt.training - 	Hypothesis: Si espande in inverno e contratti in estizione.
2024-05-19 12:14:17,307 - INFO - joeynmt.training - Example #4
2024-05-19 12:14:17,307 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:14:17,307 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:14:17,307 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'da', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:14:17,308 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:14:17,308 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:14:17,308 - INFO - joeynmt.training - 	Hypothesis: La prossima diapositiva che vi mostrerà un rapido rapida di quello che è successo negli ultimi 25 anni.
2024-05-19 12:14:21,348 - INFO - joeynmt.training - Epoch   9, Step:    42600, Batch Loss:     1.196712, Batch Acc: 0.643342, Tokens per Sec:    17103, Lr: 0.000300
2024-05-19 12:14:25,197 - INFO - joeynmt.training - Epoch   9, Step:    42700, Batch Loss:     1.195517, Batch Acc: 0.644438, Tokens per Sec:    19271, Lr: 0.000300
2024-05-19 12:14:28,578 - INFO - joeynmt.training - Epoch   9, Step:    42800, Batch Loss:     1.205651, Batch Acc: 0.649197, Tokens per Sec:    22351, Lr: 0.000300
2024-05-19 12:14:31,890 - INFO - joeynmt.training - Epoch   9, Step:    42900, Batch Loss:     1.087333, Batch Acc: 0.647221, Tokens per Sec:    22555, Lr: 0.000300
2024-05-19 12:14:35,947 - INFO - joeynmt.training - Epoch   9, Step:    43000, Batch Loss:     1.085114, Batch Acc: 0.643691, Tokens per Sec:    18853, Lr: 0.000300
2024-05-19 12:14:35,948 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:14:35,948 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:14:47,212 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.34, ppl:   3.82, acc:   0.62, generation: 11.1568[sec], evaluation: 0.0000[sec]
2024-05-19 12:14:47,213 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 12:14:47,504 - INFO - joeynmt.helpers - delete word_level_model/38000.ckpt
2024-05-19 12:14:47,517 - INFO - joeynmt.training - Example #0
2024-05-19 12:14:47,518 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:14:47,518 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:14:47,518 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'a@@', 'positi@@', 've', 'così', 'che', 'di@@', 'mostr@@', 'ano', 'che', 'l@@', '&@@', 'a@@', 'pos@@', ';', 'au@@', 'to', 'ar@@', 't@@', 'ico', 'per', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'dei', '4@@', '8', 'st@@', 'ati', ',', 'ha', 's@@', 'chi@@', 'f@@', 'o', 'del', '4@@', '8', '%', '.', '</s>']
2024-05-19 12:14:47,519 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:14:47,519 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:14:47,520 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due diapositive così che dimostrano che l'auto artico per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dei 48 stati, ha schifo del 48%.
2024-05-19 12:14:47,520 - INFO - joeynmt.training - Example #1
2024-05-19 12:14:47,520 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:14:47,520 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:14:47,520 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'seri@@', 'amente', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'il', 'po@@', 'll@@', 'o', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:14:47,521 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:14:47,521 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:14:47,521 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione seriamente la seriamente di questo particolare problema perché non mostra il pollo del ghiaccio.
2024-05-19 12:14:47,522 - INFO - joeynmt.training - Example #2
2024-05-19 12:14:47,522 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:14:47,522 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:14:47,522 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'ca@@', 'po', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:14:47,523 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:14:47,523 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:14:47,523 - INFO - joeynmt.training - 	Hypothesis: La capo artica è, in un certo senso, il cuore del sistema clima globale.
2024-05-19 12:14:47,523 - INFO - joeynmt.training - Example #3
2024-05-19 12:14:47,524 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:14:47,524 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:14:47,524 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 12:14:47,525 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:14:47,525 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:14:47,525 - INFO - joeynmt.training - 	Hypothesis: Si espande in inverno e contratti in estate.
2024-05-19 12:14:47,525 - INFO - joeynmt.training - Example #4
2024-05-19 12:14:47,525 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:14:47,526 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:14:47,526 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'al@@', 'to', 'a', 'pre@@', 'mio', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:14:47,526 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:14:47,527 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:14:47,527 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerà un rapido alto a premio di quello che è successo negli ultimi 25 anni.
2024-05-19 12:14:52,054 - INFO - joeynmt.training - Epoch   9, Step:    43100, Batch Loss:     1.237554, Batch Acc: 0.650666, Tokens per Sec:    15949, Lr: 0.000300
2024-05-19 12:14:53,877 - INFO - joeynmt.training - Epoch   9: total training loss 5724.88
2024-05-19 12:14:53,878 - INFO - joeynmt.training - EPOCH 10
2024-05-19 12:14:55,416 - INFO - joeynmt.training - Epoch  10, Step:    43200, Batch Loss:     1.022856, Batch Acc: 0.667519, Tokens per Sec:    21618, Lr: 0.000300
2024-05-19 12:14:58,657 - INFO - joeynmt.training - Epoch  10, Step:    43300, Batch Loss:     1.145994, Batch Acc: 0.667950, Tokens per Sec:    22598, Lr: 0.000300
2024-05-19 12:15:02,137 - INFO - joeynmt.training - Epoch  10, Step:    43400, Batch Loss:     1.276598, Batch Acc: 0.668082, Tokens per Sec:    22004, Lr: 0.000300
2024-05-19 12:15:06,556 - INFO - joeynmt.training - Epoch  10, Step:    43500, Batch Loss:     1.112148, Batch Acc: 0.667839, Tokens per Sec:    16674, Lr: 0.000300
2024-05-19 12:15:06,557 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:15:06,557 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:15:18,351 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.35, ppl:   3.84, acc:   0.61, generation: 11.5906[sec], evaluation: 0.0000[sec]
2024-05-19 12:15:18,643 - INFO - joeynmt.helpers - delete word_level_model/40500.ckpt
2024-05-19 12:15:18,657 - INFO - joeynmt.training - Example #0
2024-05-19 12:15:18,658 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:15:18,658 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:15:18,658 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'che', 'mostr@@', 'ai', 'due', 'di@@', 'a@@', 'positi@@', 'vi', 'che', 'la', 'di@@', 'mostr@@', 'a', 'che', 'l@@', '&@@', 'a@@', 'pos@@', ';', 'au@@', 'to', 'gh@@', 'i@@', 'acci@@', 'o', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'dell@@', '&@@', 'a@@', 'pos@@', ';', 'ulti@@', 'mo', '4@@', '8', 'st@@', 'ati', '.', '</s>']
2024-05-19 12:15:18,659 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:15:18,659 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:15:18,659 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso che mostrai due diapositivi che la dimostra che l'auto ghiaccio, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dell'ultimo 48 stati.
2024-05-19 12:15:18,660 - INFO - joeynmt.training - Example #1
2024-05-19 12:15:18,660 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:15:18,660 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:15:18,660 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'capi@@', 'sc@@', 'e', 'la', 'seri@@', 'et@@', 'à', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'la', 'sp@@', 'ess@@', 'enza', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:15:18,661 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:15:18,662 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:15:18,662 - INFO - joeynmt.training - 	Hypothesis: Ma questa capisce la serietà di questo particolare problema perché non mostra la spessenza del ghiaccio.
2024-05-19 12:15:18,662 - INFO - joeynmt.training - Example #2
2024-05-19 12:15:18,662 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:15:18,662 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:15:18,662 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'ca@@', 'p', 'ca@@', 'p', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'di@@', 'mento', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:15:18,663 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:15:18,663 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:15:18,664 - INFO - joeynmt.training - 	Hypothesis: La cap cap è, in un certo senso, il cuore dimento del sistema climatico globale.
2024-05-19 12:15:18,664 - INFO - joeynmt.training - Example #3
2024-05-19 12:15:18,664 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:15:18,664 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:15:18,664 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 12:15:18,665 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:15:18,665 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:15:18,665 - INFO - joeynmt.training - 	Hypothesis: Si espande in inverno e contratti in estate.
2024-05-19 12:15:18,666 - INFO - joeynmt.training - Example #4
2024-05-19 12:15:18,666 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:15:18,666 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:15:18,666 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'velo@@', 'c@@', 'ità', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:15:18,667 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:15:18,667 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:15:18,667 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerà una rapida velocità di ciò che è successo negli ultimi 25 anni.
2024-05-19 12:15:22,363 - INFO - joeynmt.training - Epoch  10, Step:    43600, Batch Loss:     1.201851, Batch Acc: 0.665189, Tokens per Sec:    18333, Lr: 0.000300
2024-05-19 12:15:25,755 - INFO - joeynmt.training - Epoch  10, Step:    43700, Batch Loss:     1.223797, Batch Acc: 0.661381, Tokens per Sec:    22224, Lr: 0.000300
2024-05-19 12:15:29,083 - INFO - joeynmt.training - Epoch  10, Step:    43800, Batch Loss:     1.074706, Batch Acc: 0.667350, Tokens per Sec:    22436, Lr: 0.000300
2024-05-19 12:15:33,360 - INFO - joeynmt.training - Epoch  10, Step:    43900, Batch Loss:     1.117340, Batch Acc: 0.666771, Tokens per Sec:    17996, Lr: 0.000300
2024-05-19 12:15:36,889 - INFO - joeynmt.training - Epoch  10, Step:    44000, Batch Loss:     1.126015, Batch Acc: 0.662616, Tokens per Sec:    20711, Lr: 0.000300
2024-05-19 12:15:36,890 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:15:36,890 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:15:48,914 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.35, ppl:   3.85, acc:   0.61, generation: 11.9168[sec], evaluation: 0.0000[sec]
2024-05-19 12:15:49,135 - INFO - joeynmt.helpers - delete word_level_model/37500.ckpt
2024-05-19 12:15:49,145 - INFO - joeynmt.training - Example #0
2024-05-19 12:15:49,146 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:15:49,146 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:15:49,147 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'a@@', 'positi@@', 've', 'così', 'che', 'di@@', 'mostr@@', 'ano', 'che', 'l@@', '&@@', 'a@@', 'pos@@', ';', 'au@@', 'to', 'gh@@', 'i@@', 'acci@@', 'o', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'è', 'stato', 'la', 'dimen@@', 'sione', 'dell@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'fer@@', 'i@@', 'ore', 'di', '4@@', '0', '%', '.', '</s>']
2024-05-19 12:15:49,148 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:15:49,148 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:15:49,149 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due diapositive così che dimostrano che l'auto ghiaccio, che per la maggior parte degli ultimi tre milioni di anni è stato la dimensione dell'inferiore di 40%.
2024-05-19 12:15:49,149 - INFO - joeynmt.training - Example #1
2024-05-19 12:15:49,149 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:15:49,149 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:15:49,150 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'capi@@', 'sc@@', 'e', 'la', 'seri@@', 'et@@', 'à', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'la', 'po@@', 'll@@', 'a', 'di', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:15:49,150 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:15:49,151 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:15:49,151 - INFO - joeynmt.training - 	Hypothesis: Ma questa capisce la serietà di questo particolare problema perché non mostra la polla di ghiaccio.
2024-05-19 12:15:49,151 - INFO - joeynmt.training - Example #2
2024-05-19 12:15:49,151 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:15:49,152 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:15:49,152 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'c@@', 'uc@@', 'ina', 't@@', 'ica', 'è', 'che', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:15:49,152 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:15:49,153 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:15:49,153 - INFO - joeynmt.training - 	Hypothesis: La cucina tica è che, in un certo senso, il cuore del sistema climatico globale.
2024-05-19 12:15:49,153 - INFO - joeynmt.training - Example #3
2024-05-19 12:15:49,154 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:15:49,154 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:15:49,154 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'cont@@', 'ra@@', 'st@@', 'ino', '.', '</s>']
2024-05-19 12:15:49,154 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:15:49,155 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:15:49,155 - INFO - joeynmt.training - 	Hypothesis: Si espande in inverno e contrastino.
2024-05-19 12:15:49,155 - INFO - joeynmt.training - Example #4
2024-05-19 12:15:49,155 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:15:49,156 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:15:49,156 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'da', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:15:49,156 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:15:49,157 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:15:49,157 - INFO - joeynmt.training - 	Hypothesis: La prossima diapositiva che vi mostrerà un rapido rapida di quello che è successo negli ultimi 25 anni.
2024-05-19 12:15:52,470 - INFO - joeynmt.training - Epoch  10, Step:    44100, Batch Loss:     1.204332, Batch Acc: 0.657092, Tokens per Sec:    21410, Lr: 0.000300
2024-05-19 12:15:55,855 - INFO - joeynmt.training - Epoch  10, Step:    44200, Batch Loss:     1.250711, Batch Acc: 0.659753, Tokens per Sec:    22095, Lr: 0.000300
2024-05-19 12:15:59,502 - INFO - joeynmt.training - Epoch  10, Step:    44300, Batch Loss:     1.104302, Batch Acc: 0.663801, Tokens per Sec:    20224, Lr: 0.000300
2024-05-19 12:16:03,740 - INFO - joeynmt.training - Epoch  10, Step:    44400, Batch Loss:     1.115564, Batch Acc: 0.658823, Tokens per Sec:    17794, Lr: 0.000300
2024-05-19 12:16:07,111 - INFO - joeynmt.training - Epoch  10, Step:    44500, Batch Loss:     1.132457, Batch Acc: 0.658608, Tokens per Sec:    22460, Lr: 0.000300
2024-05-19 12:16:07,111 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:16:07,112 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:16:19,151 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.35, ppl:   3.84, acc:   0.61, generation: 11.9362[sec], evaluation: 0.0000[sec]
2024-05-19 12:16:19,384 - INFO - joeynmt.helpers - delete word_level_model/42000.ckpt
2024-05-19 12:16:19,390 - INFO - joeynmt.training - Example #0
2024-05-19 12:16:19,391 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:16:19,391 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:16:19,391 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'a@@', 'positi@@', 've', 'così', 'che', 'di@@', 'mostr@@', 'ano', 'che', 'il', 'gh@@', 'i@@', 'acci@@', 'o', 'ar@@', 't@@', 'ico', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'dell@@', '&@@', 'a@@', 'pos@@', ';', 'es@@', 'peri@@', 'enza', 'di', '4@@', '8', '%', '.', '</s>']
2024-05-19 12:16:19,392 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:16:19,392 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:16:19,393 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due diapositive così che dimostrano che il ghiaccio artico, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dell'esperienza di 48%.
2024-05-19 12:16:19,393 - INFO - joeynmt.training - Example #1
2024-05-19 12:16:19,393 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:16:19,393 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:16:19,394 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'ser@@', 'ia', 'la', 'seri@@', 'et@@', 'à', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'il', 'po@@', 'll@@', 'o', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:16:19,394 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:16:19,394 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:16:19,395 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione seria la serietà di questo particolare problema perché non mostra il pollo del ghiaccio.
2024-05-19 12:16:19,395 - INFO - joeynmt.training - Example #2
2024-05-19 12:16:19,395 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:16:19,395 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:16:19,396 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'ca@@', 'p', 'ca@@', 'p', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:16:19,396 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:16:19,396 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:16:19,397 - INFO - joeynmt.training - 	Hypothesis: La cap cap è, in un certo senso, il cuore del sistema climatico globale.
2024-05-19 12:16:19,397 - INFO - joeynmt.training - Example #3
2024-05-19 12:16:19,397 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:16:19,397 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:16:19,397 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'si', 'cont@@', 'ra@@', 'i', 'in', 'est@@', 'i@@', 'ame', '.', '</s>']
2024-05-19 12:16:19,398 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:16:19,398 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:16:19,398 - INFO - joeynmt.training - 	Hypothesis: Si espande in inverno e si contrai in estiame.
2024-05-19 12:16:19,399 - INFO - joeynmt.training - Example #4
2024-05-19 12:16:19,399 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:16:19,399 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:16:19,399 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'sar@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'do', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:16:19,400 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:16:19,400 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:16:19,400 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro sarà un rapido rapido rapido di ciò che è successo negli ultimi 25 anni.
2024-05-19 12:16:22,837 - INFO - joeynmt.training - Epoch  10, Step:    44600, Batch Loss:     1.060812, Batch Acc: 0.658232, Tokens per Sec:    20529, Lr: 0.000300
2024-05-19 12:16:26,182 - INFO - joeynmt.training - Epoch  10, Step:    44700, Batch Loss:     1.080762, Batch Acc: 0.666528, Tokens per Sec:    22265, Lr: 0.000300
2024-05-19 12:16:30,384 - INFO - joeynmt.training - Epoch  10, Step:    44800, Batch Loss:     1.047651, Batch Acc: 0.665244, Tokens per Sec:    17844, Lr: 0.000300
2024-05-19 12:16:33,848 - INFO - joeynmt.training - Epoch  10, Step:    44900, Batch Loss:     0.959085, Batch Acc: 0.656040, Tokens per Sec:    22044, Lr: 0.000300
2024-05-19 12:16:37,081 - INFO - joeynmt.training - Epoch  10, Step:    45000, Batch Loss:     1.333759, Batch Acc: 0.662157, Tokens per Sec:    23785, Lr: 0.000300
2024-05-19 12:16:37,082 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:16:37,082 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:16:50,327 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.35, ppl:   3.84, acc:   0.61, generation: 13.1308[sec], evaluation: 0.0000[sec]
2024-05-19 12:16:50,562 - INFO - joeynmt.helpers - delete word_level_model/44000.ckpt
2024-05-19 12:16:50,568 - INFO - joeynmt.training - Example #0
2024-05-19 12:16:50,569 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:16:50,569 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:16:50,569 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'che', 'mostr@@', 'ò', 'queste', 'due', 'di@@', 'a@@', 'positi@@', 've', 'così', 'che', 'di@@', 'mostr@@', 'ano', 'che', 'l@@', '&@@', 'a@@', 'pos@@', ';', 'au@@', 'to', 'gh@@', 'i@@', 'acci@@', 'o', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'dei', '4@@', '0', '%', '.', '</s>']
2024-05-19 12:16:50,571 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:16:50,571 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:16:50,571 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso che mostrò queste due diapositive così che dimostrano che l'auto ghiaccio, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dei 40%.
2024-05-19 12:16:50,572 - INFO - joeynmt.training - Example #1
2024-05-19 12:16:50,572 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:16:50,572 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:16:50,573 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'com@@', 'pren@@', 'sione', 'seri@@', 'amente', 'la', 'seri@@', 'amente', 'di', 'questo', 'problema', 'parti@@', 'col@@', 'are', 'perché', 'non', 'mostr@@', 'a', 'il', 'po@@', 'll@@', 'o', 'di', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:16:50,573 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:16:50,574 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:16:50,574 - INFO - joeynmt.training - 	Hypothesis: Ma questa comprensione seriamente la seriamente di questo problema particolare perché non mostra il pollo di ghiaccio.
2024-05-19 12:16:50,574 - INFO - joeynmt.training - Example #2
2024-05-19 12:16:50,575 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:16:50,575 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:16:50,575 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'ca@@', 'p', 'ca@@', 'p', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:16:50,576 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:16:50,576 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:16:50,576 - INFO - joeynmt.training - 	Hypothesis: La cap cap è, in un certo senso, il cuore cuore del sistema climatico globale.
2024-05-19 12:16:50,577 - INFO - joeynmt.training - Example #3
2024-05-19 12:16:50,577 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:16:50,577 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:16:50,578 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'i@@', 'zione', '.', '</s>']
2024-05-19 12:16:50,578 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:16:50,578 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:16:50,579 - INFO - joeynmt.training - 	Hypothesis: Si espande in inverno e contratti in estizione.
2024-05-19 12:16:50,579 - INFO - joeynmt.training - Example #4
2024-05-19 12:16:50,579 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:16:50,579 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:16:50,580 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'av@@', 'anti', 'ra@@', 'pi@@', 'do', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:16:50,580 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:16:50,580 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:16:50,581 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerà un avanti rapido di ciò che è successo negli ultimi 25 anni.
2024-05-19 12:16:53,937 - INFO - joeynmt.training - Epoch  10, Step:    45100, Batch Loss:     1.204979, Batch Acc: 0.663635, Tokens per Sec:    20687, Lr: 0.000300
2024-05-19 12:16:57,979 - INFO - joeynmt.training - Epoch  10, Step:    45200, Batch Loss:     1.095669, Batch Acc: 0.655603, Tokens per Sec:    18743, Lr: 0.000300
2024-05-19 12:17:01,774 - INFO - joeynmt.training - Epoch  10, Step:    45300, Batch Loss:     1.198987, Batch Acc: 0.656476, Tokens per Sec:    19682, Lr: 0.000300
2024-05-19 12:17:05,166 - INFO - joeynmt.training - Epoch  10, Step:    45400, Batch Loss:     1.086387, Batch Acc: 0.654373, Tokens per Sec:    22177, Lr: 0.000300
2024-05-19 12:17:08,521 - INFO - joeynmt.training - Epoch  10, Step:    45500, Batch Loss:     1.246789, Batch Acc: 0.655703, Tokens per Sec:    22309, Lr: 0.000300
2024-05-19 12:17:08,522 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:17:08,522 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:17:20,872 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.34, ppl:   3.83, acc:   0.62, generation: 12.2390[sec], evaluation: 0.0000[sec]
2024-05-19 12:17:21,091 - INFO - joeynmt.helpers - delete word_level_model/45000.ckpt
2024-05-19 12:17:21,098 - INFO - joeynmt.helpers - delete /content/drive/MyDrive/mt5/mt-exercise-5/word_level_model/45000.ckpt
2024-05-19 12:17:21,099 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /content/drive/MyDrive/mt5/mt-exercise-5/word_level_model/45000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/content/drive/MyDrive/mt5/mt-exercise-5/word_level_model/45000.ckpt')
2024-05-19 12:17:21,103 - INFO - joeynmt.training - Example #0
2024-05-19 12:17:21,103 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:17:21,104 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:17:21,104 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'a@@', 'positi@@', 've', 'così', 'che', 'di@@', 'mostr@@', 'ano', 'che', 'l@@', '&@@', 'a@@', 'pos@@', ';', 'au@@', 'to', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'ca@@', 'p', ';', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', '3', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'di', '4@@', '0', '%', '.', '</s>']
2024-05-19 12:17:21,105 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:17:21,105 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:17:21,106 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso mostrato queste due diapositive così che dimostrano che l'auto, che per la maggior cap; che per la maggior parte degli ultimi 3 milioni di anni è stata la dimensione di 40%.
2024-05-19 12:17:21,106 - INFO - joeynmt.training - Example #1
2024-05-19 12:17:21,106 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:17:21,106 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:17:21,107 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'capi@@', 'sc@@', 'e', 'la', 'seri@@', 'et@@', 'à', 'di', 'questo', 'problema', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'il', 'po@@', 'll@@', 'ice', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:17:21,107 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:17:21,107 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:17:21,108 - INFO - joeynmt.training - 	Hypothesis: Ma questa capisce la serietà di questo problema particolare problema perché non mostra il pollice del ghiaccio.
2024-05-19 12:17:21,108 - INFO - joeynmt.training - Example #2
2024-05-19 12:17:21,108 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:17:21,109 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:17:21,109 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'ca@@', 'p', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:17:21,109 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:17:21,110 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:17:21,110 - INFO - joeynmt.training - 	Hypothesis: La cap è, in un certo senso, in un certo senso, il cuore del sistema clima globale.
2024-05-19 12:17:21,110 - INFO - joeynmt.training - Example #3
2024-05-19 12:17:21,110 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:17:21,111 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:17:21,111 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 12:17:21,111 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:17:21,111 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:17:21,112 - INFO - joeynmt.training - 	Hypothesis: Si espande in inverno e contratti in estate.
2024-05-19 12:17:21,112 - INFO - joeynmt.training - Example #4
2024-05-19 12:17:21,112 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:17:21,112 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:17:21,113 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'ra@@', 'pi@@', 'do', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:17:21,113 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:17:21,113 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:17:21,114 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerà un rapido rapido di ciò che è successo negli ultimi 25 anni.
2024-05-19 12:17:24,810 - INFO - joeynmt.training - Epoch  10, Step:    45600, Batch Loss:     1.040676, Batch Acc: 0.656330, Tokens per Sec:    18871, Lr: 0.000300
2024-05-19 12:17:29,016 - INFO - joeynmt.training - Epoch  10, Step:    45700, Batch Loss:     1.037457, Batch Acc: 0.657260, Tokens per Sec:    17569, Lr: 0.000300
2024-05-19 12:17:32,334 - INFO - joeynmt.training - Epoch  10, Step:    45800, Batch Loss:     1.139325, Batch Acc: 0.651759, Tokens per Sec:    22355, Lr: 0.000300
2024-05-19 12:17:35,621 - INFO - joeynmt.training - Epoch  10, Step:    45900, Batch Loss:     1.119466, Batch Acc: 0.657186, Tokens per Sec:    22937, Lr: 0.000300
2024-05-19 12:17:39,213 - INFO - joeynmt.training - Epoch  10, Step:    46000, Batch Loss:     1.150596, Batch Acc: 0.658398, Tokens per Sec:    20950, Lr: 0.000300
2024-05-19 12:17:39,213 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:17:39,214 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:17:51,440 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.34, ppl:   3.80, acc:   0.62, generation: 12.1192[sec], evaluation: 0.0000[sec]
2024-05-19 12:17:51,441 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 12:17:51,694 - INFO - joeynmt.helpers - delete word_level_model/43500.ckpt
2024-05-19 12:17:51,704 - INFO - joeynmt.training - Example #0
2024-05-19 12:17:51,705 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:17:51,705 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:17:51,705 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'a@@', 'positi@@', 've', 'così', 'che', 'di@@', 'mostr@@', 'ano', 'che', 'la', 'ca@@', 'de', 'ar@@', 't@@', 'ica', 'gh@@', 'i@@', 'acci@@', 'o', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'sono', 'st@@', 'ati', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', '%', '.', '</s>']
2024-05-19 12:17:51,706 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:17:51,707 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:17:51,707 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due diapositive così che dimostrano che la cade artica ghiaccio, che per la maggior parte degli ultimi tre milioni di anni sono stati le dimensioni dei 48%.
2024-05-19 12:17:51,707 - INFO - joeynmt.training - Example #1
2024-05-19 12:17:51,708 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:17:51,708 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:17:51,708 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'capi@@', 'sc@@', 'e', 'la', 'seri@@', 'amente', 'del', 'gh@@', 'i@@', 'acci@@', 'o', 'perché', 'non', 'mostr@@', 'a', 'il', 'po@@', 'll@@', 'o', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:17:51,709 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:17:51,709 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:17:51,709 - INFO - joeynmt.training - 	Hypothesis: Ma questa capisce la seriamente del ghiaccio perché non mostra il pollo del ghiaccio.
2024-05-19 12:17:51,709 - INFO - joeynmt.training - Example #2
2024-05-19 12:17:51,710 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:17:51,710 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:17:51,710 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'ca@@', 'zione', 'ar@@', 't@@', 'ica', 'è', 'il', 'ca@@', 'po', 'ar@@', 't@@', 'ico', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:17:51,711 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:17:51,711 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:17:51,711 - INFO - joeynmt.training - 	Hypothesis: La cazione artica è il capo artico è, in un certo senso, il cuore del sistema climatico globale.
2024-05-19 12:17:51,712 - INFO - joeynmt.training - Example #3
2024-05-19 12:17:51,712 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:17:51,712 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:17:51,712 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', 'p@@', 'and@@', 'e', 'in', 'cont@@', 'ra@@', 'st@@', 'i', 'in', 'est@@', 'ate', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 12:17:51,713 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:17:51,713 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:17:51,714 - INFO - joeynmt.training - 	Hypothesis: Si espande in contrasti in estate e contratti in estate.
2024-05-19 12:17:51,714 - INFO - joeynmt.training - Example #4
2024-05-19 12:17:51,714 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:17:51,714 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:17:51,714 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'sar@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'av@@', 'anti', 'ra@@', 'pi@@', 'da', 'di', 'quello', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:17:51,715 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:17:51,715 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:17:51,716 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro sarà un rapido avanti rapida di quello che è successo negli ultimi 25 anni.
2024-05-19 12:17:55,936 - INFO - joeynmt.training - Epoch  10, Step:    46100, Batch Loss:     1.182261, Batch Acc: 0.654463, Tokens per Sec:    16636, Lr: 0.000300
2024-05-19 12:17:59,446 - INFO - joeynmt.training - Epoch  10, Step:    46200, Batch Loss:     1.042903, Batch Acc: 0.649739, Tokens per Sec:    21116, Lr: 0.000300
2024-05-19 12:18:02,817 - INFO - joeynmt.training - Epoch  10, Step:    46300, Batch Loss:     1.105432, Batch Acc: 0.664271, Tokens per Sec:    22165, Lr: 0.000300
2024-05-19 12:18:06,110 - INFO - joeynmt.training - Epoch  10, Step:    46400, Batch Loss:     1.226271, Batch Acc: 0.659016, Tokens per Sec:    23821, Lr: 0.000300
2024-05-19 12:18:10,369 - INFO - joeynmt.training - Epoch  10, Step:    46500, Batch Loss:     1.088770, Batch Acc: 0.655052, Tokens per Sec:    17181, Lr: 0.000300
2024-05-19 12:18:10,369 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:18:10,370 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:18:22,573 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.34, ppl:   3.81, acc:   0.62, generation: 12.0057[sec], evaluation: 0.0000[sec]
2024-05-19 12:18:22,864 - INFO - joeynmt.helpers - delete word_level_model/44500.ckpt
2024-05-19 12:18:22,878 - INFO - joeynmt.training - Example #0
2024-05-19 12:18:22,878 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:18:22,878 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:18:22,879 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'a@@', 'positi@@', 've', 'così', 'che', 'di@@', 'mostr@@', 'ano', 'che', 'il', 'gh@@', 'i@@', 'acci@@', 'o', 'ar@@', 't@@', 'ico', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'dei', 't@@', 'ass@@', 'ori', 'del', '4@@', '8', '%', '.', '</s>']
2024-05-19 12:18:22,880 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:18:22,880 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:18:22,880 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due diapositive così che dimostrano che il ghiaccio artico, che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dei tassori del 48%.
2024-05-19 12:18:22,880 - INFO - joeynmt.training - Example #1
2024-05-19 12:18:22,880 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:18:22,881 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:18:22,881 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'capi@@', 'sc@@', 'e', 'la', 'seri@@', 'amente', 'ser@@', 'io', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'il', 't@@', 'ick@@', 'à', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:18:22,882 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:18:22,882 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:18:22,882 - INFO - joeynmt.training - 	Hypothesis: Ma capisce la seriamente serio di questo particolare problema perché non mostra il tickà del ghiaccio.
2024-05-19 12:18:22,882 - INFO - joeynmt.training - Example #2
2024-05-19 12:18:22,883 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:18:22,883 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:18:22,883 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'au@@', 'to', 'ar@@', 'c@@', 'ale', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:18:22,883 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:18:22,884 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:18:22,884 - INFO - joeynmt.training - 	Hypothesis: L'auto arcale è, in un certo senso, il cuore del sistema clima globale.
2024-05-19 12:18:22,884 - INFO - joeynmt.training - Example #3
2024-05-19 12:18:22,884 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:18:22,884 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:18:22,885 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'cont@@', 'ra@@', 'st@@', 'ino', '.', '</s>']
2024-05-19 12:18:22,886 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:18:22,886 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:18:22,886 - INFO - joeynmt.training - 	Hypothesis: Si espande in inverno e contrastino.
2024-05-19 12:18:22,887 - INFO - joeynmt.training - Example #4
2024-05-19 12:18:22,887 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:18:22,887 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:18:22,887 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'sar@@', 'ete', 'un', 'ra@@', 'pi@@', 'do', 'velo@@', 'c@@', 'emente', 'in', 'av@@', 'anti', ',', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:18:22,888 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:18:22,888 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:18:22,888 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che sarete un rapido velocemente in avanti, di ciò che è successo negli ultimi 25 anni.
2024-05-19 12:18:26,845 - INFO - joeynmt.training - Epoch  10, Step:    46600, Batch Loss:     1.134167, Batch Acc: 0.658845, Tokens per Sec:    17582, Lr: 0.000300
2024-05-19 12:18:30,103 - INFO - joeynmt.training - Epoch  10, Step:    46700, Batch Loss:     1.247615, Batch Acc: 0.652516, Tokens per Sec:    23351, Lr: 0.000300
2024-05-19 12:18:33,428 - INFO - joeynmt.training - Epoch  10, Step:    46800, Batch Loss:     1.135726, Batch Acc: 0.657654, Tokens per Sec:    22555, Lr: 0.000300
2024-05-19 12:18:37,124 - INFO - joeynmt.training - Epoch  10, Step:    46900, Batch Loss:     1.136671, Batch Acc: 0.651355, Tokens per Sec:    19459, Lr: 0.000300
2024-05-19 12:18:41,138 - INFO - joeynmt.training - Epoch  10, Step:    47000, Batch Loss:     1.244256, Batch Acc: 0.651189, Tokens per Sec:    18771, Lr: 0.000300
2024-05-19 12:18:41,139 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:18:41,139 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:18:52,803 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.34, ppl:   3.81, acc:   0.62, generation: 11.4650[sec], evaluation: 0.0000[sec]
2024-05-19 12:18:53,066 - INFO - joeynmt.helpers - delete word_level_model/42500.ckpt
2024-05-19 12:18:53,080 - INFO - joeynmt.training - Example #0
2024-05-19 12:18:53,081 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:18:53,081 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:18:53,081 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'due', 's@@', 'li@@', 'de', 'così', 'che', 'di@@', 'mostr@@', 'ano', 'che', 'la', 'ca@@', 'po', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'è', 'stato', 'la', 'dimen@@', 'sione', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'st@@', 'ati', ',', 'ha', 's@@', 'chi@@', 'f@@', 'o', 'del', '4@@', '0', '%', '.', '</s>']
2024-05-19 12:18:53,082 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:18:53,082 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:18:53,082 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso ho mostrato queste due slide così che dimostrano che la capo artica, che per la maggior parte degli ultimi tre milioni di anni è stato la dimensione degli ultimi tre milioni di stati, ha schifo del 40%.
2024-05-19 12:18:53,082 - INFO - joeynmt.training - Example #1
2024-05-19 12:18:53,083 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:18:53,083 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:18:53,083 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', 'sc@@', 'e', 'la', 'ser@@', 'ia', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'la', 'velo@@', 'c@@', 'ità', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:18:53,084 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:18:53,084 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:18:53,084 - INFO - joeynmt.training - 	Hypothesis: Ma questo capisce la seria di questo particolare problema perché non mostra la velocità del ghiaccio.
2024-05-19 12:18:53,085 - INFO - joeynmt.training - Example #2
2024-05-19 12:18:53,085 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:18:53,085 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:18:53,085 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'ca@@', 'po', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'ma', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:18:53,086 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:18:53,086 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:18:53,086 - INFO - joeynmt.training - 	Hypothesis: La capo artica è, in un certo senso, il cuore del sistema clima globale.
2024-05-19 12:18:53,087 - INFO - joeynmt.training - Example #3
2024-05-19 12:18:53,087 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:18:53,087 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:18:53,087 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'at@@', 'ti', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 12:18:53,088 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:18:53,088 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:18:53,088 - INFO - joeynmt.training - 	Hypothesis: Si espande in inverno e contratti in estate.
2024-05-19 12:18:53,088 - INFO - joeynmt.training - Example #4
2024-05-19 12:18:53,089 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:18:53,089 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:18:53,089 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'una', 'ra@@', 'pi@@', 'da', 'ra@@', 'pi@@', 'da', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:18:53,090 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:18:53,090 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:18:53,090 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro una rapida rapida di ciò che è successo negli ultimi 25 anni.
2024-05-19 12:18:56,594 - INFO - joeynmt.training - Epoch  10, Step:    47100, Batch Loss:     1.112651, Batch Acc: 0.655071, Tokens per Sec:    19556, Lr: 0.000300
2024-05-19 12:18:59,837 - INFO - joeynmt.training - Epoch  10, Step:    47200, Batch Loss:     1.241087, Batch Acc: 0.654332, Tokens per Sec:    23330, Lr: 0.000300
2024-05-19 12:19:03,204 - INFO - joeynmt.training - Epoch  10, Step:    47300, Batch Loss:     1.294803, Batch Acc: 0.654914, Tokens per Sec:    22080, Lr: 0.000300
2024-05-19 12:19:07,483 - INFO - joeynmt.training - Epoch  10, Step:    47400, Batch Loss:     1.251874, Batch Acc: 0.650940, Tokens per Sec:    17417, Lr: 0.000300
2024-05-19 12:19:10,970 - INFO - joeynmt.training - Epoch  10, Step:    47500, Batch Loss:     1.154862, Batch Acc: 0.650826, Tokens per Sec:    21037, Lr: 0.000300
2024-05-19 12:19:10,971 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:19:10,971 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:19:23,750 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.33, ppl:   3.78, acc:   0.62, generation: 12.6685[sec], evaluation: 0.0000[sec]
2024-05-19 12:19:23,751 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-19 12:19:23,996 - INFO - joeynmt.helpers - delete word_level_model/45500.ckpt
2024-05-19 12:19:24,002 - INFO - joeynmt.training - Example #0
2024-05-19 12:19:24,003 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2024-05-19 12:19:24,003 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', ',', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali', ',', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0', '%', '.']
2024-05-19 12:19:24,004 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '&@@', 'a@@', 'pos@@', ';', 'anno', 'sc@@', 'or@@', 'so', 'che', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'a@@', 'positi@@', 've', 'così', 'che', 'la', 'di@@', 'mostr@@', 'a', 'che', 'l@@', '&@@', 'a@@', 'pos@@', ';', 'au@@', 'to', '&@@', 'qu@@', 'ot@@', ';', ',', 'che', 'per', 'la', 'maggi@@', 'or', 'parte', 'degli', 'ulti@@', 'mi', 'tre', 'milioni', 'di', 'anni', 'è', 'stato', 'la', 'dimen@@', 'sione', 'degli', 'st@@', 'ati', ',', 'ha', 's@@', 'chi@@', 'ato', 'il', '4@@', '0', '%', '.', '</s>']
2024-05-19 12:19:24,005 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-19 12:19:24,005 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica, che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali, si è ristretta del 40%.
2024-05-19 12:19:24,005 - INFO - joeynmt.training - 	Hypothesis: L'anno scorso che ho mostrato queste due diapositive così che la dimostra che l'auto ", che per la maggior parte degli ultimi tre milioni di anni è stato la dimensione degli stati, ha schiato il 40%.
2024-05-19 12:19:24,006 - INFO - joeynmt.training - Example #1
2024-05-19 12:19:24,006 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-19 12:19:24,006 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.']
2024-05-19 12:19:24,006 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', 'sc@@', 'e', 'la', 'seri@@', 'amente', 'di', 'questo', 'parti@@', 'col@@', 'are', 'problema', 'perché', 'non', 'mostr@@', 'a', 'il', 'tr@@', 'ick@@', 'ento', 'del', 'gh@@', 'i@@', 'acci@@', 'o', '.', '</s>']
2024-05-19 12:19:24,007 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-19 12:19:24,007 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio.
2024-05-19 12:19:24,008 - INFO - joeynmt.training - 	Hypothesis: Ma questo capisce la seriamente di questo particolare problema perché non mostra il trickento del ghiaccio.
2024-05-19 12:19:24,008 - INFO - joeynmt.training - Example #2
2024-05-19 12:19:24,008 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sen@@', 'se', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', '.']
2024-05-19 12:19:24,009 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.']
2024-05-19 12:19:24,009 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'ca@@', 'p', 'vol@@', 'a', 'ar@@', 't@@', 'ica', 'è', 'che', 'in', 'un', 'sen@@', 'so', ',', 'il', 'cu@@', 'ore', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale', '.', '</s>']
2024-05-19 12:19:24,009 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-19 12:19:24,010 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso, il cuore pulsante del sistema climatico globale.
2024-05-19 12:19:24,010 - INFO - joeynmt.training - 	Hypothesis: La cap vola artica è che in un senso, il cuore del sistema climatico globale.
2024-05-19 12:19:24,010 - INFO - joeynmt.training - Example #3
2024-05-19 12:19:24,011 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'm@@', 'er', '.']
2024-05-19 12:19:24,011 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'p@@', 'and@@', 'e', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', 'd@@', '&@@', 'a@@', 'pos@@', ';', 'est@@', 'ate', '.']
2024-05-19 12:19:24,011 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', 'p@@', 'and@@', 'e', 'in', 'in@@', 'ver@@', 'no', 'e', 'contr@@', 'o', 'in', 'est@@', 'ate', '.', '</s>']
2024-05-19 12:19:24,012 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-19 12:19:24,012 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2024-05-19 12:19:24,013 - INFO - joeynmt.training - 	Hypothesis: Si espande in inverno e contro in estate.
2024-05-19 12:19:24,013 - INFO - joeynmt.training - Example #4
2024-05-19 12:19:24,013 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2024-05-19 12:19:24,014 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.']
2024-05-19 12:19:24,014 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'che', 'vi', 'mostr@@', 'er@@', 'à', 'un', 'ra@@', 'pi@@', 'do', 'velo@@', 'c@@', 'emente', 'di', 'ciò', 'che', 'è', 'succ@@', 'esso', 'negli', 'ulti@@', 'mi', '2@@', '5', 'anni', '.', '</s>']
2024-05-19 12:19:24,014 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-19 12:19:24,015 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-19 12:19:24,015 - INFO - joeynmt.training - 	Hypothesis: La prossima diapositiva che vi mostrerà un rapido velocemente di ciò che è successo negli ultimi 25 anni.
2024-05-19 12:19:27,385 - INFO - joeynmt.training - Epoch  10, Step:    47600, Batch Loss:     1.077768, Batch Acc: 0.651693, Tokens per Sec:    20414, Lr: 0.000300
2024-05-19 12:19:30,746 - INFO - joeynmt.training - Epoch  10, Step:    47700, Batch Loss:     1.189668, Batch Acc: 0.651693, Tokens per Sec:    21785, Lr: 0.000300
2024-05-19 12:19:34,561 - INFO - joeynmt.training - Epoch  10, Step:    47800, Batch Loss:     1.271547, Batch Acc: 0.652483, Tokens per Sec:    19058, Lr: 0.000300
2024-05-19 12:19:38,495 - INFO - joeynmt.training - Epoch  10, Step:    47900, Batch Loss:     1.089272, Batch Acc: 0.650821, Tokens per Sec:    19005, Lr: 0.000300
2024-05-19 12:19:39,653 - INFO - joeynmt.training - Epoch  10: total training loss 5581.70
2024-05-19 12:19:39,654 - INFO - joeynmt.training - Training ended after  10 epochs.
2024-05-19 12:19:39,654 - INFO - joeynmt.training - Best validation result (greedy) at step    47500:   3.78 ppl.
2024-05-19 12:19:39,678 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 12:19:39,760 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 12:19:39,913 - INFO - joeynmt.helpers - Load model from /content/drive/MyDrive/mt5/mt-exercise-5/word_level_model/47500.ckpt.
2024-05-19 12:19:39,934 - INFO - joeynmt.prediction - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=1883),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=1840),
	loss_function=None)
2024-05-19 12:19:39,935 - INFO - joeynmt.prediction - Decoding on dev set...
2024-05-19 12:19:39,935 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:19:39,936 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:20:22,600 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 42.5654[sec], evaluation: 0.0000[sec]
2024-05-19 12:20:22,606 - INFO - joeynmt.prediction - Translations saved to: /content/drive/MyDrive/mt5/mt-exercise-5/word_level_model/00047500.hyps.dev.
2024-05-19 12:20:22,607 - INFO - joeynmt.prediction - Decoding on test set...
2024-05-19 12:20:22,607 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 12:20:22,607 - INFO - joeynmt.prediction - Predicting 1566 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 12:21:21,234 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 58.4623[sec], evaluation: 0.0000[sec]
2024-05-19 12:21:21,243 - INFO - joeynmt.prediction - Translations saved to: /content/drive/MyDrive/mt5/mt-exercise-5/word_level_model/00047500.hyps.test.
